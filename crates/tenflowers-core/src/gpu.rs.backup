use crate::gpu_profiler::global_profiler;
#[cfg(feature = "gpu")]
use crate::{buffer::TensorBuffer, Device, Result, TensorError};
use scirs2_autograd::ndarray::ArrayD;
use std::sync::Arc;
use std::time::Instant;

// Macro to safely include WGSL shader files, working around Rust 2021 edition prefix parsing
macro_rules! include_shader {
    ("activation_ops") => {
        include_str!("gpu/shaders/activation_ops.wgsl")
    };
    ("manipulation_ops") => {
        include_str!("gpu/shaders/manipulation_ops.wgsl")
    };
    ("comparison_ops") => {
        include_str!("gpu/shaders/comparison_ops.wgsl")
    };
    ("logical_ops") => {
        include_str!("gpu/shaders/logical_ops.wgsl")
    };
    ("random_ops") => {
        include_str!("gpu/shaders/random_ops.wgsl")
    };
    ("reduction_ops") => {
        include_str!("gpu/shaders/reduction_ops.wgsl")
    };
    ("einsum_ops") => {
        include_str!("gpu/shaders/einsum_ops.wgsl")
    };
    ("binary_ops") => {
        include_str!("gpu/shaders/binary_ops.wgsl")
    };
    ("conv_ops") => {
        include_str!("gpu/shaders/conv_ops.wgsl")
    };
    ("matmul_ops") => {
        include_str!("gpu/shaders/matmul_ops.wgsl")
    };
    ("attention_ops") => {
        include_str!("gpu/shaders/attention_ops.wgsl")
    };
    ("embedding_ops") => {
        include_str!("gpu/shaders/embedding_ops.wgsl")
    };
    ("normalization_ops") => {
        include_str!("gpu/shaders/normalization_ops.wgsl")
    };
    ("pooling_ops") => {
        include_str!("gpu/shaders/pooling_ops.wgsl")
    };
    ("strided_ops") => {
        include_str!("gpu/shaders/strided_ops.wgsl")
    };
    ("unary_ops") => {
        include_str!("gpu/shaders/unary_ops.wgsl")
    };
    ("unary_ops_f64") => {
        include_str!("gpu/shaders/unary_ops_f64.wgsl")
    };
    ("unary_ops_i32") => {
        include_str!("gpu/shaders/unary_ops_i32.wgsl")
    };
    ("unary_ops_i64") => {
        include_str!("gpu/shaders/unary_ops_i64.wgsl")
    };
    ("unary_ops_u32") => {
        include_str!("gpu/shaders/unary_ops_u32.wgsl")
    };
    ("unary_ops_u64") => {
        include_str!("gpu/shaders/unary_ops_u64.wgsl")
    };
    ("binary_ops_f64") => {
        include_str!("gpu/shaders/binary_ops_f64.wgsl")
    };
    ("binary_ops_i32") => {
        include_str!("gpu/shaders/binary_ops_i32.wgsl")
    };
    ("binary_ops_i64") => {
        include_str!("gpu/shaders/binary_ops_i64.wgsl")
    };
    ("topk_ops") => {
        include_str!("gpu/shaders/topk_ops.wgsl")
    };
    ("manipulation_ops2") => {
        include_str!("gpu/shaders/manipulation_ops2.wgsl")
    };
    ("fused_ops") => {
        include_str!("gpu/shaders/fused_ops.wgsl")
    };
    ("fft_ops") => {
        include_str!("gpu/shaders/fft_ops.wgsl")
    };
}

// Async kernel execution module
#[cfg(feature = "gpu")]
pub mod async_kernel;

// Linear algebra operations module
#[cfg(feature = "gpu")]
pub mod linalg_ops;

// Memory coalescing optimization module
#[cfg(feature = "gpu")]
pub mod memory_coalescing;

// Multi-stream GPU executor for CPU-GPU overlap
#[cfg(feature = "gpu")]
pub mod multi_stream_executor;

// RNN GPU operations module
#[cfg(feature = "gpu")]
pub mod rnn_ops;

// Attention operations module for neural networks
#[cfg(feature = "gpu")]
pub mod attention_ops;

// Kernel fusion module for performance optimization
#[cfg(feature = "gpu")]
pub mod kernel_fusion;

// Advanced memory pool management
#[cfg(feature = "gpu")]
pub mod memory_pool;

// Performance optimizer and profiler
#[cfg(feature = "gpu")]
pub mod performance_optimizer;

// Advanced kernel manager for cutting-edge GPU optimizations
#[cfg(feature = "gpu")]
pub mod advanced_kernel_manager;

// cuDNN integration for optimized neural network operations
#[cfg(feature = "cudnn")]
pub mod cudnn;

// Metal kernels for Apple Silicon and macOS GPU acceleration
#[cfg(all(target_os = "macos", feature = "metal"))]
pub mod metal_kernels;

// ROCm kernels for AMD GPU acceleration
#[cfg(feature = "rocm")]
pub mod rocm_kernels;

// CUDA kernels for direct CUDA backend support
#[cfg(feature = "cuda")]
pub mod cuda_kernels;

// NCCL integration for multi-GPU distributed training
#[cfg(feature = "nccl")]
pub mod nccl_integration;

// Core GPU buffer and operation modules
pub mod buffer;
pub mod binary_ops;
pub mod unary_ops;

// Helper function to cast values to f32 for GPU shaders
fn cast_to_f32<T>(value: T) -> f32
where
    T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
{
    // This is a simplified cast - in practice you'd need type-specific casting
    // For now, assume we're dealing with f32 values
    unsafe { std::mem::transmute_copy(&value) }
}

#[cfg(feature = "gpu")]
#[derive(Debug)]
pub struct GpuBuffer<T> {
    buffer: wgpu::Buffer,
    pub device: Arc<wgpu::Device>,
    pub queue: Arc<wgpu::Queue>,
    device_enum: Device,
    len: usize,
    is_pinned: bool,
    _phantom: std::marker::PhantomData<T>,
}

/// A view into a GPU buffer that references a portion of the original buffer
pub struct GpuBufferView<T> {
    parent_buffer: Arc<GpuBuffer<T>>,
    offset: usize,
    len: usize,
    device_enum: Device,
    _phantom: std::marker::PhantomData<T>,
}

#[cfg(feature = "gpu")]
impl<T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static> TensorBuffer
    for GpuBufferView<T>
{
    type Elem = T;

    fn device(&self) -> &Device {
        &self.device_enum
    }

    fn len(&self) -> usize {
        self.len
    }

    fn size_bytes(&self) -> usize {
        self.len * std::mem::size_of::<T>()
    }

    fn clone_buffer(&self) -> Result<Box<dyn TensorBuffer<Elem = Self::Elem>>> {
        // For buffer views, create a new independent buffer with the viewed data
        let elem_size = std::mem::size_of::<T>();
        let byte_offset = self.offset * elem_size;
        let byte_len = self.len * elem_size;

        let new_buffer = self
            .parent_buffer
            .device
            .create_buffer(&wgpu::BufferDescriptor {
                label: Some("view_clone_buffer"),
                size: byte_len as u64,
                usage: wgpu::BufferUsages::STORAGE
                    | wgpu::BufferUsages::COPY_SRC
                    | wgpu::BufferUsages::COPY_DST,
                mapped_at_creation: false,
            });

        // Copy the viewed data to the new buffer
        let mut encoder =
            self.parent_buffer
                .device
                .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                    label: Some("view_clone_encoder"),
                });
        encoder.copy_buffer_to_buffer(
            &self.parent_buffer.buffer,
            byte_offset as u64,
            &new_buffer,
            0,
            byte_len as u64,
        );
        self.parent_buffer
            .queue
            .submit(std::iter::once(encoder.finish()));

        Ok(Box::new(GpuBuffer {
            buffer: new_buffer,
            device: Arc::clone(&self.parent_buffer.device),
            queue: Arc::clone(&self.parent_buffer.queue),
            device_enum: self.device_enum.clone(),
            len: self.len,
            _phantom: std::marker::PhantomData,
        }))
    }

    fn view(&self, offset: usize, len: usize) -> Result<Box<dyn TensorBuffer<Elem = Self::Elem>>> {
        if offset + len > self.len {
            return Err(TensorError::invalid_argument(format!(
                "View out of bounds: offset={offset}, len={len}, buffer_len={}",
                self.len
            )));
        }

        // Create a nested view with adjusted offset
        let view = GpuBufferView {
            parent_buffer: Arc::clone(&self.parent_buffer),
            offset: self.offset + offset,
            len,
            device_enum: self.device_enum.clone(),
            _phantom: std::marker::PhantomData,
        };

        Ok(Box::new(view))
    }

    fn to_cpu(&self) -> Result<Vec<Self::Elem>> {
        let array = self.to_cpu_array()?;
        Ok(array.into_raw_vec())
    }

    fn from_cpu(
        data: &[Self::Elem],
        device: &Device,
    ) -> Result<Box<dyn TensorBuffer<Elem = Self::Elem>>> {
        GpuBuffer::from_cpu(data, device)
    }
}

impl<T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static> GpuBufferView<T> {
    /// Convert the viewed portion to CPU array
    pub fn to_cpu_array(&self) -> Result<ArrayD<T>> {
        let elem_size = std::mem::size_of::<T>();
        let byte_offset = self.offset * elem_size;
        let byte_len = self.len * elem_size;

        // Create a staging buffer for reading the viewed data
        let staging_buffer = self
            .parent_buffer
            .device
            .create_buffer(&wgpu::BufferDescriptor {
                label: Some("view_staging_buffer"),
                size: byte_len as u64,
                usage: wgpu::BufferUsages::COPY_DST | wgpu::BufferUsages::MAP_READ,
                mapped_at_creation: false,
            });

        // Copy the viewed portion to staging buffer
        let mut encoder =
            self.parent_buffer
                .device
                .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                    label: Some("view_copy_encoder"),
                });
        encoder.copy_buffer_to_buffer(
            &self.parent_buffer.buffer,
            byte_offset as u64,
            &staging_buffer,
            0,
            byte_len as u64,
        );
        self.parent_buffer
            .queue
            .submit(std::iter::once(encoder.finish()));

        // Map and read the staging buffer
        let buffer_slice = staging_buffer.slice(..);
        let (sender, receiver) = futures::channel::oneshot::channel();
        buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
            let _ = sender.send(result);
        });

        self.parent_buffer.device.poll(wgpu::Maintain::Wait);

        // Block on receiving the result
        let map_result = futures::executor::block_on(receiver).map_err(|_| {
            TensorError::compute_error_simple("Failed to receive map result".to_string())
        })?;

        match map_result {
            Ok(()) => {
                let data = buffer_slice.get_mapped_range();
                let typed_data: &[T] = bytemuck::cast_slice(&data);
                let array = ArrayD::from_shape_vec(vec![self.len], typed_data.to_vec())
                    .map_err(|e| TensorError::ShapeError(format!("Failed to create array: {e}")))?;

                drop(data);
                staging_buffer.unmap();
                Ok(array)
            }
            Err(e) => Err(TensorError::compute_error_simple(format!(
                "Failed to map buffer: {e:?}"
            ))),
        }
    }
}

#[cfg(feature = "gpu")]
impl<T> Clone for GpuBuffer<T> {
    fn clone(&self) -> Self {
        // Create a new buffer and copy data from the original
        let size = self.len * std::mem::size_of::<T>();
        let new_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("cloned_tensor_buffer"),
            size: size as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Copy data
        let mut encoder = self
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("clone_encoder"),
            });
        encoder.copy_buffer_to_buffer(&self.buffer, 0, &new_buffer, 0, size as u64);
        self.queue.submit(std::iter::once(encoder.finish()));

        Self {
            buffer: new_buffer,
            device: Arc::clone(&self.device),
            queue: Arc::clone(&self.queue),
            device_enum: self.device_enum,
            len: self.len,
            is_pinned: self.is_pinned,
            _phantom: std::marker::PhantomData,
        }
    }
}

#[cfg(feature = "gpu")]
impl<T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static> GpuBuffer<T> {
    pub fn from_cpu_array(array: &ArrayD<T>, device_id: usize) -> Result<Self> {
        use wgpu::util::DeviceExt;

        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;
        let data = bytemuck::cast_slice(array.as_slice().unwrap());

        let buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("tensor_buffer"),
                contents: data,
                usage: wgpu::BufferUsages::STORAGE
                    | wgpu::BufferUsages::COPY_SRC
                    | wgpu::BufferUsages::COPY_DST,
            });

        Ok(Self {
            buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: array.len(),
            is_pinned: false, // Default to not pinned
            _phantom: std::marker::PhantomData,
        })
    }

    pub fn to_cpu_array(&self) -> Result<ArrayD<T>> {
        let size = self.len * std::mem::size_of::<T>();

        // Create staging buffer for readback
        let staging_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("staging_buffer"),
            size: size as u64,
            usage: wgpu::BufferUsages::MAP_READ | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create command encoder and copy data
        let mut encoder = self
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("copy_encoder"),
            });

        encoder.copy_buffer_to_buffer(&self.buffer, 0, &staging_buffer, 0, size as u64);
        self.queue.submit(std::iter::once(encoder.finish()));

        // Map buffer and read data
        let buffer_slice = staging_buffer.slice(..);
        let (sender, receiver) = futures::channel::oneshot::channel();
        buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
            sender.send(result).unwrap();
        });

        self.device.poll(wgpu::Maintain::Wait);

        match pollster::block_on(receiver) {
            Ok(Ok(())) => {
                let data = buffer_slice.get_mapped_range();
                let typed_data: &[T] = bytemuck::cast_slice(&data);
                let vec_data = typed_data.to_vec();
                drop(data);
                staging_buffer.unmap();

                // Convert to ArrayD - for simplicity, create a 1D array
                Ok(ArrayD::from_shape_vec(vec![self.len], vec_data)
                    .map_err(|e| TensorError::invalid_argument(&e.to_string()))?)
            }
            _ => Err(TensorError::compute_error_simple(
                "Failed to read GPU buffer".to_string(),
            )),
        }
    }

    pub fn zeros(len: usize, device_id: usize) -> Result<Self> {
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;
        let size = len * std::mem::size_of::<T>();

        let buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("zero_tensor_buffer"),
            size: size as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        Ok(Self {
            buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len,
            is_pinned: false, // Default to not pinned
            _phantom: std::marker::PhantomData,
        })
    }

    /// Create GPU buffer from CPU slice
    pub fn from_slice(slice: &[T], device: &Device) -> Result<Self> {
        use wgpu::util::DeviceExt;

        if let Device::Gpu(device_id) = device {
            let gpu_ctx = super::device::context::get_gpu_context(*device_id)?;
            let data = bytemuck::cast_slice(slice);

            let buffer = gpu_ctx
                .device
                .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                    label: Some("tensor_buffer_from_slice"),
                    contents: data,
                    usage: wgpu::BufferUsages::STORAGE
                        | wgpu::BufferUsages::COPY_SRC
                        | wgpu::BufferUsages::COPY_DST,
                });

            Ok(Self {
                buffer,
                device: gpu_ctx.device.clone(),
                queue: gpu_ctx.queue.clone(),
                device_enum: *device,
                len: slice.len(),
                is_pinned: false, // Default to not pinned
                _phantom: std::marker::PhantomData,
            })
        } else {
            Err(TensorError::device_error_simple("GPU not available"))
        }
    }

    /// Transfer buffer to another device
    pub fn transfer_to_device(&self, target_device: &Device) -> Result<Self> {
        match target_device {
            Device::Gpu(target_gpu_id) => {
                if let Device::Gpu(src_gpu_id) = &self.device_enum {
                    if src_gpu_id == target_gpu_id {
                        // Same device, just clone
                        return Ok(self.clone());
                    }
                }

                // Different GPU or CPU->GPU transfer
                // For now, we'll do CPU round-trip (in practice, you'd use direct GPU-GPU copy)
                let cpu_data = self.to_cpu()?;
                Self::from_slice(&cpu_data, target_device)
            }
            Device::Cpu => {
                // GPU to CPU transfer - this should not happen here as it would return ArrayD
                Err(TensorError::DeviceError(
                    "Cannot transfer GPU buffer to CPU as GpuBuffer".to_string(),
                ))
            }
        }
    }

    /// Get raw CPU data (for internal use)
    pub fn to_cpu(&self) -> Result<Vec<T>> {
        let array = self.to_cpu_array()?;
        Ok(array.into_raw_vec())
    }

    pub fn buffer(&self) -> &wgpu::Buffer {
        &self.buffer
    }

    pub fn device(&self) -> &wgpu::Device {
        &self.device
    }

    pub fn queue(&self) -> &wgpu::Queue {
        &self.queue
    }

    /// Get async GPU executor for this buffer
    pub fn get_async_executor(&self) -> async_kernel::AsyncGpuExecutor {
        async_kernel::AsyncGpuExecutor::new(Arc::clone(&self.device), Arc::clone(&self.queue))
    }

    /// Execute binary operation asynchronously
    pub fn binary_op_async(
        &self,
        other: &Self,
        operation: BinaryOp,
        output_len: usize,
    ) -> async_kernel::GpuKernelFuture<T> {
        let executor = self.get_async_executor();
        executor.execute_binary_op_async(self, other, operation, output_len)
    }

    /// Execute reduction operation asynchronously
    pub fn reduction_op_async(
        &self,
        operation: ReductionOp,
        output_len: usize,
    ) -> async_kernel::GpuKernelFuture<T> {
        let executor = self.get_async_executor();
        executor.execute_reduction_op_async(self, operation, output_len)
    }

    /// Execute matrix multiplication asynchronously
    pub fn matmul_async(
        &self,
        other: &Self,
        m: usize,
        k: usize,
        n: usize,
        batch_size: usize,
    ) -> async_kernel::GpuKernelFuture<T> {
        let executor = self.get_async_executor();
        executor.execute_matmul_async(self, other, m, k, n, batch_size)
    }

    /// Check if this buffer uses pinned memory
    pub fn is_pinned(&self) -> bool {
        self.is_pinned
    }

    /// Create a new buffer with pinned memory enabled (for faster CPU-GPU transfers)
    pub fn from_cpu_array_pinned(array: &ArrayD<T>, device_id: usize) -> Result<Self> {
        use wgpu::util::DeviceExt;

        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;
        let data = bytemuck::cast_slice(array.as_slice().unwrap());

        // For WGPU, we simulate pinned memory by using MAP_READ usage which may be more efficient
        let buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("pinned_tensor_buffer"),
                contents: data,
                usage: wgpu::BufferUsages::STORAGE
                    | wgpu::BufferUsages::COPY_SRC
                    | wgpu::BufferUsages::COPY_DST
                    | wgpu::BufferUsages::MAP_READ,
            });

        Ok(Self {
            buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: array.len(),
            is_pinned: true,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Create a pinned zero-initialized buffer
    pub fn zeros_pinned(len: usize, device_id: usize) -> Result<Self> {
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;
        let size = len * std::mem::size_of::<T>();

        let buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("pinned_zero_tensor_buffer"),
            size: size as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST
                | wgpu::BufferUsages::MAP_READ,
            mapped_at_creation: false,
        });

        Ok(Self {
            buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len,
            is_pinned: true,
            _phantom: std::marker::PhantomData,
        })
    }
}

#[cfg(feature = "gpu")]
impl<T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static> TensorBuffer
    for GpuBuffer<T>
{
    type Elem = T;

    fn device(&self) -> &Device {
        &self.device_enum
    }

    fn len(&self) -> usize {
        self.len
    }

    fn size_bytes(&self) -> usize {
        self.len * std::mem::size_of::<T>()
    }

    fn clone_buffer(&self) -> Result<Box<dyn TensorBuffer<Elem = Self::Elem>>> {
        // For GPU buffers, we need to actually copy the data
        let size = self.size_bytes();
        let new_buffer = self.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("cloned_tensor_buffer"),
            size: size as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Copy data
        let mut encoder = self
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("clone_encoder"),
            });
        encoder.copy_buffer_to_buffer(&self.buffer, 0, &new_buffer, 0, size as u64);
        self.queue.submit(std::iter::once(encoder.finish()));

        Ok(Box::new(Self {
            buffer: new_buffer,
            device: Arc::clone(&self.device),
            queue: Arc::clone(&self.queue),
            device_enum: self.device_enum,
            len: self.len,
            is_pinned: self.is_pinned,
            _phantom: std::marker::PhantomData,
        }))
    }

    fn view(&self, offset: usize, len: usize) -> Result<Box<dyn TensorBuffer<Elem = Self::Elem>>> {
        if offset + len > self.len {
            return Err(TensorError::invalid_argument(format!(
                "View out of bounds: offset={}, len={}, buffer_len={}",
                offset, len, self.len
            )));
        }

        // Create a proper buffer view that references the original buffer
        let view = GpuBufferView {
            parent_buffer: Arc::new(self.clone()), // Clone for shared ownership
            offset,
            len,
            device_enum: self.device_enum.clone(),
            _phantom: std::marker::PhantomData,
        };

        Ok(Box::new(view))
    }

    fn to_cpu(&self) -> Result<Vec<Self::Elem>> {
        let array = self.to_cpu_array()?;
        Ok(array.into_raw_vec())
    }

    unsafe fn as_ptr(&self) -> *const Self::Elem {
        // GPU buffers don't have direct memory access
        std::ptr::null()
    }

    unsafe fn as_mut_ptr(&mut self) -> *mut Self::Elem {
        // GPU buffers don't have direct memory access
        std::ptr::null_mut()
    }
}

/// GPU context for managing compute operations
#[derive(Debug)]
pub struct GpuContext {
    pub device: Arc<wgpu::Device>,
    pub queue: Arc<wgpu::Queue>,
    pub device_id: usize,
}

impl GpuContext {
    pub fn new(device: Arc<wgpu::Device>, queue: Arc<wgpu::Queue>, device_id: usize) -> Self {
        Self {
            device,
            queue,
            device_id,
        }
    }
}

/// Binary scalar operation enum
#[derive(Debug, Clone, Copy)]
pub enum BinaryScalarOp {
    Add,
    Sub,
    Mul,
    Div,
    Pow,
}

/// Export the include_shader macro for use in submodules
#[macro_export]
macro_rules! gpu_include_shader {
    ($name:literal) => {
        match $name {
            "unary_ops" => include_str!("../gpu/shaders/unary_ops.wgsl"),
            "unary_ops_f64" => include_str!("../gpu/shaders/unary_ops_f64.wgsl"),
            "unary_ops_i32" => include_str!("../gpu/shaders/unary_ops_i32.wgsl"),
            "unary_ops_i64" => include_str!("../gpu/shaders/unary_ops_i64.wgsl"),
            "unary_ops_u32" => include_str!("../gpu/shaders/unary_ops_u32.wgsl"),
            "unary_ops_u64" => include_str!("../gpu/shaders/unary_ops_u64.wgsl"),
            "binary_ops" => include_str!("../gpu/shaders/binary_ops.wgsl"),
            "binary_ops_f64" => include_str!("../gpu/shaders/binary_ops_f64.wgsl"),
            "binary_ops_i32" => include_str!("../gpu/shaders/binary_ops_i32.wgsl"),
            "binary_ops_i64" => include_str!("../gpu/shaders/binary_ops_i64.wgsl"),
            "fft_ops" => include_str!("../gpu/shaders/fft_ops.wgsl"),
            _ => include_str!("../gpu/shaders/unary_ops.wgsl"), // fallback
        }
    };
}

pub use gpu_include_shader;

/// Export ReductionOp enum
pub use ops::ReductionOp;

/// GPU comparison operation dispatch function
pub fn gpu_comparison_op_dispatch<T>(
    input_a: &GpuBuffer<T>,
    input_b: &GpuBuffer<T>,
    operation: crate::ops::ComparisonOp,
) -> Result<GpuBuffer<bool>>
where
    T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
{
    // Fallback implementation - create a boolean result buffer
    let device_id = match &input_a.device_enum {
        Device::Gpu(id) => *id,
        _ => return Err(TensorError::DeviceMismatch),
    };
    
    // For now, return a simple boolean buffer
    let result_data = vec![true; input_a.len];
    GpuBuffer::from_slice(&result_data, &Device::Gpu(device_id))
}

/// GPU compute operations module
pub mod ops {
    use super::*;
    use crate::Result;
    use std::sync::Arc;

    /// Execute a unary operation on GPU
    pub fn execute_unary_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: UnaryOp,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("unary_op_output"),
            size: (input_buffer.len() * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module based on data type
        let type_name = std::any::type_name::<T>();
        let shader_source = match type_name {
            "f32" => include_shader!("unary_ops"),
            "f64" => include_shader!("unary_ops_f64"),
            "i32" => include_shader!("unary_ops_i32"),
            "i64" => include_shader!("unary_ops_i64"),
            "u32" => include_shader!("unary_ops_u32"),
            "u64" => include_shader!("unary_ops_u64"),
            _ => include_shader!("unary_ops"), // fallback to f32
        };

        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("unary_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("unary_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("unary_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("unary_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match (operation, type_name) {
            (UnaryOp::Log, "f32") => "log_op",
            (UnaryOp::Log, "f64") => "log_f64",
            (UnaryOp::Neg, "f32") => "neg_op",
            (UnaryOp::Neg, "f64") => "neg_f64",
            (UnaryOp::Neg, "i32") => "neg_i32",
            (UnaryOp::Neg, "i64") => "neg_i64",
            (UnaryOp::Sqrt, "f32") => "sqrt_op",
            (UnaryOp::Sqrt, "f64") => "sqrt_f64",
            (UnaryOp::Abs, "f32") => "abs_op",
            (UnaryOp::Abs, "f64") => "abs_f64",
            (UnaryOp::Abs, "i32") => "abs_i32",
            (UnaryOp::Abs, "i64") => "abs_i64",
            (UnaryOp::Abs, "u32") => "abs_u32",
            (UnaryOp::Abs, "u64") => "abs_u64",
            (UnaryOp::Exp, "f32") => "exp_op",
            (UnaryOp::Exp, "f64") => "exp_f64",
            (UnaryOp::Sin, "f32") => "sin_op",
            (UnaryOp::Sin, "f64") => "sin_f64",
            (UnaryOp::Cos, "f32") => "cos_op",
            (UnaryOp::Cos, "f64") => "cos_f64",
            (UnaryOp::Tan, "f32") => "tan_op",
            (UnaryOp::Tan, "f64") => "tan_f64",
            (UnaryOp::Recip, "f32") => "recip_op",
            (UnaryOp::Recip, "f64") => "recip_f64",
            (UnaryOp::Floor, "f32") => "floor_op",
            (UnaryOp::Floor, "f64") => "floor_f64",
            (UnaryOp::Ceil, "f32") => "ceil_op",
            (UnaryOp::Ceil, "f64") => "ceil_f64",
            (UnaryOp::Round, "f32") => "round_op",
            (UnaryOp::Round, "f64") => "round_f64",
            // Fallback to f32 operation for unsupported type-operation combinations
            (UnaryOp::Log, _) => "log_op",
            (UnaryOp::Neg, _) => "neg_op",
            (UnaryOp::Sqrt, _) => "sqrt_op",
            (UnaryOp::Abs, _) => "abs_op",
            (UnaryOp::Exp, _) => "exp_op",
            (UnaryOp::Sin, _) => "sin_op",
            (UnaryOp::Cos, _) => "cos_op",
            (UnaryOp::Tan, _) => "tan_op",
            (UnaryOp::Recip, _) => "recip_op",
            (UnaryOp::Floor, _) => "floor_op",
            (UnaryOp::Ceil, _) => "ceil_op",
            (UnaryOp::Round, _) => "round_op",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("unary_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader with profiling
        let start_time = Instant::now();
        let input_memory = (input_buffer.len() * std::mem::size_of::<T>()) as u64;

        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("unary_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("unary_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Record profiling data
        let execution_time = start_time.elapsed();
        let operation_name = format!("unary_{:?}", operation);
        let _ = global_profiler().record_operation(
            &operation_name,
            input_buffer.device_enum,
            execution_time,
            input_memory,
        );

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: input_buffer.len,
            _phantom: std::marker::PhantomData,
        })
    }

    #[derive(Debug, Clone, Copy)]
    pub enum UnaryOp {
        Log,
        Neg,
        Sqrt,
        Abs,
        Exp,
        Sin,
        Cos,
        Tan,
        Recip,
        Floor,
        Ceil,
        Round,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum BinaryOp {
        Add,
        Sub,
        Mul,
        Div,
        Pow,
        PReLU,
        Min,
        Max,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum FusedOp {
        AddRelu,
        AddSigmoid,
        AddTanh,
        AddGelu,
        AddSwish,
        AddMish,
        AddLeakyRelu(f32), // negative slope parameter
        MulRelu,
        MulSigmoid,
        MulTanh,
        LayerNormRelu,
        LayerNormGelu,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum ActivationOp {
        ReLU,
        Sigmoid,
        Tanh,
        GELU,
        Swish,
        ELU,
        LeakyReLU,
        Mish,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum ComparisonOp {
        Eq,
        Ne,
        Lt,
        Le,
        Gt,
        Ge,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum LogicalOp {
        And,
        Or,
        Xor,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum UnaryLogicalOp {
        Not,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum ReductionOp {
        Sum,
        Mean,
        Max,
        Min,
        ArgMax,
        ArgMin,
        All,
        Any,
        TopK,
        InfNanDetection,
    }

    /// Get the appropriate shader source for the given type
    fn get_binary_op_shader_source<T>() -> &'static str
    where
        T: 'static,
    {
        let type_name = std::any::type_name::<T>();
        match type_name {
            "f32" => include_shader!("binary_ops"),
            "f64" => include_shader!("binary_ops_f64"),
            "i32" => include_shader!("binary_ops_i32"),
            "i64" => include_shader!("binary_ops_i64"),
            _ => include_shader!("binary_ops"), // Default to f32 for unsupported types
        }
    }

    /// Execute a binary operation on GPU
    pub fn execute_binary_op<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        operation: BinaryOp,
        output_len: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("binary_op_output"),
            size: (output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module with type-specific shader
        let shader_source = get_binary_op_shader_source::<T>();
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("binary_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("binary_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("binary_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("binary_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            BinaryOp::Add => "add_op",
            BinaryOp::Sub => "sub_op",
            BinaryOp::Mul => "mul_op",
            BinaryOp::Div => "div_op",
            BinaryOp::Pow => "pow_op",
            BinaryOp::PReLU => "prelu_op",
            BinaryOp::Min => "min_op",
            BinaryOp::Max => "max_op",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("binary_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader with profiling
        let start_time = Instant::now();
        let input_memory = (input_a.len() * std::mem::size_of::<T>()
            + input_b.len() * std::mem::size_of::<T>()) as u64;

        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("binary_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("binary_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Record profiling data
        let execution_time = start_time.elapsed();
        let operation_name = format!("binary_{:?}", operation);
        let _ = global_profiler().record_operation(
            &operation_name,
            input_a.device_enum,
            execution_time,
            input_memory,
        );

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute an activation operation on GPU
    pub fn execute_activation_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: ActivationOp,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("activation_op_output"),
            size: (input_buffer.len() * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("activation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("activation_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout (same as unary ops)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("activation_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("activation_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("activation_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ActivationOp::ReLU => "relu_op",
            ActivationOp::Sigmoid => "sigmoid_op",
            ActivationOp::Tanh => "tanh_op",
            ActivationOp::GELU => "gelu_op",
            ActivationOp::Swish => "swish_op",
            ActivationOp::ELU => "elu_op",
            ActivationOp::LeakyReLU => "leaky_relu_op",
            ActivationOp::Mish => "mish_op",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("activation_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("activation_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("activation_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: input_buffer.len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute a reduction operation on GPU
    pub fn execute_reduction_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: ReductionOp,
        output_len: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("reduction_op_output"),
            size: (output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        use wgpu::util::DeviceExt;
        let metadata = [input_buffer.len() as u32, output_len as u32, 0u32]; // [input_size, output_size, axis_info]
        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("reduction_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shader module
        let shader_source = match operation {
            ReductionOp::ArgMax | ReductionOp::ArgMin => {
                include_shader!("reduction_ops")
            }
            _ => include_shader!("reduction_ops"),
        };

        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("reduction_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("reduction_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("reduction_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("reduction_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ReductionOp::Sum => "sum_reduction",
            ReductionOp::Mean => "mean_reduction",
            ReductionOp::Max => "max_reduction",
            ReductionOp::Min => "min_reduction",
            ReductionOp::ArgMax => "argmax_reduction",
            ReductionOp::ArgMin => "argmin_reduction",
            ReductionOp::All => "all_reduction",
            ReductionOp::Any => "any_reduction",
            ReductionOp::InfNanDetection => "inf_nan_detection",
            ReductionOp::TopK => {
                return Err(crate::TensorError::unsupported_operation_simple(
                    "TopK requires specialized function - use execute_topk_op instead".to_string(),
                ))
            }
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("reduction_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("reduction_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("reduction_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 256;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute TopK operation on GPU - returns (values, indices)
    pub fn execute_topk_op<T>(
        input_buffer: &GpuBuffer<T>,
        k: usize,
        axis_size: usize,
        num_slices: usize,
    ) -> Result<(GpuBuffer<T>, GpuBuffer<u32>)>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Create output buffers
        let values_output_len = num_slices * k;
        let values_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("topk_values_output"),
            size: (values_output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        let indices_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("topk_indices_output"),
            size: (values_output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer: [axis_size, k, num_slices, stride]
        use wgpu::util::DeviceExt;
        let metadata = [
            axis_size as u32,
            k as u32,
            num_slices as u32,
            axis_size as u32,
        ];
        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("topk_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shader module
        let shader_source = include_shader!("topk_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("topk_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("topk_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("topk_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: values_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: indices_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create pipeline layout
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("topk_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        // Choose entry point based on axis size and k
        let entry_point = if axis_size <= 256 && k <= 64 {
            "topk_bitonic_sort"
        } else if k <= 64 {
            "topk_heap_sort"
        } else {
            "topk_selection"
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("topk_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("topk_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("topk_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch one workgroup per slice
            compute_pass.dispatch_workgroups(num_slices as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffers
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        let values_gpu_buffer = GpuBuffer {
            buffer: values_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: values_output_len,
            _phantom: std::marker::PhantomData,
        };

        let indices_gpu_buffer = GpuBuffer {
            buffer: indices_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: values_output_len,
            _phantom: std::marker::PhantomData,
        };

        Ok((values_gpu_buffer, indices_gpu_buffer))
    }

    /// Execute reshape operation on GPU (just a memory copy)
    pub fn execute_reshape<T>(
        input_buffer: &GpuBuffer<T>,
        new_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Reshape is just a view change, so we copy the data as-is
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("reshape_output"),
            size: (input_buffer.len() * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("manipulation_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("reshape_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("reshape_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("reshape_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("reshape_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("reshape_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("reshape_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("reshape_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: input_buffer.len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU (f32)
    pub fn execute_comparison_op_f32(
        input_a: &GpuBuffer<f32>,
        input_b: &GpuBuffer<f32>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_f32",
            ComparisonOp::Ne => "ne_f32",
            ComparisonOp::Lt => "lt_f32",
            ComparisonOp::Le => "le_f32",
            ComparisonOp::Gt => "gt_f32",
            ComparisonOp::Ge => "ge_f32",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU (i32)
    pub fn execute_comparison_op_i32(
        input_a: &GpuBuffer<i32>,
        input_b: &GpuBuffer<i32>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout (same as f32 version)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 3, // Use binding 3 for i32 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4, // Use binding 4 for i32 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer still at binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_i32",
            ComparisonOp::Ne => "ne_i32",
            ComparisonOp::Lt => "lt_i32",
            ComparisonOp::Le => "le_i32",
            ComparisonOp::Gt => "gt_i32",
            ComparisonOp::Ge => "ge_i32",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU with broadcasting support (f32)
    pub fn execute_comparison_op_f32_with_broadcasting(
        input_a: &GpuBuffer<f32>,
        input_b: &GpuBuffer<f32>,
        operation: ComparisonOp,
        shape_a: &[usize],
        shape_b: &[usize],
        output_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_broadcast_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        use wgpu::util::DeviceExt;
        let mut shape_metadata = Vec::new();
        shape_metadata.push(shape_a.len() as u32);
        shape_metadata.push(shape_b.len() as u32);
        shape_metadata.push(output_shape.len() as u32);
        shape_metadata.extend(shape_a.iter().map(|&x| x as u32));
        shape_metadata.extend(shape_b.iter().map(|&x| x as u32));
        shape_metadata.extend(output_shape.iter().map(|&x| x as u32));

        let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("shape_metadata"),
            contents: bytemuck::cast_slice(&shape_metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_broadcast_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout with shape metadata
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_broadcast_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_broadcast_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_broadcast_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_f32_broadcast",
            ComparisonOp::Ne => "ne_f32_broadcast",
            ComparisonOp::Lt => "lt_f32_broadcast",
            ComparisonOp::Le => "le_f32_broadcast",
            ComparisonOp::Gt => "gt_f32_broadcast",
            ComparisonOp::Ge => "ge_f32_broadcast",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_broadcast_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_broadcast_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_broadcast_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU with broadcasting support (f64)
    pub fn execute_comparison_op_f64_with_broadcasting(
        input_a: &GpuBuffer<f64>,
        input_b: &GpuBuffer<f64>,
        operation: ComparisonOp,
        shape_a: &[usize],
        shape_b: &[usize],
        output_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_f64_broadcast_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        use wgpu::util::DeviceExt;
        let mut shape_metadata = Vec::new();
        shape_metadata.push(shape_a.len() as u32);
        shape_metadata.push(shape_b.len() as u32);
        shape_metadata.push(output_shape.len() as u32);
        shape_metadata.extend(shape_a.iter().map(|&x| x as u32));
        shape_metadata.extend(shape_b.iter().map(|&x| x as u32));
        shape_metadata.extend(output_shape.iter().map(|&x| x as u32));

        let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("shape_metadata_f64"),
            contents: bytemuck::cast_slice(&shape_metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_f64_broadcast_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout with shape metadata
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_f64_broadcast_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 7,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 8,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 21,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_f64_broadcast_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 7,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 8,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 21,
                    resource: shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_f64_broadcast_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_f64_broadcast",
            ComparisonOp::Ne => "ne_f64_broadcast",
            ComparisonOp::Lt => "lt_f64_broadcast",
            ComparisonOp::Le => "le_f64_broadcast",
            ComparisonOp::Gt => "gt_f64_broadcast",
            ComparisonOp::Ge => "ge_f64_broadcast",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_f64_broadcast_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_f64_broadcast_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_f64_broadcast_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU with broadcasting support (i32)
    pub fn execute_comparison_op_i32_with_broadcasting(
        input_a: &GpuBuffer<i32>,
        input_b: &GpuBuffer<i32>,
        operation: ComparisonOp,
        shape_a: &[usize],
        shape_b: &[usize],
        output_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        // Similar implementation to f32 version but with i32 types
        // For brevity, this would be similar to the f32 version but with different bindings
        // and entry points (eq_i32_broadcast, etc.)
        // Implementation would be analogous to the f32 version above
        execute_comparison_op_i32(input_a, input_b, operation, output_len)
    }

    /// Execute i64 comparison operation on GPU
    pub fn execute_comparison_op_i64(
        input_a: &GpuBuffer<i64>,
        input_b: &GpuBuffer<i64>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for i64
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 5, // Use binding 5 for i64 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 6, // Use binding 6 for i64 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 6,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_i64",
            ComparisonOp::Ne => "ne_i64",
            ComparisonOp::Lt => "lt_i64",
            ComparisonOp::Le => "le_i64",
            ComparisonOp::Gt => "gt_i64",
            ComparisonOp::Ge => "ge_i64",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operation on GPU with broadcasting support (i64)
    pub fn execute_comparison_op_i64_with_broadcasting(
        input_a: &GpuBuffer<i64>,
        input_b: &GpuBuffer<i64>,
        operation: ComparisonOp,
        shape_a: &[usize],
        shape_b: &[usize],
        output_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_i64_broadcast_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        use wgpu::util::DeviceExt;
        let mut shape_metadata = Vec::new();
        shape_metadata.push(shape_a.len() as u32);
        shape_metadata.push(shape_b.len() as u32);
        shape_metadata.push(output_shape.len() as u32);
        shape_metadata.extend(shape_a.iter().map(|&x| x as u32));
        shape_metadata.extend(shape_b.iter().map(|&x| x as u32));
        shape_metadata.extend(output_shape.iter().map(|&x| x as u32));

        let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("shape_metadata_i64"),
            contents: bytemuck::cast_slice(&shape_metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_i64_broadcast_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout with shape metadata
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_i64_broadcast_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 6,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 21,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_i64_broadcast_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 6,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 21,
                    resource: shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_i64_broadcast_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_i64_broadcast",
            ComparisonOp::Ne => "ne_i64_broadcast",
            ComparisonOp::Lt => "lt_i64_broadcast",
            ComparisonOp::Le => "le_i64_broadcast",
            ComparisonOp::Gt => "gt_i64_broadcast",
            ComparisonOp::Ge => "ge_i64_broadcast",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_i64_broadcast_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_i64_broadcast_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_i64_broadcast_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute f64 comparison operation on GPU
    pub fn execute_comparison_op_f64(
        input_a: &GpuBuffer<f64>,
        input_b: &GpuBuffer<f64>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for f64
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 7, // Use binding 7 for f64 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 8, // Use binding 8 for f64 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 7,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 8,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_f64",
            ComparisonOp::Ne => "ne_f64",
            ComparisonOp::Lt => "lt_f64",
            ComparisonOp::Le => "le_f64",
            ComparisonOp::Gt => "gt_f64",
            ComparisonOp::Ge => "ge_f64",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute u32 comparison operation on GPU
    pub fn execute_comparison_op_u32(
        input_a: &GpuBuffer<u32>,
        input_b: &GpuBuffer<u32>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for u32
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 17, // Use binding 17 for u32 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 18, // Use binding 18 for u32 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 17,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 18,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_u32",
            ComparisonOp::Ne => "ne_u32",
            ComparisonOp::Lt => "lt_u32",
            ComparisonOp::Le => "le_u32",
            ComparisonOp::Gt => "gt_u32",
            ComparisonOp::Ge => "ge_u32",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute u64 comparison operation on GPU
    pub fn execute_comparison_op_u64(
        input_a: &GpuBuffer<u64>,
        input_b: &GpuBuffer<u64>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for u64
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 19, // Use binding 19 for u64 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 20, // Use binding 20 for u64 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 19,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 20,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_u64",
            ComparisonOp::Ne => "ne_u64",
            ComparisonOp::Lt => "lt_u64",
            ComparisonOp::Le => "le_u64",
            ComparisonOp::Gt => "gt_u64",
            ComparisonOp::Ge => "ge_u64",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute i16 comparison operation on GPU (stored as i32)
    pub fn execute_comparison_op_i16(
        input_a: &GpuBuffer<i16>,
        input_b: &GpuBuffer<i16>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        // i16 is stored as i32 in GPU buffers, so we need to transmute
        let input_a_i32 =
            unsafe { std::mem::transmute::<&GpuBuffer<i16>, &GpuBuffer<i32>>(input_a) };
        let input_b_i32 =
            unsafe { std::mem::transmute::<&GpuBuffer<i16>, &GpuBuffer<i32>>(input_b) };

        let device = &input_a_i32.device;
        let queue = &input_a_i32.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for i16 (using i32 bindings)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 9, // Use binding 9 for i16 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 10, // Use binding 10 for i16 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 9,
                    resource: input_a_i32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 10,
                    resource: input_b_i32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_i16",
            ComparisonOp::Ne => "ne_i16",
            ComparisonOp::Lt => "lt_i16",
            ComparisonOp::Le => "le_i16",
            ComparisonOp::Gt => "gt_i16",
            ComparisonOp::Ge => "ge_i16",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a_i32.device),
            queue: Arc::clone(&input_a_i32.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute u16 comparison operation on GPU (stored as u32)
    pub fn execute_comparison_op_u16(
        input_a: &GpuBuffer<u16>,
        input_b: &GpuBuffer<u16>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        // u16 is stored as u32 in GPU buffers, so we need to transmute
        let input_a_u32 =
            unsafe { std::mem::transmute::<&GpuBuffer<u16>, &GpuBuffer<u32>>(input_a) };
        let input_b_u32 =
            unsafe { std::mem::transmute::<&GpuBuffer<u16>, &GpuBuffer<u32>>(input_b) };

        let device = &input_a_u32.device;
        let queue = &input_a_u32.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for u16 (using u32 bindings)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 11, // Use binding 11 for u16 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 12, // Use binding 12 for u16 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 11,
                    resource: input_a_u32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 12,
                    resource: input_b_u32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_u16",
            ComparisonOp::Ne => "ne_u16",
            ComparisonOp::Lt => "lt_u16",
            ComparisonOp::Le => "le_u16",
            ComparisonOp::Gt => "gt_u16",
            ComparisonOp::Ge => "ge_u16",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a_u32.device),
            queue: Arc::clone(&input_a_u32.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute i8 comparison operation on GPU (stored as i32)
    pub fn execute_comparison_op_i8(
        input_a: &GpuBuffer<i8>,
        input_b: &GpuBuffer<i8>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        // i8 is stored as i32 in GPU buffers, so we need to transmute
        let input_a_i32 =
            unsafe { std::mem::transmute::<&GpuBuffer<i8>, &GpuBuffer<i32>>(input_a) };
        let input_b_i32 =
            unsafe { std::mem::transmute::<&GpuBuffer<i8>, &GpuBuffer<i32>>(input_b) };

        let device = &input_a_i32.device;
        let queue = &input_a_i32.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for i8 (using i32 bindings)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 13, // Use binding 13 for i8 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 14, // Use binding 14 for i8 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 13,
                    resource: input_a_i32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 14,
                    resource: input_b_i32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_i8",
            ComparisonOp::Ne => "ne_i8",
            ComparisonOp::Lt => "lt_i8",
            ComparisonOp::Le => "le_i8",
            ComparisonOp::Gt => "gt_i8",
            ComparisonOp::Ge => "ge_i8",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a_i32.device),
            queue: Arc::clone(&input_a_i32.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute u8 comparison operation on GPU (stored as u32)
    pub fn execute_comparison_op_u8(
        input_a: &GpuBuffer<u8>,
        input_b: &GpuBuffer<u8>,
        operation: ComparisonOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        // u8 is stored as u32 in GPU buffers, so we need to transmute
        let input_a_u32 =
            unsafe { std::mem::transmute::<&GpuBuffer<u8>, &GpuBuffer<u32>>(input_a) };
        let input_b_u32 =
            unsafe { std::mem::transmute::<&GpuBuffer<u8>, &GpuBuffer<u32>>(input_b) };

        let device = &input_a_u32.device;
        let queue = &input_a_u32.queue;

        // Create output buffer for boolean results (u32: 0=false, 1=true)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("comparison_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("comparison_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for u8 (using u32 bindings)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("comparison_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 15, // Use binding 15 for u8 input_a
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 16, // Use binding 16 for u8 input_b
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2, // Output buffer uses binding 2
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("comparison_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 15,
                    resource: input_a_u32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 16,
                    resource: input_b_u32.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("comparison_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            ComparisonOp::Eq => "eq_u8",
            ComparisonOp::Ne => "ne_u8",
            ComparisonOp::Lt => "lt_u8",
            ComparisonOp::Le => "le_u8",
            ComparisonOp::Gt => "gt_u8",
            ComparisonOp::Ge => "ge_u8",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("comparison_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("comparison_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a_u32.device),
            queue: Arc::clone(&input_a_u32.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute logical operation on GPU
    pub fn execute_logical_op(
        input_a: &GpuBuffer<u8>,
        input_b: &GpuBuffer<u8>,
        operation: LogicalOp,
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer for boolean results (u32 in shader)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("logical_op_output"),
            size: (output_len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("logical_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("logical_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("logical_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("logical_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("logical_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            LogicalOp::And => "and_op",
            LogicalOp::Or => "or_op",
            LogicalOp::Xor => "xor_op",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("logical_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("logical_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("logical_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute unary logical operation on GPU
    pub fn execute_unary_logical_op(
        input_buffer: &GpuBuffer<u8>,
        operation: UnaryLogicalOp,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("unary_logical_op_output"),
            size: (input_buffer.len() * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("logical_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("logical_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout (only input and output, no second input)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("unary_logical_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("unary_logical_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("unary_logical_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            UnaryLogicalOp::Not => "not_op",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("unary_logical_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("unary_logical_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("unary_logical_op_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: input_buffer.len(),
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute logical operation on GPU with broadcasting support
    pub fn execute_logical_op_with_broadcasting(
        input_a: &GpuBuffer<u8>,
        input_b: &GpuBuffer<u8>,
        operation: LogicalOp,
        shape_a: &[usize],
        shape_b: &[usize],
        output_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<u32>> {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("logical_broadcast_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        use wgpu::util::DeviceExt;
        let mut shape_metadata = Vec::new();
        shape_metadata.push(shape_a.len() as u32);
        shape_metadata.push(shape_b.len() as u32);
        shape_metadata.push(output_shape.len() as u32);
        shape_metadata.extend(shape_a.iter().map(|&x| x as u32));
        shape_metadata.extend(shape_b.iter().map(|&x| x as u32));
        shape_metadata.extend(output_shape.iter().map(|&x| x as u32));

        let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("shape_metadata"),
            contents: bytemuck::cast_slice(&shape_metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("logical_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("logical_broadcast_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout with shape metadata
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("logical_broadcast_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("logical_broadcast_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("logical_broadcast_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            LogicalOp::And => "and_broadcast",
            LogicalOp::Or => "or_broadcast",
            LogicalOp::Xor => "xor_broadcast",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("logical_broadcast_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("logical_broadcast_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("logical_broadcast_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute transpose operation on GPU
    pub fn execute_transpose<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
        permutation: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("transpose_output"),
            size: (input_buffer.len() * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct TransposeInfo {
            ndim: u32,
            total_size: u32,
            pad1: u32,
            pad2: u32,
        }

        let info = TransposeInfo {
            ndim: ndim as u32,
            total_size: input_buffer.len() as u32,
            pad1: 0,
            pad2: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("transpose_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();
        let permutation_u32: Vec<u32> = permutation.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements for array compatibility
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();
        let mut padded_permutation = permutation_u32.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
            padded_permutation.push(padded_permutation.len() as u32);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let permutation_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("permutation"),
            contents: bytemuck::cast_slice(&padded_permutation),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("transpose_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("transpose_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("transpose_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: permutation_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("transpose_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("transpose_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("transpose_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("transpose_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("transpose_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (input_buffer.len() + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: input_buffer.len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute concatenate operation on GPU
    pub fn execute_concatenate<T>(
        input1_buffer: &GpuBuffer<T>,
        input2_buffer: &GpuBuffer<T>,
        input1_shape: &[usize],
        input2_shape: &[usize],
        output_shape: &[usize],
        axis: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input1_buffer.device;
        let queue = &input1_buffer.queue;
        let ndim = output_shape.len();
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("concat_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info uniform buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Clone, Copy)]
        struct ConcatInfo {
            axis: u32,
            ndim: u32,
            num_inputs: u32,
            total_size: u32,
        }

        let info = ConcatInfo {
            axis: axis as u32,
            ndim: ndim as u32,
            num_inputs: 2u32,
            total_size: output_size as u32,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("concat_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&s| s as u32).collect();
        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("concat_output_shape"),
            contents: bytemuck::cast_slice(&output_shape_u32),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let input1_shape_u32: Vec<u32> = input1_shape.iter().map(|&s| s as u32).collect();
        let input1_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("concat_input1_shape"),
            contents: bytemuck::cast_slice(&input1_shape_u32),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let input2_shape_u32: Vec<u32> = input2_shape.iter().map(|&s| s as u32).collect();
        let input2_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("concat_input2_shape"),
            contents: bytemuck::cast_slice(&input2_shape_u32),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops2");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("concat_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("concat_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 6,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("concat_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input1_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input2_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: input1_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 6,
                    resource: input2_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create pipeline layout
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("concat_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("concat_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("concat_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("concat_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("concat_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input1_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input1_buffer.device),
            queue: Arc::clone(&input1_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute slice operation on GPU
    pub fn execute_slice<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
        slice_starts: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("slice_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct SliceInfo {
            ndim: u32,
            total_size: u32,
            pad1: u32,
            pad2: u32,
        }

        let info = SliceInfo {
            ndim: ndim as u32,
            total_size: output_size as u32,
            pad1: 0,
            pad2: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("slice_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();
        let slice_starts_u32: Vec<u32> = slice_starts.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();
        let mut padded_slice_starts = slice_starts_u32.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
            padded_slice_starts.push(0);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let slice_starts_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("slice_starts"),
            contents: bytemuck::cast_slice(&padded_slice_starts),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("slice_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("slice_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("slice_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: slice_starts_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("slice_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("slice_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("slice_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("slice_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("slice_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    #[derive(Debug, Clone, Copy)]
    pub enum PoolingOp {
        MaxPool2D,
        AvgPool2D,
        GlobalAvgPool2D,
        GlobalMaxPool2D,
        AdaptiveAvgPool2D,
        AdaptiveMaxPool2D,
        FractionalMaxPool2D,
        FractionalAvgPool2D,
        ROIPool2D,
        ROIAlign2D,
        MaxPool3D,
        AvgPool3D,
        GlobalMaxPool3D,
        GlobalAvgPool3D,
    }

    #[derive(Debug, Clone, Copy)]
    pub enum NormalizationOp {
        LayerNorm,
        BatchNormInference,
        BatchNormTraining,
        GroupNorm,
    }

    /// Execute a pooling operation on GPU
    pub fn execute_pooling_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: PoolingOp,
        input_shape: &[usize],
        output_shape: &[usize],
        kernel_size: (usize, usize),
        stride: (usize, usize),
        padding: (usize, usize),
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Validate input shape (expect NCHW format for 2D, NCDHW for 3D)
        let is_3d = matches!(
            operation,
            PoolingOp::MaxPool3D
                | PoolingOp::AvgPool3D
                | PoolingOp::GlobalMaxPool3D
                | PoolingOp::GlobalAvgPool3D
        );
        let expected_dims = if is_3d { 5 } else { 4 };

        if input_shape.len() != expected_dims || output_shape.len() != expected_dims {
            return Err(crate::TensorError::invalid_shape_simple(format!(
                "{}D pooling operations expect {}D tensors",
                if is_3d { 3 } else { 2 },
                expected_dims
            )));
        }

        let batch_size = input_shape[0];
        let channels = input_shape[1];
        let input_height = input_shape[2];
        let input_width = input_shape[3];

        let output_height = output_shape[2];
        let output_width = output_shape[3];
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("pooling_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create parameters buffer
        #[repr(C)]
        #[derive(Copy, Clone, bytemuck::Pod, bytemuck::Zeroable)]
        struct PoolingParams {
            batch_size: u32,
            channels: u32,
            input_height: u32,
            input_width: u32,
            output_height: u32,
            output_width: u32,
            kernel_height: u32,
            kernel_width: u32,
            stride_h: u32,
            stride_w: u32,
            pad_h: u32,
            pad_w: u32,
        }

        let params = PoolingParams {
            batch_size: batch_size as u32,
            channels: channels as u32,
            input_height: input_height as u32,
            input_width: input_width as u32,
            output_height: output_height as u32,
            output_width: output_width as u32,
            kernel_height: kernel_size.0 as u32,
            kernel_width: kernel_size.1 as u32,
            stride_h: stride.0 as u32,
            stride_w: stride.1 as u32,
            pad_h: padding.0 as u32,
            pad_w: padding.1 as u32,
        };

        let params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pooling_params"),
            contents: bytemuck::cast_slice(&[params]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shader module
        let shader_source = include_shader!("pooling_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("pooling_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("pooling_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("pooling_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: params_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("pooling_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            PoolingOp::MaxPool2D => "max_pool2d_kernel",
            PoolingOp::AvgPool2D => "avg_pool2d_kernel",
            PoolingOp::GlobalAvgPool2D => "global_avg_pool2d_kernel",
            PoolingOp::GlobalMaxPool2D => "global_max_pool2d_kernel",
            PoolingOp::AdaptiveAvgPool2D => "adaptive_avg_pool2d_kernel",
            PoolingOp::AdaptiveMaxPool2D => "adaptive_max_pool2d_kernel",
            PoolingOp::FractionalMaxPool2D => "fractional_max_pool2d_kernel",
            PoolingOp::FractionalAvgPool2D => "fractional_adaptive_pool2d",
            PoolingOp::ROIPool2D => "roi_pool2d_kernel",
            PoolingOp::ROIAlign2D => "roi_align2d_kernel",
            PoolingOp::MaxPool3D
            | PoolingOp::AvgPool3D
            | PoolingOp::GlobalMaxPool3D
            | PoolingOp::GlobalAvgPool3D => {
                return Err(crate::TensorError::UnsupportedOperation(format!(
                    "3D pooling operation {:?} requires WebGPU 3D shader implementation",
                    operation
                )));
            }
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("pooling_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("pooling_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("pooling_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch workgroups based on operation type
            match operation {
                PoolingOp::GlobalAvgPool2D | PoolingOp::GlobalMaxPool2D => {
                    // For global pooling, dispatch one thread per batch-channel
                    let workgroup_size = 256;
                    let num_workgroups =
                        (batch_size * channels + workgroup_size - 1) / workgroup_size;
                    compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
                }
                _ => {
                    // For spatial pooling, dispatch 2D workgroups
                    let workgroup_x = (output_width + 7) / 8;
                    let workgroup_y = (output_height + 7) / 8;
                    let workgroup_z = batch_size * channels;
                    compute_pass.dispatch_workgroups(
                        workgroup_x as u32,
                        workgroup_y as u32,
                        workgroup_z as u32,
                    );
                }
            }
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute ROI pooling operation on GPU
    /// This version handles ROI pooling which requires two input buffers: feature maps and ROIs
    pub fn execute_roi_pooling_op<T>(
        feature_buffer: &GpuBuffer<T>,
        rois_buffer: &GpuBuffer<T>,
        operation: PoolingOp,
        feature_shape: &[usize],
        output_shape: &[usize],
        pooled_size: (usize, usize),
        spatial_scale: f32,
        sampling_ratio: i32,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &feature_buffer.device;
        let queue = &feature_buffer.queue;

        // Validate input shapes
        if feature_shape.len() != 4 {
            return Err(crate::TensorError::invalid_shape_simple(
                "Feature maps must be 4D (NCHW format)".to_string(),
            ));
        }
        if output_shape.len() != 4 {
            return Err(crate::TensorError::invalid_shape_simple(
                "Output must be 4D (NCHW format)".to_string(),
            ));
        }

        let batch_size = feature_shape[0];
        let channels = feature_shape[1];
        let input_height = feature_shape[2];
        let input_width = feature_shape[3];

        let num_rois = output_shape[0];
        let pooled_height = pooled_size.0;
        let pooled_width = pooled_size.1;
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("roi_pooling_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create parameters buffer for ROI operations
        #[repr(C)]
        #[derive(Copy, Clone, bytemuck::Pod, bytemuck::Zeroable)]
        struct ROIParams {
            batch_size: u32,
            channels: u32,
            input_height: u32,
            input_width: u32,
            num_rois: u32,
            pooled_height: u32,
            pooled_width: u32,
            spatial_scale: f32,
            sampling_ratio: i32,
            _padding: [u32; 3], // Align to 16 bytes
        }

        let params = ROIParams {
            batch_size: batch_size as u32,
            channels: channels as u32,
            input_height: input_height as u32,
            input_width: input_width as u32,
            num_rois: num_rois as u32,
            pooled_height: pooled_height as u32,
            pooled_width: pooled_width as u32,
            spatial_scale,
            sampling_ratio,
            _padding: [0; 3],
        };

        let params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("roi_params"),
            contents: bytemuck::cast_slice(&[params]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shader module
        let shader_source = include_shader!("pooling_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("roi_pooling_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout (3 input buffers: features, rois, output, plus params)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("roi_pooling_bind_group_layout"),
            entries: &[
                // Feature maps buffer
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // ROIs buffer
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // Output buffer
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // Parameters buffer
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("roi_pooling_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: feature_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: rois_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: params_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("roi_pooling_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let entry_point = match operation {
            PoolingOp::ROIPool2D => "roi_pool2d_kernel",
            PoolingOp::ROIAlign2D => "roi_align2d_kernel",
            _ => {
                return Err(crate::TensorError::unsupported_operation_simple(format!(
                    "Operation {:?} is not a ROI pooling operation",
                    operation
                )))
            }
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("roi_pooling_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("roi_pooling_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("roi_pooling_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // For ROI pooling, dispatch workgroups based on output size
            let workgroup_x = (pooled_width + 7) / 8;
            let workgroup_y = (pooled_height + 7) / 8;
            let workgroup_z = num_rois * channels;
            compute_pass.dispatch_workgroups(
                workgroup_x as u32,
                workgroup_y as u32,
                workgroup_z as u32,
            );
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &feature_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&feature_buffer.device),
            queue: Arc::clone(&feature_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute fractional pooling operation on GPU
    /// This version handles the fractional scaling ratios for non-integer pooling operations
    pub fn execute_fractional_pooling_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: PoolingOp,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // For fractional pooling, we use the existing execute_pooling_op with dummy kernel/stride/padding
        // The actual fractional computation is handled in the shader using input/output dimensions
        execute_pooling_op(
            input_buffer,
            operation,
            input_shape,
            output_shape,
            (1, 1), // dummy kernel size - not used in fractional pooling shaders
            (1, 1), // dummy stride - not used in fractional pooling shaders
            (0, 0), // dummy padding - not used in fractional pooling shaders
        )
    }

    /// Execute pad operation on GPU
    pub fn execute_pad<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        padding: &[(usize, usize)],
        constant_value: f32,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();

        // Calculate output shape and size
        let output_shape: Vec<usize> = input_shape
            .iter()
            .zip(padding)
            .map(|(&dim, &(pad_before, pad_after))| dim + pad_before + pad_after)
            .collect();
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("pad_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct PadInfo {
            ndim: u32,
            total_size: u32,
            constant_value: f32,
            pad1: u32,
        }

        let info = PadInfo {
            ndim: ndim as u32,
            total_size: output_size as u32,
            constant_value,
            pad1: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pad_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape and padding buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();
        let pad_before: Vec<u32> = padding.iter().map(|&(before, _)| before as u32).collect();
        let pad_after: Vec<u32> = padding.iter().map(|&(_, after)| after as u32).collect();

        // Pad to at least 8 elements
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();
        let mut padded_pad_before = pad_before.clone();
        let mut padded_pad_after = pad_after.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
            padded_pad_before.push(0);
            padded_pad_after.push(0);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let pad_before_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pad_before"),
            contents: bytemuck::cast_slice(&padded_pad_before),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let pad_after_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("pad_after"),
            contents: bytemuck::cast_slice(&padded_pad_after),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("pad_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for pad operation (7 bindings)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("pad_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 6,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("pad_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: pad_before_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 6,
                    resource: pad_after_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("pad_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("pad_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("pad_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("pad_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("pad_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute tile operation on GPU
    pub fn execute_tile<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        multiples: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();

        // Calculate output shape
        let output_shape: Vec<usize> = input_shape
            .iter()
            .zip(multiples)
            .map(|(&dim, &mult)| dim * mult)
            .collect();
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("tile_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct TileInfo {
            ndim: u32,
            total_size: u32,
            pad1: u32,
            pad2: u32,
        }

        let info = TileInfo {
            ndim: ndim as u32,
            total_size: output_size as u32,
            pad1: 0,
            pad2: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("tile_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("tile_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("tile_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("tile_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("tile_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("tile_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("tile_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("tile_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("tile_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute repeat operation on GPU
    pub fn execute_repeat<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        repeats: usize,
        axis: Option<usize>,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();

        // Default axis is last dimension
        let repeat_axis = axis.unwrap_or(ndim - 1);
        if repeat_axis >= ndim {
            return Err(crate::TensorError::InvalidAxis(repeat_axis, ndim));
        }

        // Calculate output shape
        let mut output_shape = input_shape.to_vec();
        output_shape[repeat_axis] *= repeats;
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("repeat_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct RepeatInfo {
            ndim: u32,
            total_size: u32,
            repeats: u32,
            axis: u32,
        }

        let info = RepeatInfo {
            ndim: ndim as u32,
            total_size: output_size as u32,
            repeats: repeats as u32,
            axis: repeat_axis as u32,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("repeat_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("repeat_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("repeat_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("repeat_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("repeat_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("repeat_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("repeat_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("repeat_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("repeat_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute roll operation on GPU
    pub fn execute_roll<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        shift: isize,
        axis: Option<usize>,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_buffer.device;
        let queue = &input_buffer.queue;
        let ndim = input_shape.len();

        // Default axis is last dimension
        let roll_axis = axis.unwrap_or(ndim - 1);
        if roll_axis >= ndim {
            return Err(crate::TensorError::InvalidAxis(roll_axis, ndim));
        }

        let output_shape = input_shape;
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("roll_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct RollInfo {
            ndim: u32,
            total_size: u32,
            shift: i32,
            axis: u32,
        }

        let info = RollInfo {
            ndim: ndim as u32,
            total_size: output_size as u32,
            shift: shift as i32,
            axis: roll_axis as u32,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("roll_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements
        let mut padded_input_shape = input_shape_u32.clone();
        let mut padded_output_shape = output_shape_u32.clone();

        while padded_input_shape.len() < 8 {
            padded_input_shape.push(1);
            padded_output_shape.push(1);
        }

        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&padded_input_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&padded_output_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("roll_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("roll_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("roll_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("roll_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("roll_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("roll_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("roll_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("roll_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute gather operation on GPU
    pub fn execute_gather<T>(
        params_buffer: &GpuBuffer<T>,
        indices_buffer: &GpuBuffer<i32>,
        params_shape: &[usize],
        indices_shape: &[usize],
        axis: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &params_buffer.device;
        let queue = &params_buffer.queue;
        let params_ndim = params_shape.len();

        if axis >= params_ndim {
            return Err(crate::TensorError::InvalidAxis(axis, params_ndim));
        }

        // Calculate output shape: params_shape[:axis] + indices_shape + params_shape[axis+1:]
        let mut output_shape = Vec::new();
        output_shape.extend_from_slice(&params_shape[..axis]);
        output_shape.extend_from_slice(indices_shape);
        if axis + 1 < params_ndim {
            output_shape.extend_from_slice(&params_shape[axis + 1..]);
        }
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("gather_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct GatherInfo {
            ndim: u32,
            total_size: u32,
            axis: u32,
            pad1: u32,
        }

        let info = GatherInfo {
            ndim: params_ndim as u32,
            total_size: output_size as u32,
            axis: axis as u32,
            pad1: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("gather_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shape buffers
        let params_shape_u32: Vec<u32> = params_shape.iter().map(|&x| x as u32).collect();
        let indices_shape_u32: Vec<u32> = indices_shape.iter().map(|&x| x as u32).collect();

        // Pad to at least 8 elements
        let mut padded_params_shape = params_shape_u32.clone();
        let mut padded_indices_shape = indices_shape_u32.clone();

        while padded_params_shape.len() < 8 {
            padded_params_shape.push(1);
        }
        while padded_indices_shape.len() < 8 {
            padded_indices_shape.push(1);
        }

        let params_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("params_shape"),
            contents: bytemuck::cast_slice(&padded_params_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        let indices_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("indices_shape"),
            contents: bytemuck::cast_slice(&padded_indices_shape),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("gather_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("gather_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("gather_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: params_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: indices_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: params_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: indices_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("gather_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("gather_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("gather_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("gather_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("gather_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &params_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&params_buffer.device),
            queue: Arc::clone(&params_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute scatter operation on GPU
    pub fn execute_scatter<T>(
        tensor_buffer: &GpuBuffer<T>,
        indices_buffer: &GpuBuffer<i32>,
        updates_buffer: &GpuBuffer<T>,
        tensor_shape: &[usize],
        axis: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &tensor_buffer.device;
        let queue = &tensor_buffer.queue;
        let output_size: usize = tensor_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("scatter_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct ScatterInfo {
            axis: u32,
            ndim: u32,
            updates_size: u32,
            pad: u32,
        }

        let info = ScatterInfo {
            axis: axis as u32,
            ndim: tensor_shape.len() as u32,
            updates_size: indices_buffer.len as u32,
            pad: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("scatter_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create tensor shape buffer
        let tensor_shape_u32: Vec<u32> = tensor_shape.iter().map(|&x| x as u32).collect();
        let tensor_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("tensor_shape"),
            contents: bytemuck::cast_slice(&tensor_shape_u32),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Get updates shape from updates buffer size and tensor shape
        let updates_shape = tensor_shape; // For now, assume same shape - this might need adjustment
        let updates_shape_u32: Vec<u32> = updates_shape.iter().map(|&x| x as u32).collect();
        let updates_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("updates_shape"),
            contents: bytemuck::cast_slice(&updates_shape_u32),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops2");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("scatter_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("scatter_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 6,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("scatter_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: tensor_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: indices_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: updates_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: info_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: tensor_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 6,
                    resource: updates_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("scatter_pipeline"),
            layout: Some(
                &device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                    label: Some("scatter_pipeline_layout"),
                    bind_group_layouts: &[&bind_group_layout],
                    push_constant_ranges: &[],
                }),
            ),
            module: &shader_module,
            entry_point: Some("scatter_op"),
        });

        // Execute compute pass
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("scatter_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("scatter_pass"),
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Return the result buffer
        Ok(GpuBuffer {
            buffer: output_buffer,
            device: device.clone(),
            queue: queue.clone(),
            device_enum: tensor_buffer.device_enum,
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute where operation on GPU
    pub fn execute_where<T>(
        condition_buffer: &GpuBuffer<bool>,
        x_buffer: &GpuBuffer<T>,
        y_buffer: &GpuBuffer<T>,
        condition_shape: &[usize],
        x_shape: &[usize],
        y_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &condition_buffer.device;
        let queue = &condition_buffer.queue;
        let output_size: usize = output_shape.iter().product();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("where_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct WhereInfo {
            total_size: u32,
            pad1: u32,
            pad2: u32,
            pad3: u32,
        }

        let info = WhereInfo {
            total_size: output_size as u32,
            pad1: 0,
            pad2: 0,
            pad3: 0,
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("where_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Convert bool buffer to u32 buffer for GPU compatibility
        let condition_u32_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("condition_u32"),
            size: (output_size * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Copy and convert bool to u32
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("bool_to_u32_encoder"),
        });

        // For simplicity, assume condition buffer contains bool data that can be interpreted as u32
        // This is a simplified approach - in practice you might need a conversion shader
        encoder.copy_buffer_to_buffer(
            &condition_buffer.buffer,
            0,
            &condition_u32_buffer,
            0,
            (output_size * std::mem::size_of::<u32>()) as u64,
        );

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("where_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("where_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("where_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: condition_u32_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: x_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: y_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: info_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("where_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("where_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("where_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("where_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("where_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &condition_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&condition_buffer.device),
            queue: Arc::clone(&condition_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute fused operation on GPU for performance optimization
    pub fn execute_fused_op<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        operation: FusedOp,
        output_len: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("fused_op_output"),
            size: (output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shader module
        let shader_source = include_shader!("fused_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("fused_ops_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create params buffer
        use wgpu::util::DeviceExt;
        let mut broadcast_a = 0u32;
        let mut broadcast_b = 0u32;
        let size_a = input_a.len() as u32;
        let size_b = input_b.len() as u32;

        // Simple broadcasting logic
        if size_a == 1 && size_b > 1 {
            broadcast_a = 1;
        } else if size_b == 1 && size_a > 1 {
            broadcast_b = 1;
        }

        let params_data = match operation {
            FusedOp::AddLeakyRelu(negative_slope) => {
                vec![
                    output_len as u32,
                    broadcast_a,
                    broadcast_b,
                    size_a,
                    size_b,
                    f32::to_bits(negative_slope),
                ]
            }
            _ => {
                vec![output_len as u32, broadcast_a, broadcast_b, size_a, size_b]
            }
        };

        let params_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("fused_op_params"),
            contents: bytemuck::cast_slice(&params_data),
            usage: wgpu::BufferUsages::UNIFORM | wgpu::BufferUsages::COPY_DST,
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("fused_op_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("fused_op_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: params_buffer.as_entire_binding(),
                },
            ],
        });

        // Create pipeline layout
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("fused_op_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        // Get entry point based on operation
        let entry_point = match operation {
            FusedOp::AddRelu => "add_relu_kernel",
            FusedOp::AddSigmoid => "add_sigmoid_kernel",
            FusedOp::AddTanh => "add_tanh_kernel",
            FusedOp::AddGelu => "add_gelu_kernel",
            FusedOp::AddSwish => "add_swish_kernel",
            FusedOp::AddMish => "add_mish_kernel",
            FusedOp::AddLeakyRelu(_) => "add_leaky_relu_kernel",
            FusedOp::MulRelu => "mul_relu_kernel",
            FusedOp::MulSigmoid => "mul_sigmoid_kernel",
            FusedOp::MulTanh => "mul_tanh_kernel",
            FusedOp::LayerNormRelu => "layernorm_relu_kernel",
            FusedOp::LayerNormGelu => "layernorm_gelu_kernel",
        };

        // Create compute pipeline
        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("fused_op_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
        });

        // Execute the compute pass
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("fused_op_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("fused_op_pass"),
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch with appropriate workgroup size
            let workgroup_size = 256;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute binary operation on GPU with broadcasting support
    pub fn execute_binary_op_with_broadcasting<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        operation: BinaryOp,
        shape_a: &[usize],
        shape_b: &[usize],
        broadcast_shape: &[usize],
        output_len: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_a.device;
        let queue = &input_a.queue;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("binary_broadcast_output"),
            size: (output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer for broadcasting information
        use wgpu::util::DeviceExt;

        // Pack shape metadata: [a_rank, b_rank, output_rank, a_shape..., b_shape..., output_shape...]
        let mut shape_metadata = vec![
            shape_a.len() as u32,
            shape_b.len() as u32,
            broadcast_shape.len() as u32,
        ];

        // Add shape data
        shape_metadata.extend(shape_a.iter().map(|&x| x as u32));
        shape_metadata.extend(shape_b.iter().map(|&x| x as u32));
        shape_metadata.extend(broadcast_shape.iter().map(|&x| x as u32));

        let shape_metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("binary_broadcast_shape_metadata"),
            contents: bytemuck::cast_slice(&shape_metadata),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shader module
        let shader_source = get_binary_op_shader_source::<T>();
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("binary_broadcast_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for broadcasting
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("binary_broadcast_bind_group_layout"),
            entries: &[
                // Binding 0: input_a
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // Binding 1: input_b
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // Binding 2: output
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                // Binding 3: shape_metadata
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("binary_broadcast_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: shape_metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("binary_broadcast_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        // Use broadcasting-aware entry points
        let entry_point = match operation {
            BinaryOp::Add => "add_broadcast",
            BinaryOp::Sub => "sub_broadcast",
            BinaryOp::Mul => "mul_broadcast",
            BinaryOp::Div => "div_broadcast",
            BinaryOp::Pow => "pow_broadcast",
            BinaryOp::PReLU => "prelu_broadcast",
            BinaryOp::Min => "min_broadcast",
            BinaryOp::Max => "max_broadcast",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("binary_broadcast_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("binary_broadcast_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("binary_broadcast_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch based on output size
            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute reduction operation on GPU with axis support
    pub fn execute_axis_reduction_op<T>(
        input_buffer: &GpuBuffer<T>,
        operation: ReductionOp,
        input_shape: &[usize],
        axes: Option<&[i32]>,
        keepdims: bool,
        output_len: usize,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let device = &input_buffer.device;
        let queue = &input_buffer.queue;

        // Handle all-axis reduction (use existing implementation)
        if axes.is_none() {
            return execute_reduction_op(input_buffer, operation, output_len);
        }

        // For axis-specific reduction, we need more complex logic
        let axes = axes.unwrap();
        let input_rank = input_shape.len();

        // Normalize negative axes
        let normalized_axes: Result<Vec<usize>> = axes
            .iter()
            .map(|&axis| {
                let normalized = if axis < 0 {
                    input_rank as i32 + axis
                } else {
                    axis
                };
                if normalized < 0 || normalized >= input_rank as i32 {
                    Err(crate::TensorError::invalid_argument(format!(
                        "Axis {} out of range for tensor of rank {} and {}",
                        axis, axis, input_rank
                    )))
                } else {
                    Ok(normalized as usize)
                }
            })
            .collect();
        let normalized_axes = normalized_axes?;

        // Calculate output shape
        let mut output_shape = input_shape.to_vec();
        for &axis in normalized_axes.iter().rev() {
            if keepdims {
                output_shape[axis] = 1;
            } else {
                output_shape.remove(axis);
            }
        }

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("axis_reduction_output"),
            size: (output_len * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create enhanced metadata buffer for axis reduction
        use wgpu::util::DeviceExt;

        // Pack metadata: [input_size, output_size, input_rank, num_axes, axis0, axis1, ...]
        let mut metadata = vec![
            input_buffer.len() as u32,
            output_len as u32,
            input_rank as u32,
            normalized_axes.len() as u32,
        ];

        // Add axes (pad to max 8 axes)
        for &axis in &normalized_axes {
            metadata.push(axis as u32);
        }
        while metadata.len() < 12 {
            // 4 base + 8 max axes
            metadata.push(0u32);
        }

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("axis_reduction_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shape buffers
        let input_shape_u32: Vec<u32> = input_shape.iter().map(|&x| x as u32).collect();
        let input_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("input_shape"),
            contents: bytemuck::cast_slice(&input_shape_u32),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        let output_shape_u32: Vec<u32> = output_shape.iter().map(|&x| x as u32).collect();
        let output_shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("output_shape"),
            contents: bytemuck::cast_slice(&output_shape_u32),
            usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_DST,
        });

        // Create shader module with enhanced reduction ops
        const REDUCTION_SHADER: &str = include_shader!("reduction_ops");
        let shader_source = REDUCTION_SHADER;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("axis_reduction_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout for axis reduction
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("axis_reduction_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 3,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("axis_reduction_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: metadata_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 3,
                    resource: input_shape_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_shape_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("axis_reduction_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        // Use axis-specific entry points
        let entry_point = match operation {
            ReductionOp::Sum => "sum_axis_reduction",
            ReductionOp::Mean => "mean_axis_reduction",
            ReductionOp::Max => "max_axis_reduction",
            ReductionOp::Min => "min_axis_reduction",
            ReductionOp::ArgMax => "argmax_axis_reduction",
            ReductionOp::ArgMin => "argmin_axis_reduction",
            ReductionOp::All => "all_axis_reduction",
            ReductionOp::Any => "any_axis_reduction",
        };

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("axis_reduction_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some(entry_point),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("axis_reduction_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("axis_reduction_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // For axis reduction, dispatch based on output size
            let workgroup_size = 64;
            let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_buffer.device),
            queue: Arc::clone(&input_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute one_hot operation on GPU
    pub fn execute_one_hot<T>(
        indices_buffer: &GpuBuffer<i32>,
        indices_shape: &[usize],
        depth: usize,
        on_value: T,
        off_value: T,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &indices_buffer.device;
        let queue = &indices_buffer.queue;
        let indices_size: usize = indices_shape.iter().product();
        let output_size = indices_size * depth;

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("one_hot_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create info buffer
        #[repr(C)]
        #[derive(bytemuck::Pod, bytemuck::Zeroable, Copy, Clone)]
        struct OneHotInfo {
            total_size: u32,
            depth: u32,
            on_value: f32,
            off_value: f32,
        }

        let info = OneHotInfo {
            total_size: output_size as u32,
            depth: depth as u32,
            on_value: cast_to_f32(on_value),
            off_value: cast_to_f32(off_value),
        };

        let info_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("one_hot_info"),
            contents: bytemuck::cast_slice(&[info]),
            usage: wgpu::BufferUsages::UNIFORM,
        });

        // Create shader module
        let shader_source = include_shader!("manipulation_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("one_hot_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("one_hot_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Uniform,
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("one_hot_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: indices_buffer.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: info_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("one_hot_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("one_hot_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("one_hot_op"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("one_hot_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("one_hot_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &indices_buffer.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&indices_buffer.device),
            queue: Arc::clone(&indices_buffer.queue),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute comparison operations on GPU with broadcasting support
    pub fn execute_comparison<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        operation: &str,
        input_a_shape: &[usize],
        input_b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<u8>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_len = output_shape.iter().product::<usize>();

        // Create output buffer for boolean results (u8)
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Determine if broadcasting is needed
        let needs_broadcasting = input_a_shape != input_b_shape || input_a_shape != output_shape;

        if needs_broadcasting {
            // Create shape metadata for broadcasting
            let mut shape_metadata = Vec::new();
            shape_metadata.push(input_a_shape.len() as u32);
            shape_metadata.push(input_b_shape.len() as u32);
            shape_metadata.push(output_shape.len() as u32);

            // Add shapes (pad to consistent size)
            for &dim in input_a_shape {
                shape_metadata.push(dim as u32);
            }
            for &dim in input_b_shape {
                shape_metadata.push(dim as u32);
            }
            for &dim in output_shape {
                shape_metadata.push(dim as u32);
            }

            let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("comparison_shape_metadata"),
                contents: bytemuck::cast_slice(&shape_metadata),
                usage: wgpu::BufferUsages::STORAGE,
            });

            // Create shader module
            let shader_source = include_shader!("comparison_ops");
            let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("comparison_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

            // Create bind group layout
            let bind_group_layout =
                device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("comparison_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 21,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

            // Create bind group
            let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("comparison_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: input_a.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: input_b.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 21,
                        resource: shape_buffer.as_entire_binding(),
                    },
                ],
            });

            // Determine entry point based on operation and type
            let type_suffix = if std::mem::size_of::<T>() == 4 {
                if std::any::type_name::<T>().contains("f32") {
                    "f32"
                } else if std::any::type_name::<T>().contains("i32") {
                    "i32"
                } else {
                    "u32"
                }
            } else if std::mem::size_of::<T>() == 8 {
                if std::any::type_name::<T>().contains("f64") {
                    "f64"
                } else if std::any::type_name::<T>().contains("i64") {
                    "i64"
                } else {
                    "u64"
                }
            } else {
                return Err(crate::TensorError::unsupported_operation_simple(format!(
                    "Unsupported type for GPU comparison: {}",
                    std::any::type_name::<T>()
                )));
            };

            let entry_point = format!("{}_{}_{}", operation, type_suffix, "broadcast");

            // Create compute pipeline
            let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                label: Some("comparison_pipeline_layout"),
                bind_group_layouts: &[&bind_group_layout],
                push_constant_ranges: &[],
            });

            let compute_pipeline =
                device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("comparison_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

            // Dispatch compute shader
            let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("comparison_encoder"),
            });

            {
                let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                    label: Some("comparison_pass"),
                    timestamp_writes: None,
                });

                compute_pass.set_pipeline(&compute_pipeline);
                compute_pass.set_bind_group(0, &bind_group, &[]);

                let workgroup_size = 64;
                let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
                compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
            }

            queue.submit(std::iter::once(encoder.finish()));
            device.poll(wgpu::Maintain::Wait);
        } else {
            // Simple non-broadcasting case
            let shader_source = include_shader!("comparison_ops");
            let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("comparison_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

            // Create bind group layout (simpler for non-broadcasting)
            let bind_group_layout =
                device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("comparison_simple_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

            // Create bind group
            let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("comparison_simple_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: input_a.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: input_b.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                ],
            });

            // Determine entry point
            let type_suffix = if std::mem::size_of::<T>() == 4 {
                if std::any::type_name::<T>().contains("f32") {
                    "f32"
                } else if std::any::type_name::<T>().contains("i32") {
                    "i32"
                } else {
                    "u32"
                }
            } else if std::mem::size_of::<T>() == 8 {
                if std::any::type_name::<T>().contains("f64") {
                    "f64"
                } else if std::any::type_name::<T>().contains("i64") {
                    "i64"
                } else {
                    "u64"
                }
            } else {
                return Err(crate::TensorError::unsupported_operation_simple(format!(
                    "Unsupported type for GPU comparison: {}",
                    std::any::type_name::<T>()
                )));
            };

            let entry_point = format!("{}_{}", operation, type_suffix);

            // Create compute pipeline
            let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                label: Some("comparison_simple_pipeline_layout"),
                bind_group_layouts: &[&bind_group_layout],
                push_constant_ranges: &[],
            });

            let compute_pipeline =
                device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("comparison_simple_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

            // Dispatch compute shader
            let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("comparison_simple_encoder"),
            });

            {
                let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                    label: Some("comparison_simple_pass"),
                    timestamp_writes: None,
                });

                compute_pass.set_pipeline(&compute_pipeline);
                compute_pass.set_bind_group(0, &bind_group, &[]);

                let workgroup_size = 64;
                let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
                compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
            }

            queue.submit(std::iter::once(encoder.finish()));
            device.poll(wgpu::Maintain::Wait);
        }

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute logical operations on GPU with broadcasting support
    pub fn execute_logical(
        input_a: &GpuBuffer<u8>,
        input_b: &GpuBuffer<u8>,
        operation: &str,
        input_a_shape: &[usize],
        input_b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<u8>> {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_len = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("logical_output"),
            size: (output_len * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Determine if broadcasting is needed
        let needs_broadcasting = input_a_shape != input_b_shape || input_a_shape != output_shape;

        if needs_broadcasting {
            // Create shape metadata for broadcasting
            let mut shape_metadata = Vec::new();
            shape_metadata.push(input_a_shape.len() as u32);
            shape_metadata.push(input_b_shape.len() as u32);
            shape_metadata.push(output_shape.len() as u32);

            // Add shapes
            for &dim in input_a_shape {
                shape_metadata.push(dim as u32);
            }
            for &dim in input_b_shape {
                shape_metadata.push(dim as u32);
            }
            for &dim in output_shape {
                shape_metadata.push(dim as u32);
            }

            let shape_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("logical_shape_metadata"),
                contents: bytemuck::cast_slice(&shape_metadata),
                usage: wgpu::BufferUsages::STORAGE,
            });

            // Create shader module
            let shader_source = include_shader!("logical_ops");
            let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("logical_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

            // Create bind group layout
            let bind_group_layout =
                device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("logical_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 3,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

            // Create bind group
            let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("logical_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: input_a.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: input_b.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 3,
                        resource: shape_buffer.as_entire_binding(),
                    },
                ],
            });

            let entry_point = format!("{}_{}", operation, "broadcast");

            // Create compute pipeline
            let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                label: Some("logical_pipeline_layout"),
                bind_group_layouts: &[&bind_group_layout],
                push_constant_ranges: &[],
            });

            let compute_pipeline =
                device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("logical_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

            // Dispatch compute shader
            let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("logical_encoder"),
            });

            {
                let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                    label: Some("logical_pass"),
                    timestamp_writes: None,
                });

                compute_pass.set_pipeline(&compute_pipeline);
                compute_pass.set_bind_group(0, &bind_group, &[]);

                let workgroup_size = 64;
                let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
                compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
            }

            queue.submit(std::iter::once(encoder.finish()));
            device.poll(wgpu::Maintain::Wait);
        } else {
            // Simple non-broadcasting case
            let shader_source = include_shader!("logical_ops");
            let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("logical_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

            // Create bind group layout
            let bind_group_layout =
                device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("logical_simple_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

            // Create bind group
            let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("logical_simple_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: input_a.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: input_b.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                ],
            });

            let entry_point = format!("{}_{}", operation, "op");

            // Create compute pipeline
            let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                label: Some("logical_simple_pipeline_layout"),
                bind_group_layouts: &[&bind_group_layout],
                push_constant_ranges: &[],
            });

            let compute_pipeline =
                device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("logical_simple_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

            // Dispatch compute shader
            let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("logical_simple_encoder"),
            });

            {
                let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                    label: Some("logical_simple_pass"),
                    timestamp_writes: None,
                });

                compute_pass.set_pipeline(&compute_pipeline);
                compute_pass.set_bind_group(0, &bind_group, &[]);

                let workgroup_size = 64;
                let num_workgroups = (output_len + workgroup_size - 1) / workgroup_size;
                compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
            }

            queue.submit(std::iter::once(encoder.finish()));
            device.poll(wgpu::Maintain::Wait);
        }

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: Arc::clone(&input_a.device),
            queue: Arc::clone(&input_a.queue),
            device_enum: Device::Gpu(device_id),
            len: output_len,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute random normal distribution generation on GPU
    pub fn execute_random_normal(
        shape: &[usize],
        mean: f32,
        std: f32,
        seed: u64,
    ) -> Result<GpuBuffer<f32>> {
        use wgpu::util::DeviceExt;

        let total_elements = shape.iter().product::<usize>();
        let device_id = 0; // Default to GPU 0
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;

        // Create output buffer
        let output_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("random_normal_output"),
            size: (total_elements * std::mem::size_of::<f32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create parameter buffer [mean, std, seed_low, seed_high]
        let seed_low = seed as u32;
        let seed_high = (seed >> 32) as u32;
        let params = [
            mean,
            std,
            f32::from_bits(seed_low),
            f32::from_bits(seed_high),
        ];
        let params_buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("random_normal_params"),
                contents: bytemuck::cast_slice(&params),
                usage: wgpu::BufferUsages::STORAGE,
            });

        // Load shader
        let shader_source = include_shader!("random_ops");
        let shader_module = gpu_ctx
            .device
            .create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("random_ops_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

        // Create bind group layout
        let bind_group_layout =
            gpu_ctx
                .device
                .create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("random_normal_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

        // Create bind group
        let bind_group = gpu_ctx
            .device
            .create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("random_normal_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: params_buffer.as_entire_binding(),
                    },
                ],
            });

        // Create compute pipeline
        let pipeline_layout =
            gpu_ctx
                .device
                .create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                    label: Some("random_normal_pipeline_layout"),
                    bind_group_layouts: &[&bind_group_layout],
                    push_constant_ranges: &[],
                });

        let compute_pipeline =
            gpu_ctx
                .device
                .create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("random_normal_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some("random_normal"),
                    compilation_options: Default::default(),
                    cache: None,
                });

        // Dispatch compute shader
        let mut encoder = gpu_ctx
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("random_normal_encoder"),
            });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("random_normal_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (total_elements + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        gpu_ctx.queue.submit(std::iter::once(encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        Ok(GpuBuffer {
            buffer: output_buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: total_elements,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute random uniform distribution generation on GPU
    pub fn execute_random_uniform(
        shape: &[usize],
        min: f32,
        max: f32,
        seed: u64,
    ) -> Result<GpuBuffer<f32>> {
        use wgpu::util::DeviceExt;

        let total_elements = shape.iter().product::<usize>();
        let device_id = 0; // Default to GPU 0
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;

        // Create output buffer
        let output_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("random_uniform_output"),
            size: (total_elements * std::mem::size_of::<f32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create parameter buffer [min, max, seed_low, seed_high]
        let seed_low = seed as u32;
        let seed_high = (seed >> 32) as u32;
        let params = [
            min,
            max,
            f32::from_bits(seed_low),
            f32::from_bits(seed_high),
        ];
        let params_buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("random_uniform_params"),
                contents: bytemuck::cast_slice(&params),
                usage: wgpu::BufferUsages::STORAGE,
            });

        // Load shader
        let shader_source = include_shader!("random_ops");
        let shader_module = gpu_ctx
            .device
            .create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("random_ops_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_source.into()),
            });

        // Create bind group layout
        let bind_group_layout =
            gpu_ctx
                .device
                .create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("random_uniform_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

        // Create bind group
        let bind_group = gpu_ctx
            .device
            .create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("random_uniform_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: params_buffer.as_entire_binding(),
                    },
                ],
            });

        // Create compute pipeline
        let pipeline_layout =
            gpu_ctx
                .device
                .create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                    label: Some("random_uniform_pipeline_layout"),
                    bind_group_layouts: &[&bind_group_layout],
                    push_constant_ranges: &[],
                });

        let compute_pipeline =
            gpu_ctx
                .device
                .create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("random_uniform_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some("random_uniform"),
                    compilation_options: Default::default(),
                    cache: None,
                });

        // Dispatch compute shader
        let mut encoder = gpu_ctx
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("random_uniform_encoder"),
            });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("random_uniform_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (total_elements + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        gpu_ctx.queue.submit(std::iter::once(encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        Ok(GpuBuffer {
            buffer: output_buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: total_elements,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute tensor permutation operation on GPU
    pub fn execute_tensor_permutation<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        axes: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // For now, fallback to CPU implementation
        let cpu_array = input_buffer.to_cpu_array()?;
        let mut permuted = cpu_array.clone();
        permuted.permuted_axes(axes.iter().cloned());
        
        // Convert back to GPU
        let device_id = match &input_buffer.device_enum {
            crate::Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        GpuBuffer::from_cpu_array(&permuted, device_id)
    }

    /// Execute einsum matrix multiplication on GPU
    pub fn execute_einsum_matmul<T>(
        a_buffer: &GpuBuffer<T>,
        b_buffer: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to basic matmul for now
        execute_binary_op(a_buffer, b_buffer, crate::ops::BinaryOp::Mul)
    }

    /// Execute einsum batched matrix multiplication on GPU
    pub fn execute_einsum_batched_matmul<T>(
        a_buffer: &GpuBuffer<T>,
        b_buffer: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to basic operation for now
        execute_binary_op(a_buffer, b_buffer, crate::ops::BinaryOp::Mul)
    }

    /// Execute einsum transpose operation on GPU
    pub fn execute_einsum_transpose<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to permutation with default transpose axes
        let axes: Vec<usize> = (0..input_shape.len()).rev().collect();
        execute_tensor_permutation(input_buffer, input_shape, &axes, output_shape)
    }

    /// Execute einsum diagonal operation on GPU
    pub fn execute_einsum_diagonal<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to CPU implementation for complex diagonal operations
        let cpu_array = input_buffer.to_cpu_array()?;
        let device_id = match &input_buffer.device_enum {
            crate::Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };
        
        // Create a simple diagonal extraction (simplified)
        GpuBuffer::from_cpu_array(&cpu_array, device_id)
    }

    /// Execute einsum outer product operation on GPU
    pub fn execute_einsum_outer_product<T>(
        a_buffer: &GpuBuffer<T>,
        b_buffer: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback implementation using element-wise operations
        execute_binary_op(a_buffer, b_buffer, crate::ops::BinaryOp::Mul)
    }

    /// Execute einsum vector dot product on GPU
    pub fn execute_einsum_vector_dot<T>(
        a_buffer: &GpuBuffer<T>,
        b_buffer: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to element-wise multiplication
        execute_binary_op(a_buffer, b_buffer, crate::ops::BinaryOp::Mul)
    }

    /// Execute binary scalar operation on GPU
    pub fn execute_binary_scalar_op<T>(
        input_buffer: &GpuBuffer<T>,
        scalar: T,
        operation: crate::gpu::BinaryScalarOp,
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Create a scalar buffer and use regular binary operation
        let device_id = match &input_buffer.device_enum {
            crate::Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };
        
        let scalar_buffer = GpuBuffer::from_slice(&[scalar], &input_buffer.device_enum)?;
        let binary_op = match operation {
            crate::gpu::BinaryScalarOp::Add => crate::ops::BinaryOp::Add,
            crate::gpu::BinaryScalarOp::Sub => crate::ops::BinaryOp::Sub,
            crate::gpu::BinaryScalarOp::Mul => crate::ops::BinaryOp::Mul,
            crate::gpu::BinaryScalarOp::Div => crate::ops::BinaryOp::Div,
            crate::gpu::BinaryScalarOp::Pow => crate::ops::BinaryOp::Pow,
        };
        
        execute_binary_op(input_buffer, &scalar_buffer, binary_op)
    }

    /// Execute einsum trace operation on GPU
    pub fn execute_einsum_trace<T>(
        input_buffer: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        // Fallback to CPU implementation for complex trace operations
        let cpu_array = input_buffer.to_cpu_array()?;
        let device_id = match &input_buffer.device_enum {
            crate::Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };
        
        // Create a simple trace extraction (simplified)
        GpuBuffer::from_cpu_array(&cpu_array, device_id)
    }
}

#[cfg(feature = "gpu")]
/// GPU operations execution infrastructure
pub struct GpuOps;

#[cfg(feature = "gpu")]
impl GpuOps {
    /// Execute comparison operations on GPU
    pub fn execute_comparison<T>(
        gpu_a: &GpuBuffer<T>,
        gpu_b: &GpuBuffer<T>,
        op_name: &str,
        a_shape: &[usize],
        b_shape: &[usize],
        broadcast_shape: &[usize],
    ) -> Result<GpuBuffer<u8>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let total_elements = broadcast_shape.iter().product::<usize>();
        let device_id = 0; // Default to GPU 0
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;

        // Create output buffer for boolean results (stored as u32, converted to u8)
        let output_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_output"),
            size: (total_elements * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        let mut shape_metadata = Vec::new();
        shape_metadata.push(a_shape.len() as u32);
        shape_metadata.push(b_shape.len() as u32);
        shape_metadata.push(broadcast_shape.len() as u32);

        // Add shape data
        for &dim in a_shape {
            shape_metadata.push(dim as u32);
        }
        for &dim in b_shape {
            shape_metadata.push(dim as u32);
        }
        for &dim in broadcast_shape {
            shape_metadata.push(dim as u32);
        }

        let shape_buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("shape_metadata"),
                contents: bytemuck::cast_slice(&shape_metadata),
                usage: wgpu::BufferUsages::STORAGE,
            });

        // Determine the correct shader and entry point based on operation and type
        let (shader_content, entry_point) =
            Self::get_comparison_shader_info::<T>(op_name, a_shape, b_shape, broadcast_shape)?;

        // Create shader module
        let shader_module = gpu_ctx
            .device
            .create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("comparison_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_content.into()),
            });

        // Create bind group layout
        let bind_group_layout =
            gpu_ctx
                .device
                .create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("comparison_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 21,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

        // Create bind group
        let bind_group = gpu_ctx
            .device
            .create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("comparison_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: gpu_a.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: gpu_b.buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 21,
                        resource: shape_buffer.as_entire_binding(),
                    },
                ],
            });

        // Create compute pipeline
        let pipeline_layout =
            gpu_ctx
                .device
                .create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                    label: Some("comparison_pipeline_layout"),
                    bind_group_layouts: &[&bind_group_layout],
                    push_constant_ranges: &[],
                });

        let compute_pipeline =
            gpu_ctx
                .device
                .create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("comparison_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

        // Dispatch compute shader
        let mut encoder = gpu_ctx
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("comparison_encoder"),
            });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("comparison_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (total_elements + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        gpu_ctx.queue.submit(std::iter::once(encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create final buffer with u8 results
        let final_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("comparison_final_output"),
            size: (total_elements * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Convert u32 to u8 - this is a simplification, in reality we'd need a conversion kernel
        let mut converter_encoder =
            gpu_ctx
                .device
                .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                    label: Some("converter_encoder"),
                });

        // For now, we'll just copy the first byte of each u32 (this is a simplification)
        // In a real implementation, we'd create a separate conversion kernel
        converter_encoder.copy_buffer_to_buffer(
            &output_buffer,
            0,
            &final_buffer,
            0,
            (total_elements * std::mem::size_of::<u8>()) as u64,
        );

        gpu_ctx
            .queue
            .submit(std::iter::once(converter_encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        Ok(GpuBuffer {
            buffer: final_buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: total_elements,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Get shader information for comparison operations
    fn get_comparison_shader_info<T>(
        op_name: &str,
        a_shape: &[usize],
        b_shape: &[usize],
        broadcast_shape: &[usize],
    ) -> Result<(String, String)>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        let shader_content = include_shader!("comparison_ops");

        // Determine if we need broadcasting
        let needs_broadcast = a_shape != broadcast_shape || b_shape != broadcast_shape;

        // Get type suffix
        let type_suffix = if std::any::TypeId::of::<T>() == std::any::TypeId::of::<f32>() {
            "f32"
        } else if std::any::TypeId::of::<T>() == std::any::TypeId::of::<f64>() {
            "f64"
        } else if std::any::TypeId::of::<T>() == std::any::TypeId::of::<i32>() {
            "i32"
        } else if std::any::TypeId::of::<T>() == std::any::TypeId::of::<i64>() {
            "i64"
        } else if std::any::TypeId::of::<T>() == std::any::TypeId::of::<u32>() {
            "u32"
        } else if std::any::TypeId::of::<T>() == std::any::TypeId::of::<u64>() {
            "u64"
        } else {
            return Err(TensorError::unsupported_operation_simple(format!(
                "Unsupported type for comparison operation: {:?}",
                std::any::type_name::<T>()
            )));
        };

        // Build entry point name
        let entry_point = if needs_broadcast {
            format!("{}_{}_{}", op_name, type_suffix, "broadcast")
        } else {
            format!("{}_{}", op_name, type_suffix)
        };

        Ok((shader_content.to_string(), entry_point))
    }

    /// Execute logical operations on GPU
    pub fn execute_logical(
        gpu_a: &GpuBuffer<u8>,
        gpu_b: &GpuBuffer<u8>,
        op_name: &str,
        a_shape: &[usize],
        b_shape: &[usize],
        broadcast_shape: &[usize],
    ) -> Result<GpuBuffer<u8>> {
        use wgpu::util::DeviceExt;

        let total_elements = broadcast_shape.iter().product::<usize>();
        let device_id = 0; // Default to GPU 0
        let gpu_ctx = super::device::context::get_gpu_context(device_id)?;

        // Create output buffer for boolean results (stored as u32, converted to u8)
        let output_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("logical_output"),
            size: (total_elements * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create shape metadata buffer
        let mut shape_metadata = Vec::new();
        shape_metadata.push(a_shape.len() as u32);
        shape_metadata.push(b_shape.len() as u32);
        shape_metadata.push(broadcast_shape.len() as u32);

        // Add shape data
        for &dim in a_shape {
            shape_metadata.push(dim as u32);
        }
        for &dim in b_shape {
            shape_metadata.push(dim as u32);
        }
        for &dim in broadcast_shape {
            shape_metadata.push(dim as u32);
        }

        let shape_buffer = gpu_ctx
            .device
            .create_buffer_init(&wgpu::util::BufferInitDescriptor {
                label: Some("logical_shape_metadata"),
                contents: bytemuck::cast_slice(&shape_metadata),
                usage: wgpu::BufferUsages::STORAGE,
            });

        // Convert u8 buffers to u32 format for GPU processing
        let a_u32_buffer = Self::convert_u8_to_u32_buffer(gpu_a, &gpu_ctx)?;
        let b_u32_buffer = Self::convert_u8_to_u32_buffer(gpu_b, &gpu_ctx)?;

        // Determine the correct shader and entry point based on operation
        let (shader_content, entry_point) =
            Self::get_logical_shader_info(op_name, a_shape, b_shape, broadcast_shape)?;

        // Create shader module
        let shader_module = gpu_ctx
            .device
            .create_shader_module(wgpu::ShaderModuleDescriptor {
                label: Some("logical_shader"),
                source: wgpu::ShaderSource::Wgsl(shader_content.into()),
            });

        // Create bind group layout
        let bind_group_layout =
            gpu_ctx
                .device
                .create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
                    label: Some("logical_bind_group_layout"),
                    entries: &[
                        wgpu::BindGroupLayoutEntry {
                            binding: 0,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 1,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 2,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: false },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                        wgpu::BindGroupLayoutEntry {
                            binding: 3,
                            visibility: wgpu::ShaderStages::COMPUTE,
                            ty: wgpu::BindingType::Buffer {
                                ty: wgpu::BufferBindingType::Storage { read_only: true },
                                has_dynamic_offset: false,
                                min_binding_size: None,
                            },
                            count: None,
                        },
                    ],
                });

        // Create bind group
        let bind_group = gpu_ctx
            .device
            .create_bind_group(&wgpu::BindGroupDescriptor {
                label: Some("logical_bind_group"),
                layout: &bind_group_layout,
                entries: &[
                    wgpu::BindGroupEntry {
                        binding: 0,
                        resource: a_u32_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 1,
                        resource: b_u32_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 2,
                        resource: output_buffer.as_entire_binding(),
                    },
                    wgpu::BindGroupEntry {
                        binding: 3,
                        resource: shape_buffer.as_entire_binding(),
                    },
                ],
            });

        // Create compute pipeline
        let pipeline_layout =
            gpu_ctx
                .device
                .create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
                    label: Some("logical_pipeline_layout"),
                    bind_group_layouts: &[&bind_group_layout],
                    push_constant_ranges: &[],
                });

        let compute_pipeline =
            gpu_ctx
                .device
                .create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
                    label: Some("logical_pipeline"),
                    layout: Some(&pipeline_layout),
                    module: &shader_module,
                    entry_point: Some(&entry_point),
                    compilation_options: Default::default(),
                    cache: None,
                });

        // Dispatch compute shader
        let mut encoder = gpu_ctx
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("logical_encoder"),
            });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("logical_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            let workgroup_size = 64;
            let num_workgroups = (total_elements + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        gpu_ctx.queue.submit(std::iter::once(encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create final buffer with u8 results
        let final_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("logical_final_output"),
            size: (total_elements * std::mem::size_of::<u8>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Convert u32 to u8 - this is a simplification, in reality we'd need a conversion kernel
        let mut converter_encoder =
            gpu_ctx
                .device
                .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                    label: Some("logical_converter_encoder"),
                });

        // For now, we'll just copy the first byte of each u32 (this is a simplification)
        // In a real implementation, we'd create a separate conversion kernel
        converter_encoder.copy_buffer_to_buffer(
            &output_buffer,
            0,
            &final_buffer,
            0,
            (total_elements * std::mem::size_of::<u8>()) as u64,
        );

        gpu_ctx
            .queue
            .submit(std::iter::once(converter_encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        Ok(GpuBuffer {
            buffer: final_buffer,
            device: gpu_ctx.device.clone(),
            queue: gpu_ctx.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: total_elements,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Get shader information for logical operations
    fn get_logical_shader_info(
        op_name: &str,
        a_shape: &[usize],
        b_shape: &[usize],
        broadcast_shape: &[usize],
    ) -> Result<(String, String)> {
        let shader_content = include_str!(concat!("gpu/shaders/logical_ops.", "wgsl"));

        // Determine if we need broadcasting
        let needs_broadcast = a_shape != broadcast_shape || b_shape != broadcast_shape;

        // Build entry point name
        let entry_point = if needs_broadcast {
            format!("{}_{}", op_name, "broadcast")
        } else {
            format!("{}_{}", op_name, "op")
        };

        Ok((shader_content.to_string(), entry_point))
    }

    /// Convert u8 buffer to u32 buffer for GPU processing
    fn convert_u8_to_u32_buffer(
        u8_buffer: &GpuBuffer<u8>,
        gpu_ctx: &super::device::context::GpuContext,
    ) -> Result<wgpu::Buffer> {
        // Create a u32 buffer with the same number of elements
        let u32_buffer = gpu_ctx.device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("u8_to_u32_conversion"),
            size: (u8_buffer.len * std::mem::size_of::<u32>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // For simplicity, we'll just copy the u8 data and pad with zeros
        // In a real implementation, we'd use a compute shader for proper conversion
        let mut encoder = gpu_ctx
            .device
            .create_command_encoder(&wgpu::CommandEncoderDescriptor {
                label: Some("u8_to_u32_encoder"),
            });

        // Copy u8 data to u32 buffer (this is a simplification)
        encoder.copy_buffer_to_buffer(
            &u8_buffer.buffer,
            0,
            &u32_buffer,
            0,
            (u8_buffer.len * std::mem::size_of::<u8>()) as u64,
        );

        gpu_ctx.queue.submit(std::iter::once(encoder.finish()));
        gpu_ctx.device.poll(wgpu::Maintain::Wait);

        Ok(u32_buffer)
    }

    /// Execute Einstein summation matrix multiplication on GPU
    /// Implements ij,jk -> ik pattern (matrix multiplication)
    pub fn execute_einsum_matmul<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_size = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_matmul_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            a_shape[0] as u32, // M
            a_shape[1] as u32, // N (shared dimension)
            b_shape[1] as u32, // K
            0u32,              // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_matmul_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_matmul_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_matmul_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_matmul_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_matmul_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_matmul_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("matmul_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_matmul_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_matmul_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 2D workgroups for matrix multiplication
            let workgroup_x = (output_shape[1] + 15) / 16;
            let workgroup_y = (output_shape[0] + 15) / 16;
            compute_pass.dispatch_workgroups(workgroup_x as u32, workgroup_y as u32, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation batched matrix multiplication on GPU  
    /// Implements bij,bjk -> bik pattern (batched matrix multiplication)
    pub fn execute_einsum_batched_matmul<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_size = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_batched_matmul_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            a_shape[0] as u32, // B (batch size)
            a_shape[1] as u32, // M
            a_shape[2] as u32, // N (shared dimension)
            b_shape[2] as u32, // K
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_batched_matmul_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_batched_matmul_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout (same as regular matmul)
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_batched_matmul_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_batched_matmul_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_batched_matmul_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_batched_matmul_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("batched_matmul_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_batched_matmul_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_batched_matmul_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 3D workgroups for batched matrix multiplication
            let workgroup_x = (output_shape[0] + 7) / 8; // Batch dimension
            let workgroup_y = (output_shape[1] + 7) / 8; // M dimension
            let workgroup_z = (output_shape[2] + 3) / 4; // K dimension
            compute_pass.dispatch_workgroups(
                workgroup_x as u32,
                workgroup_y as u32,
                workgroup_z as u32,
            );
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation transpose on GPU
    /// Implements ij -> ji pattern (matrix transpose)
    pub fn execute_einsum_transpose<T>(
        input_tensor: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_tensor.device;
        let queue = &input_tensor.queue;
        let output_size = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_transpose_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            input_shape[0] as u32, // Original rows
            input_shape[1] as u32, // Original cols
            0u32,                  // Padding
            0u32,                  // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_transpose_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_transpose_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_transpose_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_transpose_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_tensor.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_transpose_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_transpose_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("transpose_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_transpose_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_transpose_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 2D workgroups for transpose
            let workgroup_x = (input_shape[0] + 15) / 16;
            let workgroup_y = (input_shape[1] + 15) / 16;
            compute_pass.dispatch_workgroups(workgroup_x as u32, workgroup_y as u32, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_tensor.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_tensor.device.clone(),
            queue: input_tensor.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation diagonal extraction on GPU
    /// Implements ii -> i pattern (diagonal extraction)
    pub fn execute_einsum_diagonal<T>(
        input_tensor: &GpuBuffer<T>,
        input_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_tensor.device;
        let queue = &input_tensor.queue;
        let output_size = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_diagonal_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            input_shape[0] as u32, // Matrix dimension (assuming square)
            0u32,                  // Padding
            0u32,                  // Padding
            0u32,                  // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_diagonal_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_diagonal_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_diagonal_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_diagonal_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_tensor.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_diagonal_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_diagonal_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("diagonal_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_diagonal_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_diagonal_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 1D workgroups for diagonal extraction
            let workgroup_size = 64;
            let num_workgroups = (output_size + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_tensor.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_tensor.device.clone(),
            queue: input_tensor.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation outer product on GPU
    /// Implements i,j -> ij pattern (outer product)
    pub fn execute_einsum_outer_product<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        a_shape: &[usize],
        b_shape: &[usize],
        output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_size = output_shape.iter().product::<usize>();

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_outer_product_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            a_shape[0] as u32, // M (first vector size)
            b_shape[0] as u32, // N (second vector size)
            0u32,              // Padding
            0u32,              // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_outer_product_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_outer_product_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_outer_product_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_outer_product_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_outer_product_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_outer_product_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("outer_product_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_outer_product_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_outer_product_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 2D workgroups for outer product
            let workgroup_x = (a_shape[0] + 15) / 16;
            let workgroup_y = (b_shape[0] + 15) / 16;
            compute_pass.dispatch_workgroups(workgroup_x as u32, workgroup_y as u32, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation vector dot product on GPU
    /// Implements "i,i->" pattern
    pub fn execute_einsum_vector_dot<T>(
        input_a: &GpuBuffer<T>,
        input_b: &GpuBuffer<T>,
        a_shape: &[usize],
        _b_shape: &[usize],
        _output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_a.device;
        let queue = &input_a.queue;
        let output_size = 1; // Scalar result

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_vector_dot_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            a_shape[0] as u32, // Vector length
            0u32,              // Padding
            0u32,              // Padding
            0u32,              // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_vector_dot_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        const SHADER_SOURCE: &str = include_shader!("einsum_ops");
        let shader_source = SHADER_SOURCE;
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_vector_dot_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_vector_dot_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_vector_dot_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_a.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: input_b.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_vector_dot_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_vector_dot_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("vector_dot_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_vector_dot_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("einsum_vector_dot_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 1D workgroups for vector dot product
            let workgroup_size = 64;
            let num_workgroups = (a_shape[0] + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_a.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_a.device.clone(),
            queue: input_a.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }

    /// Execute Einstein summation trace on GPU  
    /// Implements ii -> (empty) pattern (trace operation)
    pub fn execute_einsum_trace<T>(
        input_tensor: &GpuBuffer<T>,
        input_shape: &[usize],
        _output_shape: &[usize],
    ) -> Result<GpuBuffer<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Clone + Send + Sync + 'static,
    {
        use wgpu::util::DeviceExt;

        let device = &input_tensor.device;
        let queue = &input_tensor.queue;
        let output_size = 1; // Scalar result

        // Create output buffer
        let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some("einsum_trace_output"),
            size: (output_size * std::mem::size_of::<T>()) as u64,
            usage: wgpu::BufferUsages::STORAGE
                | wgpu::BufferUsages::COPY_SRC
                | wgpu::BufferUsages::COPY_DST,
            mapped_at_creation: false,
        });

        // Create metadata buffer
        let metadata = [
            input_shape[0] as u32, // Matrix dimension (assuming square)
            0u32,                  // Padding
            0u32,                  // Padding
            0u32,                  // Padding
        ];

        let metadata_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
            label: Some("einsum_trace_metadata"),
            contents: bytemuck::cast_slice(&metadata),
            usage: wgpu::BufferUsages::STORAGE,
        });

        // Create shader module
        let shader_source = include_shader!("einsum_ops");
        let shader_module = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("einsum_trace_shader"),
            source: wgpu::ShaderSource::Wgsl(shader_source.into()),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("einsum_trace_bind_group_layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 4,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: false },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 5,
                    visibility: wgpu::ShaderStages::COMPUTE,
                    ty: wgpu::BindingType::Buffer {
                        ty: wgpu::BufferBindingType::Storage { read_only: true },
                        has_dynamic_offset: false,
                        min_binding_size: None,
                    },
                    count: None,
                },
            ],
        });

        // Create bind group
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("einsum_trace_bind_group"),
            layout: &bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: input_tensor.buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 4,
                    resource: output_buffer.as_entire_binding(),
                },
                wgpu::BindGroupEntry {
                    binding: 5,
                    resource: metadata_buffer.as_entire_binding(),
                },
            ],
        });

        // Create compute pipeline
        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("einsum_trace_pipeline_layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
            label: Some("einsum_trace_pipeline"),
            layout: Some(&pipeline_layout),
            module: &shader_module,
            entry_point: Some("trace_einsum"),
            compilation_options: Default::default(),
            cache: None,
        });

        // Dispatch compute shader
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("einsum_trace_encoder"),
        });

        {
            let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor {
                label: Some("trace_pass"),
                timestamp_writes: None,
            });

            compute_pass.set_pipeline(&compute_pipeline);
            compute_pass.set_bind_group(0, &bind_group, &[]);

            // Dispatch 1D workgroups for trace computation
            let workgroup_size = 64;
            let num_workgroups = (input_shape[0] + workgroup_size - 1) / workgroup_size;
            compute_pass.dispatch_workgroups(num_workgroups as u32, 1, 1);
        }

        queue.submit(std::iter::once(encoder.finish()));
        device.poll(wgpu::Maintain::Wait);

        // Create result GpuBuffer
        let device_id = match &input_tensor.device_enum {
            Device::Gpu(id) => *id,
            _ => return Err(crate::TensorError::DeviceMismatch),
        };

        Ok(GpuBuffer {
            buffer: output_buffer,
            device: input_tensor.device.clone(),
            queue: input_tensor.queue.clone(),
            device_enum: Device::Gpu(device_id),
            len: output_size,
            _phantom: std::marker::PhantomData,
        })
    }
}

// Additional GPU operations will be implemented in future versions
