"""
Type stubs for TenfloweRS Python bindings.

This file provides type hints for IDE support and static type checking.
"""

from typing import Optional, List, Tuple, Union, Dict, Any, Sequence
from enum import Enum
import numpy as np
import numpy.typing as npt

# Version info
__version__: str

# DType enumeration
class DType:
    """Data type enumeration for tensors."""

    float32: DType
    float64: DType
    float16: DType
    bfloat16: DType
    int8: DType
    int16: DType
    int32: DType
    int64: DType
    uint8: DType
    uint16: DType
    uint32: DType
    uint64: DType
    bool_dtype: DType

    def is_floating_point(self) -> bool: ...
    def is_integer(self) -> bool: ...
    def is_signed(self) -> bool: ...
    def is_supported(self) -> bool: ...
    def size_bytes(self) -> int: ...
    def result_type(self, other: DType) -> DType: ...
    def can_cast_to(self, target: DType) -> bool: ...
    def is_safe_cast(self, target: DType) -> bool: ...
    def __str__(self) -> str: ...
    def __repr__(self) -> str: ...
    def __eq__(self, other: object) -> bool: ...
    def __ne__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...

# Dtype module-level functions
def result_type(dtype1: DType, dtype2: DType) -> DType: ...
def is_safe_cast_py(from_dtype: DType, to_dtype: DType) -> bool: ...
def can_cast(from_dtype: DType, to_dtype: DType) -> bool: ...

# Device type
class Device:
    """Device placement for tensors."""

    def __init__(self, device_str: str) -> None: ...
    def is_cpu(self) -> bool: ...
    def is_gpu(self) -> bool: ...
    def __str__(self) -> str: ...
    def __repr__(self) -> str: ...
    def __eq__(self, other: object) -> bool: ...

def cpu() -> Device: ...
def gpu(index: int = 0) -> Device: ...

# Tensor class
class Tensor:
    """N-dimensional tensor with automatic differentiation support."""

    def __init__(
        self,
        data: Union[List[float], npt.NDArray[np.float32]],
        requires_grad: bool = False
    ) -> None: ...

    # Creation methods
    @staticmethod
    def zeros(shape: Sequence[int]) -> Tensor: ...

    @staticmethod
    def ones(shape: Sequence[int]) -> Tensor: ...

    @staticmethod
    def rand(shape: Sequence[int]) -> Tensor: ...

    @staticmethod
    def randn(shape: Sequence[int]) -> Tensor: ...

    @staticmethod
    def from_numpy(array: npt.NDArray[np.float32]) -> Tensor: ...

    @staticmethod
    def arange(start: float, end: float, step: float = 1.0) -> Tensor: ...

    @staticmethod
    def linspace(start: float, end: float, steps: int) -> Tensor: ...

    # Properties
    def shape(self) -> List[int]: ...
    def ndim(self) -> int: ...
    def numel(self) -> int: ...
    def dtype(self) -> DType: ...
    def device(self) -> Device: ...
    def requires_grad(self) -> bool: ...
    def grad(self) -> Optional[Tensor]: ...

    # Shape operations
    def reshape(self, shape: Sequence[int]) -> Tensor: ...
    def transpose(self, dim0: int, dim1: int) -> Tensor: ...
    def permute(self, dims: Sequence[int]) -> Tensor: ...
    def squeeze(self, dim: Optional[int] = None) -> Tensor: ...
    def unsqueeze(self, dim: int) -> Tensor: ...
    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> Tensor: ...
    def view(self, shape: Sequence[int]) -> Tensor: ...

    # Arithmetic operations
    def __add__(self, other: Union[Tensor, float]) -> Tensor: ...
    def __sub__(self, other: Union[Tensor, float]) -> Tensor: ...
    def __mul__(self, other: Union[Tensor, float]) -> Tensor: ...
    def __truediv__(self, other: Union[Tensor, float]) -> Tensor: ...
    def __neg__(self) -> Tensor: ...

    # Matrix operations
    def matmul(self, other: Tensor) -> Tensor: ...
    def __matmul__(self, other: Tensor) -> Tensor: ...

    # Reduction operations
    def sum(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    def mean(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    def max(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    def min(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...
    def prod(self, dim: Optional[int] = None, keepdim: bool = False) -> Tensor: ...

    # Math operations
    def abs(self) -> Tensor: ...
    def sqrt(self) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def sin(self) -> Tensor: ...
    def cos(self) -> Tensor: ...
    def tanh(self) -> Tensor: ...
    def sigmoid(self) -> Tensor: ...
    def relu(self) -> Tensor: ...

    # Gradient operations
    def backward(self, gradient: Optional[Tensor] = None) -> None: ...
    def zero_grad(self) -> None: ...
    def detach(self) -> Tensor: ...

    # Device operations
    def to(self, device: Device) -> Tensor: ...
    def cpu(self) -> Tensor: ...
    def cuda(self, index: int = 0) -> Tensor: ...

    # Conversion
    def numpy(self) -> npt.NDArray[np.float32]: ...
    def item(self) -> float: ...
    def tolist(self) -> List[Any]: ...

    # Clone and copy
    def clone(self) -> Tensor: ...
    def copy(self) -> Tensor: ...

    # String representation
    def __str__(self) -> str: ...
    def __repr__(self) -> str: ...

    # Indexing
    def __getitem__(self, key: Any) -> Tensor: ...
    def __setitem__(self, key: Any, value: Union[Tensor, float]) -> None: ...

# Tensor creation functions
def zeros(shape: Sequence[int], dtype: Optional[DType] = None, device: Optional[Device] = None) -> Tensor: ...
def ones(shape: Sequence[int], dtype: Optional[DType] = None, device: Optional[Device] = None) -> Tensor: ...
def rand(shape: Sequence[int], dtype: Optional[DType] = None, device: Optional[Device] = None) -> Tensor: ...
def randn(shape: Sequence[int], dtype: Optional[DType] = None, device: Optional[Device] = None) -> Tensor: ...
def arange(start: float, end: float, step: float = 1.0, dtype: Optional[DType] = None) -> Tensor: ...
def linspace(start: float, end: float, steps: int, dtype: Optional[DType] = None) -> Tensor: ...
def eye(n: int, m: Optional[int] = None, dtype: Optional[DType] = None) -> Tensor: ...
def full(shape: Sequence[int], fill_value: float, dtype: Optional[DType] = None) -> Tensor: ...

# Tensor operations
def add(a: Tensor, b: Tensor) -> Tensor: ...
def sub(a: Tensor, b: Tensor) -> Tensor: ...
def mul(a: Tensor, b: Tensor) -> Tensor: ...
def div(a: Tensor, b: Tensor) -> Tensor: ...
def matmul(a: Tensor, b: Tensor) -> Tensor: ...
def cat(tensors: Sequence[Tensor], dim: int = 0) -> Tensor: ...
def stack(tensors: Sequence[Tensor], dim: int = 0) -> Tensor: ...

# Gradient tape
class GradientTape:
    """Context manager for automatic differentiation."""

    def __init__(self, persistent: bool = False) -> None: ...
    def __enter__(self) -> GradientTape: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...
    def watch(self, tensor: Tensor) -> None: ...
    def gradient(self, target: Tensor, source: Tensor) -> Optional[Tensor]: ...
    def gradients(self, target: Tensor, sources: Sequence[Tensor]) -> List[Optional[Tensor]]: ...
    def reset(self) -> None: ...

# Parameter
class Parameter:
    """Trainable parameter wrapper."""

    def __init__(self, tensor: Tensor, requires_grad: bool = True) -> None: ...
    def data(self) -> Tensor: ...
    def grad(self) -> Optional[Tensor]: ...
    def zero_grad(self) -> None: ...
    def __repr__(self) -> str: ...

# Neural network layers
class Dense:
    """Fully connected (dense) layer."""

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        use_bias: bool = True,
        activation: Optional[str] = None
    ) -> None: ...

    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...
    def parameters(self) -> List[Parameter]: ...
    def train(self) -> None: ...
    def eval(self) -> None: ...
    def is_training(self) -> bool: ...
    def state_dict(self) -> Dict[str, Tensor]: ...
    def load_state_dict(self, state: Dict[str, Tensor]) -> None: ...

class Sequential:
    """Sequential container for layers."""

    def __init__(self, *layers: Any) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...
    def add(self, layer: Any) -> None: ...
    def parameters(self) -> List[Parameter]: ...
    def train(self) -> None: ...
    def eval(self) -> None: ...

# Normalization layers
class BatchNorm1d:
    """Batch normalization for 1D inputs."""

    def __init__(
        self,
        num_features: int,
        eps: float = 1e-5,
        momentum: float = 0.1,
        affine: bool = True,
        track_running_stats: bool = True
    ) -> None: ...

    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...
    def train(self) -> None: ...
    def eval(self) -> None: ...

class LayerNorm:
    """Layer normalization."""

    def __init__(
        self,
        normalized_shape: Union[int, Sequence[int]],
        eps: float = 1e-5,
        elementwise_affine: bool = True
    ) -> None: ...

    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...

class GroupNorm:
    """Group normalization."""

    def __init__(
        self,
        num_groups: int,
        num_channels: int,
        eps: float = 1e-5,
        affine: bool = True
    ) -> None: ...

    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...

class InstanceNorm1d:
    """Instance normalization for 1D inputs."""

    def __init__(
        self,
        num_features: int,
        eps: float = 1e-5,
        momentum: float = 0.1,
        affine: bool = False,
        track_running_stats: bool = False
    ) -> None: ...

    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...

# Recurrent layers
class LSTM:
    """Long Short-Term Memory layer."""

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        batch_first: bool = False,
        dropout: float = 0.0,
        bidirectional: bool = False
    ) -> None: ...

    def forward(
        self,
        input: Tensor,
        hidden: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]: ...

class GRU:
    """Gated Recurrent Unit layer."""

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        batch_first: bool = False,
        dropout: float = 0.0,
        bidirectional: bool = False
    ) -> None: ...

    def forward(
        self,
        input: Tensor,
        hidden: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]: ...

class RNN:
    """Simple RNN layer."""

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        nonlinearity: str = "tanh",
        bias: bool = True,
        batch_first: bool = False,
        dropout: float = 0.0,
        bidirectional: bool = False
    ) -> None: ...

    def forward(
        self,
        input: Tensor,
        hidden: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]: ...

# SSM layers
class Mamba:
    """Mamba (Selective State Space Model) layer."""

    def __init__(
        self,
        d_model: int,
        d_state: int = 16,
        expand_factor: int = 2,
        dt_rank: Optional[int] = None,
        dropout: float = 0.0,
        bias: bool = False
    ) -> None: ...

    def forward(
        self,
        input: Tensor,
        initial_state: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]: ...

    def state_dict(self) -> Dict[str, Any]: ...
    def load_state_dict(self, state: Dict[str, Any]) -> None: ...

class StateSpaceModel:
    """General state space model."""

    def __init__(
        self,
        input_dim: int,
        state_dim: int,
        output_dim: int,
        has_feedthrough: bool = False
    ) -> None: ...

    def forward(
        self,
        input: Tensor,
        initial_state: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]: ...

# Optimizers
class SGD:
    """Stochastic Gradient Descent optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float,
        momentum: float = 0.0,
        dampening: float = 0.0,
        weight_decay: float = 0.0,
        nesterov: bool = False
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...
    def get_lr(self) -> float: ...
    def set_lr(self, lr: float) -> None: ...
    def state_dict(self) -> Dict[str, Any]: ...
    def load_state_dict(self, state: Dict[str, Any]) -> None: ...

class Adam:
    """Adam optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8,
        weight_decay: float = 0.0,
        amsgrad: bool = False
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...
    def get_lr(self) -> float: ...
    def set_lr(self, lr: float) -> None: ...

class AdamW:
    """AdamW optimizer (Adam with decoupled weight decay)."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8,
        weight_decay: float = 0.01,
        amsgrad: bool = False
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

class RMSprop:
    """RMSprop optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.01,
        alpha: float = 0.99,
        epsilon: float = 1e-8,
        weight_decay: float = 0.0,
        momentum: float = 0.0,
        centered: bool = False
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

# Extended optimizers
class AdaBelief:
    """AdaBelief optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-16,
        weight_decay: float = 0.0,
        amsgrad: bool = False
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

class RAdam:
    """Rectified Adam optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.001,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8,
        weight_decay: float = 0.0
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

class Nadam:
    """Nesterov-accelerated Adam optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.002,
        beta1: float = 0.9,
        beta2: float = 0.999,
        epsilon: float = 1e-8,
        weight_decay: float = 0.0
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

class AdaGrad:
    """AdaGrad optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 0.01,
        lr_decay: float = 0.0,
        weight_decay: float = 0.0,
        initial_accumulator_value: float = 0.0,
        epsilon: float = 1e-10
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

class AdaDelta:
    """AdaDelta optimizer."""

    def __init__(
        self,
        parameters: Sequence[Parameter],
        lr: float = 1.0,
        rho: float = 0.9,
        epsilon: float = 1e-6,
        weight_decay: float = 0.0
    ) -> None: ...

    def step(self) -> None: ...
    def zero_grad(self) -> None: ...

# Utility functions
def tensor_info(tensor: Tensor) -> Dict[str, Any]: ...
def same_shape(a: Tensor, b: Tensor) -> bool: ...
def is_scalar(tensor: Tensor) -> bool: ...
def is_vector(tensor: Tensor) -> bool: ...
def is_matrix(tensor: Tensor) -> bool: ...
def numel(tensor: Tensor) -> int: ...
def validate_shapes(a: Tensor, b: Tensor, operation: str) -> None: ...
def tensor_summary(tensor: Tensor, name: Optional[str] = None) -> str: ...
def all_same_shape(tensors: Sequence[Tensor]) -> bool: ...
def broadcast_shape(shape_a: Sequence[int], shape_b: Sequence[int]) -> List[int]: ...
def is_broadcastable(shape_a: Sequence[int], shape_b: Sequence[int]) -> bool: ...
def tensor_memory_bytes(tensor: Tensor) -> int: ...
def tensor_memory_str(tensor: Tensor) -> str: ...
def format_bytes(bytes: int) -> str: ...
def print_tensor_info(tensor: Tensor, name: Optional[str] = None) -> None: ...
def validate_dimension(tensor: Tensor, dim: int) -> None: ...
def normalize_dimension(tensor: Tensor, dim: int) -> int: ...
def get_device_info() -> str: ...
def is_gpu_available() -> bool: ...
def version() -> str: ...

# Numpy interop
def tensor_from_numpy(array: npt.NDArray[np.float32]) -> Tensor: ...
def tensor_to_numpy(tensor: Tensor) -> npt.NDArray[np.float32]: ...

# Memory management
def enable_memory_profiling() -> None: ...
def disable_memory_profiling() -> None: ...
def get_memory_info() -> Dict[str, Any]: ...

# Gradient management
def is_grad_enabled() -> bool: ...
def set_grad_enabled(enabled: bool) -> None: ...
def get_default_device() -> Device: ...
def set_default_device(device: Device) -> None: ...

# Custom exceptions
class ShapeError(ValueError):
    """Shape mismatch error."""
    pass

class DeviceError(RuntimeError):
    """Device placement error."""
    pass

class GradientError(RuntimeError):
    """Gradient computation error."""
    pass

class NumericalError(RuntimeError):
    """Numerical stability error."""
    pass

class MemoryError(Exception):
    """Memory allocation error."""
    pass

class TensorOpError(RuntimeError):
    """Tensor operation error."""
    pass

class LayerConfigError(ValueError):
    """Layer configuration error."""
    pass

class OptimizerError(RuntimeError):
    """Optimizer error."""
    pass

class SerializationError(RuntimeError):
    """Serialization error."""
    pass

class DataLoadError(RuntimeError):
    """Data loading error."""
    pass

class GraphCompileError(RuntimeError):
    """Graph compilation error."""
    pass

class CheckpointError(RuntimeError):
    """Checkpoint error."""
    pass
