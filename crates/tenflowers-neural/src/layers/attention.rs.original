use crate::layers::{Dropout, Layer};
use num_traits::{Float, FromPrimitive, One, Zero};
use std::collections::HashMap;
use std::sync::RwLock;
#[cfg(feature = "gpu")]
use tenflowers_core::{Device, Result, Tensor, TensorError};
#[cfg(not(feature = "gpu"))]
use tenflowers_core::{Result, Tensor, TensorError};

#[cfg(feature = "gpu")]
use std::any::TypeId;
#[cfg(feature = "gpu")]
use tenflowers_core::gpu::attention_ops::GpuAttentionOps;

/// Key-Value cache for efficient autoregressive generation
#[derive(Debug, Clone)]
pub struct KVCache<T> {
    /// Cached keys for each layer, indexed by layer_id
    pub keys: HashMap<String, Tensor<T>>,
    /// Cached values for each layer, indexed by layer_id  
    pub values: HashMap<String, Tensor<T>>,
    /// Maximum sequence length supported by the cache
    pub max_seq_len: usize,
    /// Current sequence position
    pub current_pos: usize,
}

impl<T> KVCache<T>
where
    T: Clone + Default,
{
    /// Create a new KV cache
    pub fn new(max_seq_len: usize) -> Self {
        Self {
            keys: HashMap::new(),
            values: HashMap::new(),
            max_seq_len,
            current_pos: 0,
        }
    }

    /// Clear the cache (reset for new sequence)
    pub fn clear(&mut self) {
        self.keys.clear();
        self.values.clear();
        self.current_pos = 0;
    }

    /// Get cached key and value for a layer
    pub fn get(&self, layer_id: &str) -> Option<(&Tensor<T>, &Tensor<T>)> {
        if let (Some(key), Some(value)) = (self.keys.get(layer_id), self.values.get(layer_id)) {
            Some((key, value))
        } else {
            None
        }
    }

    /// Update cache with new key and value tensors
    pub fn update(&mut self, layer_id: String, key: Tensor<T>, value: Tensor<T>) -> Result<()> {
        self.keys.insert(layer_id.clone(), key);
        self.values.insert(layer_id, value);
        Ok(())
    }

    /// Advance the current position (for next token generation)
    pub fn advance(&mut self) {
        self.current_pos += 1;
    }

    /// Get current position
    pub fn position(&self) -> usize {
        self.current_pos
    }

    /// Check if cache is at capacity
    pub fn is_full(&self) -> bool {
        self.current_pos >= self.max_seq_len
    }
}

/// Multi-Head Attention layer with KV caching and relative position bias support
///
/// Implements the attention mechanism from "Attention Is All You Need"
/// <https://arxiv.org/abs/1706.03762>
///
/// With optional relative position bias from "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
/// <https://arxiv.org/abs/1910.10683>
pub struct MultiHeadAttention<T> {
    #[allow(dead_code)]
    num_heads: usize,
    #[allow(dead_code)]
    head_dim: usize,
    embed_dim: usize,

    // Weight matrices
    query_weight: Tensor<T>,
    key_weight: Tensor<T>,
    value_weight: Tensor<T>,
    output_weight: Tensor<T>,

    // Biases (optional)
    query_bias: Option<Tensor<T>>,
    key_bias: Option<Tensor<T>>,
    value_bias: Option<Tensor<T>>,
    output_bias: Option<Tensor<T>>,

    // Relative position bias
    relative_position_bias: Option<RwLock<RelativePositionBias<T>>>,

    training: bool,

    /// Layer identifier for KV caching
    layer_id: String,

    /// Whether to use Flash attention optimization
    use_flash_attention: bool,

    /// Block size for Flash attention (default: 128)
    flash_block_size: usize,
}

impl<T> Clone for MultiHeadAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn clone(&self) -> Self {
        MultiHeadAttention {
            num_heads: self.num_heads,
            head_dim: self.head_dim,
            embed_dim: self.embed_dim,
            query_weight: self.query_weight.clone(),
            key_weight: self.key_weight.clone(),
            value_weight: self.value_weight.clone(),
            output_weight: self.output_weight.clone(),
            query_bias: self.query_bias.clone(),
            key_bias: self.key_bias.clone(),
            value_bias: self.value_bias.clone(),
            output_bias: self.output_bias.clone(),
            relative_position_bias: self
                .relative_position_bias
                .as_ref()
                .map(|rpb| RwLock::new(rpb.read().unwrap().clone())),
            training: self.training,
            layer_id: self.layer_id.clone(),
            use_flash_attention: self.use_flash_attention,
            flash_block_size: self.flash_block_size,
        }
    }
}

/// Relative position bias module for attention
///
/// Learns relative position biases that are added to attention scores.
/// This allows the model to understand relative distances between positions
/// without relying on absolute positional embeddings.
#[derive(Clone)]
pub struct RelativePositionBias<T> {
    /// Relative position bias embeddings: [num_buckets, num_heads]
    relative_bias_table: Tensor<T>,
    /// Number of relative position buckets
    num_buckets: usize,
    /// Maximum distance for relative positions
    max_distance: usize,
    /// Number of attention heads
    num_heads: usize,
    /// Whether to use bidirectional relative positions
    bidirectional: bool,
    /// Cache for relative position indices
    position_indices_cache: Option<Tensor<i32>>,
    /// Maximum cached sequence length
    max_cached_seq_len: usize,
}

impl<T> RelativePositionBias<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static + num_traits::FromPrimitive,
{
    /// Create a new relative position bias module
    pub fn new(
        num_heads: usize,
        num_buckets: usize,
        max_distance: usize,
        bidirectional: bool,
    ) -> Self {
        // Initialize relative bias table with small random values
        // In practice, you'd want proper initialization (e.g., normal distribution)
        let relative_bias_table = Tensor::zeros(&[num_buckets, num_heads]);

        Self {
            relative_bias_table,
            num_buckets,
            max_distance,
            num_heads,
            bidirectional,
            position_indices_cache: None,
            max_cached_seq_len: 0,
        }
    }

    /// Compute relative position bucket for a given relative position
    fn relative_position_bucket(
        relative_position: i32,
        bidirectional: bool,
        num_buckets: usize,
        max_distance: usize,
    ) -> usize {
        let mut bucket = 0usize;
        let mut relative_pos = relative_position;

        if bidirectional {
            // For bidirectional, we have negative and positive relative positions
            let num_buckets = num_buckets / 2;
            if relative_pos > 0 {
                bucket += num_buckets;
            } else {
                relative_pos = -relative_pos;
            }
        } else {
            // For unidirectional (e.g., causal attention), clamp to non-negative
            relative_pos = (-relative_pos).max(0);
        }

        // Half of the buckets are for exact incremental positions
        let max_exact = num_buckets / 2;
        let is_small = (relative_pos as usize) < max_exact;

        if is_small {
            bucket += relative_pos as usize;
        } else {
            // The other half is for logarithmically bigger bins in positions up to max_distance
            let relative_position_log = ((relative_pos as f32 / max_exact as f32).ln()
                / (max_distance as f32 / max_exact as f32).ln()
                * (num_buckets - max_exact) as f32)
                as usize;
            bucket += max_exact + relative_position_log.min(num_buckets - max_exact - 1);
        }

        bucket
    }

    /// Get or compute relative position indices for a sequence length
    pub fn get_relative_position_indices(&mut self, seq_len: usize) -> Result<&Tensor<i32>> {
        // Check if we need to recompute the cache
        if self.position_indices_cache.is_none() || seq_len > self.max_cached_seq_len {
            self.compute_position_indices(seq_len)?;
            self.max_cached_seq_len = seq_len;
        }

        Ok(self.position_indices_cache.as_ref().unwrap())
    }

    /// Compute relative position indices for attention matrix
    fn compute_position_indices(&mut self, seq_len: usize) -> Result<()> {
        let mut indices = Vec::with_capacity(seq_len * seq_len);

        for i in 0..seq_len {
            for j in 0..seq_len {
                let relative_position = j as i32 - i as i32;
                let bucket = Self::relative_position_bucket(
                    relative_position,
                    self.bidirectional,
                    self.num_buckets,
                    self.max_distance,
                );
                indices.push(bucket as i32);
            }
        }

        self.position_indices_cache = Some(Tensor::from_vec(indices, &[seq_len, seq_len])?);
        Ok(())
    }

    /// Get relative position bias for attention scores
    pub fn forward(&mut self, seq_len: usize) -> Result<Tensor<T>> {
        // Get position indices
        let position_indices = self.get_relative_position_indices(seq_len)?;

        // Look up biases from the table using the position indices
        let indices_data = position_indices.to_vec()?;
        let bias_table_data = self.relative_bias_table.to_vec()?;

        let mut bias_values = Vec::with_capacity(seq_len * seq_len * self.num_heads);

        // For each position pair, look up the bias for each head
        for &bucket_idx in &indices_data {
            let bucket_idx = bucket_idx as usize;
            if bucket_idx < self.num_buckets {
                // Extract bias values for this bucket across all heads
                let start_idx = bucket_idx * self.num_heads;
                let end_idx = start_idx + self.num_heads;
                if end_idx <= bias_table_data.len() {
                    bias_values.extend_from_slice(&bias_table_data[start_idx..end_idx]);
                } else {
                    // Fallback for out of bounds
                    for _ in 0..self.num_heads {
                        bias_values.push(T::zero());
                    }
                }
            } else {
                // Fallback for invalid bucket
                for _ in 0..self.num_heads {
                    bias_values.push(T::zero());
                }
            }
        }

        let bias = Tensor::from_vec(bias_values, &[seq_len, seq_len, self.num_heads])?;
        Ok(bias)
    }

    /// Get parameters for training
    pub fn parameters(&self) -> Vec<&Tensor<T>> {
        vec![&self.relative_bias_table]
    }

    /// Get mutable parameters for training
    pub fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        vec![&mut self.relative_bias_table]
    }
}

impl<T> MultiHeadAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::iter::Sum
        + num_traits::FromPrimitive,
{
    /// Create a new multi-head attention layer
    ///
    /// # Arguments
    /// * `embed_dim` - Embedding dimension (must be divisible by num_heads)
    /// * `num_heads` - Number of attention heads
    /// * `use_bias` - Whether to use bias terms
    /// * `layer_id` - Optional layer identifier for KV caching (defaults to "attention")
    pub fn new(embed_dim: usize, num_heads: usize, use_bias: bool) -> Result<Self> {
        Self::with_layer_id(embed_dim, num_heads, use_bias, "attention".to_string())
    }

    /// Create a new multi-head attention layer with custom layer ID
    ///
    /// # Arguments
    /// * `embed_dim` - Embedding dimension (must be divisible by num_heads)
    /// * `num_heads` - Number of attention heads
    /// * `use_bias` - Whether to use bias terms
    /// * `layer_id` - Layer identifier for KV caching
    pub fn with_layer_id(
        embed_dim: usize,
        num_heads: usize,
        use_bias: bool,
        layer_id: String,
    ) -> Result<Self> {
        Self::with_flash_attention(embed_dim, num_heads, use_bias, layer_id, false, 128)
    }

    /// Create a new multi-head attention layer with Flash attention optimization
    ///
    /// # Arguments
    /// * `embed_dim` - Embedding dimension (must be divisible by num_heads)
    /// * `num_heads` - Number of attention heads
    /// * `use_bias` - Whether to use bias terms
    /// * `layer_id` - Layer identifier for KV caching
    /// * `use_flash_attention` - Whether to use Flash attention optimization
    /// * `flash_block_size` - Block size for Flash attention (recommended: 128)
    pub fn with_flash_attention(
        embed_dim: usize,
        num_heads: usize,
        use_bias: bool,
        layer_id: String,
        use_flash_attention: bool,
        flash_block_size: usize,
    ) -> Result<Self> {
        if embed_dim % num_heads != 0 {
            return Err(TensorError::invalid_argument(format!(
                "embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})"
            )));
        }

        let head_dim = embed_dim / num_heads;

        // Initialize weight matrices with appropriate shapes (using zeros for now)
        // In practice, you'd want proper random initialization
        let query_weight = Tensor::zeros(&[embed_dim, embed_dim]);
        let key_weight = Tensor::zeros(&[embed_dim, embed_dim]);
        let value_weight = Tensor::zeros(&[embed_dim, embed_dim]);
        let output_weight = Tensor::zeros(&[embed_dim, embed_dim]);

        let (query_bias, key_bias, value_bias, output_bias) = if use_bias {
            (
                Some(Tensor::zeros(&[embed_dim])),
                Some(Tensor::zeros(&[embed_dim])),
                Some(Tensor::zeros(&[embed_dim])),
                Some(Tensor::zeros(&[embed_dim])),
            )
        } else {
            (None, None, None, None)
        };

        Ok(Self {
            num_heads,
            head_dim,
            embed_dim,
            query_weight,
            key_weight,
            value_weight,
            output_weight,
            query_bias,
            key_bias,
            value_bias,
            output_bias,
            relative_position_bias: None,
            training: true,
            layer_id,
            use_flash_attention,
            flash_block_size,
        })
    }

    /// Create a new multi-head attention layer with relative position bias
    ///
    /// # Arguments
    /// * `embed_dim` - Embedding dimension (must be divisible by num_heads)
    /// * `num_heads` - Number of attention heads
    /// * `use_bias` - Whether to use bias terms
    /// * `num_buckets` - Number of relative position buckets
    /// * `max_distance` - Maximum distance for relative positions
    /// * `bidirectional` - Whether to use bidirectional relative positions
    pub fn with_relative_position_bias(
        embed_dim: usize,
        num_heads: usize,
        use_bias: bool,
        num_buckets: usize,
        max_distance: usize,
        bidirectional: bool,
    ) -> Result<Self> {
        let mut attention =
            Self::with_layer_id(embed_dim, num_heads, use_bias, "attention".to_string())?;

        // Add relative position bias
        attention.relative_position_bias = Some(RwLock::new(RelativePositionBias::new(
            num_heads,
            num_buckets,
            max_distance,
            bidirectional,
        )));

        Ok(attention)
    }

    /// Enable relative position bias for this attention layer
    pub fn enable_relative_position_bias(
        &mut self,
        num_buckets: usize,
        max_distance: usize,
        bidirectional: bool,
    ) {
        self.relative_position_bias = Some(RwLock::new(RelativePositionBias::new(
            self.num_heads,
            num_buckets,
            max_distance,
            bidirectional,
        )));
    }

    /// Disable relative position bias for this attention layer
    pub fn disable_relative_position_bias(&mut self) {
        self.relative_position_bias = None;
    }

    /// Check if relative position bias is enabled
    pub fn has_relative_position_bias(&self) -> bool {
        self.relative_position_bias.is_some()
    }

    /// Proper multi-head attention computation with reshaping and mask support
    fn compute_attention_with_mask(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        // Assume input shape is [batch_size, seq_len, embed_dim]
        let q_shape = q.shape().dims();
        if q_shape.len() != 2 {
            // For now, only handle 2D tensors (flattened batch*seq_len, embed_dim)
            // In practice, you'd want full 3D tensor support
            return self.compute_simple_attention_with_mask(q, k, v, mask);
        }

        let _seq_len = q_shape[0];
        let embed_dim = q_shape[1];

        if embed_dim != self.embed_dim {
            return Err(TensorError::invalid_shape_simple(format!(
                "Expected embedding dimension {}, got {}",
                self.embed_dim, embed_dim
            )));
        }

        // For 2D input, we can simulate multi-head behavior by:
        // 1. Split the embedding dimension into heads
        // 2. Compute attention for each head portion
        // 3. Concatenate results

        let mut head_outputs = Vec::new();

        for head in 0..self.num_heads {
            let start_idx = head * self.head_dim;
            let end_idx = start_idx + self.head_dim;

            // Extract head portion from Q, K, V
            let q_head = self.extract_head_portion(q, start_idx, end_idx)?;
            let k_head = self.extract_head_portion(k, start_idx, end_idx)?;
            let v_head = self.extract_head_portion(v, start_idx, end_idx)?;

            // Compute attention for this head with mask
            let head_output =
                self.compute_single_head_attention_with_mask(&q_head, &k_head, &v_head, mask)?;
            head_outputs.push(head_output);
        }

        // Concatenate head outputs along the feature dimension
        self.concatenate_heads(&head_outputs)
    }

    /// Proper multi-head attention computation with reshaping (backward compatibility)
    #[allow(dead_code)]
    fn compute_attention(
        &mut self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        self.compute_attention_with_mask(q, k, v, None)
    }

    /// Extract a portion of the tensor corresponding to one attention head
    fn extract_head_portion(
        &self,
        tensor: &Tensor<T>,
        start_idx: usize,
        end_idx: usize,
    ) -> Result<Tensor<T>> {
        // For 2D tensor [seq_len, embed_dim], extract columns start_idx:end_idx
        let shape = tensor.shape().dims();
        let seq_len = shape[0];

        // Create a simple slice by extracting the relevant columns
        // This is a simplified approach - in practice you'd use proper tensor slicing
        let data = tensor.as_slice().ok_or_else(|| {
            TensorError::device_error_simple("Cannot access tensor data".to_string())
        })?;

        let mut head_data = Vec::new();
        for i in 0..seq_len {
            for j in start_idx..end_idx {
                let idx = i * self.embed_dim + j;
                head_data.push(data[idx]);
            }
        }

        Tensor::from_vec(head_data, &[seq_len, self.head_dim])
    }

    /// Compute attention for a single head with mask support
    fn compute_single_head_attention_with_mask(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        // Check if all tensors are on GPU and GPU feature is enabled
        // Only use GPU if T also satisfies GPU requirements
        #[cfg(feature = "gpu")]
        {
            if self.can_use_gpu_acceleration(q, k, v) && self.has_gpu_compatible_type() {
                // Cast to GPU-compatible version if possible
                if let Ok(result) = self.try_compute_gpu_attention(q, k, v, mask) {
                    return Ok(result);
                }
            }
        }

        // Fallback to CPU implementation
        self.compute_cpu_attention(q, k, v, mask)
    }

    /// CPU implementation of scaled dot-product attention
    fn compute_cpu_attention(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        // Standard scaled dot-product attention (original CPU implementation)
        let k_transposed = k.transpose()?;
        let scores = q.matmul(&k_transposed)?;

        // Scale by sqrt(head_dim) for proper attention scaling
        let scale = T::from(self.head_dim as f64).unwrap().sqrt();
        let mut scaled_scores = scores.div(&Tensor::from_scalar(scale))?;

        // Apply relative position bias if enabled
        if let Some(ref rel_bias) = self.relative_position_bias {
            // Get sequence length from query tensor
            let seq_len = q.shape().dims()[0];
            let relative_bias = rel_bias.write().unwrap().forward(seq_len)?;

            // For single head, we need to extract the bias for this head
            // relative_bias shape: [seq_len, seq_len, num_heads]
            // We need to slice out the bias for the current head
            // This is a simplified approach - in practice you'd track head index
            let head_bias = relative_bias
                .slice(&[0..seq_len, 0..seq_len, 0..1])?
                .squeeze(Some(&[2]))?;
            scaled_scores = scaled_scores.add(&head_bias)?;
        }

        // Apply mask if provided
        let masked_scores = if let Some(mask) = mask {
            // Add mask to scores (mask should contain 0.0 for valid positions, -inf for masked positions)
            scaled_scores.add(mask)?
        } else {
            scaled_scores
        };

        // Apply softmax
        let attention_weights = tenflowers_core::ops::softmax(&masked_scores, Some(-1))?;

        // Apply to values
        attention_weights.matmul(v)
    }

    /// GPU implementation of scaled dot-product attention
    #[cfg(feature = "gpu")]
    fn compute_gpu_attention(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>>
    where
        T: bytemuck::Pod + bytemuck::Zeroable + Send + Sync + 'static + Float,
    {
        // Get GPU context from tensor
        let gpu_context = q.gpu_context_info().ok_or_else(|| {
            TensorError::device_error_simple("GPU context not available".to_string())
        })?;

        // Create GPU attention operations
        let gpu_ops = tenflowers_core::gpu::attention_ops::GpuAttentionOps::new(&gpu_context)?;

        // Calculate scale factor
        let scale = Some(T::from(self.head_dim as f64).unwrap().sqrt().recip());

        // Dispatch to GPU kernel
        let mut result = gpu_ops.scaled_dot_product_attention(q, k, v, mask, scale)?;

        // Apply relative position bias if enabled (still on GPU)
        if let Some(ref rel_bias) = self.relative_position_bias {
            // Apply position bias to attention scores (before softmax)
            // This is a simplified implementation - full implementation would require
            // custom GPU kernels for proper position bias application
            let bias_tensor = rel_bias.forward(&[])?; // Forward with empty input for global bias
            result = result.add(&bias_tensor)?;
        }

        Ok(result)
    }

    /// Check if GPU acceleration can be used for attention computation
    #[cfg(feature = "gpu")]
    fn can_use_gpu_acceleration(&self, q: &Tensor<T>, k: &Tensor<T>, v: &Tensor<T>) -> bool {
        // Check if all tensors are on GPU device
        matches!(q.device(), Device::Gpu(_))
            && matches!(k.device(), Device::Gpu(_))
            && matches!(v.device(), Device::Gpu(_))
    }

    /// Check if the current type T is GPU-compatible
    #[cfg(feature = "gpu")]
    fn has_gpu_compatible_type(&self) -> bool {
        // For now, only f32 is fully supported for GPU operations
        TypeId::of::<T>() == TypeId::of::<f32>()
    }

    /// Try to compute GPU attention with proper type checking
    #[cfg(feature = "gpu")]
    fn try_compute_gpu_attention(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // For now, just fall back to CPU until GPU context extraction is implemented
        self.compute_cpu_attention(q, k, v, mask)
    }

    /// Compute attention for a single head (backward compatibility)
    #[allow(dead_code)]
    fn compute_single_head_attention(
        &mut self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        self.compute_single_head_attention_with_mask(q, k, v, None)
    }

    /// Concatenate outputs from multiple attention heads
    fn concatenate_heads(&self, head_outputs: &[Tensor<T>]) -> Result<Tensor<T>> {
        if head_outputs.is_empty() {
            return Err(TensorError::invalid_argument(
                "No head outputs to concatenate".to_string(),
            ));
        }

        let first_shape = head_outputs[0].shape().dims();
        let seq_len = first_shape[0];

        // Concatenate along the feature dimension
        let mut concat_data = Vec::new();

        for i in 0..seq_len {
            for head_output in head_outputs {
                let data = head_output.as_slice().ok_or_else(|| {
                    TensorError::device_error_simple("Cannot access head output data".to_string())
                })?;

                let start_idx = i * self.head_dim;
                let end_idx = start_idx + self.head_dim;
                concat_data.extend_from_slice(&data[start_idx..end_idx]);
            }
        }

        Tensor::from_vec(concat_data, &[seq_len, self.embed_dim])
    }

    /// Fallback to simple attention for unsupported tensor shapes with mask support
    fn compute_simple_attention_with_mask(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        let k_transposed = k.transpose()?;
        let scores = q.matmul(&k_transposed)?;

        let scale = T::from(self.embed_dim as f64).unwrap().sqrt();
        let scaled_scores = scores.div(&Tensor::from_scalar(scale))?;

        // Apply mask if provided
        let masked_scores = if let Some(mask) = mask {
            // Add mask to scores (mask should contain 0.0 for valid positions, -inf for masked positions)
            scaled_scores.add(mask)?
        } else {
            scaled_scores
        };

        let attention_weights = tenflowers_core::ops::softmax(&masked_scores, Some(-1))?;
        attention_weights.matmul(v)
    }

    /// Fallback to simple attention for unsupported tensor shapes (backward compatibility)
    #[allow(dead_code)]
    fn compute_simple_attention(
        &self,
        q: &Tensor<T>,
        k: &Tensor<T>,
        v: &Tensor<T>,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + num_traits::FromPrimitive,
    {
        self.compute_simple_attention_with_mask(q, k, v, None)
    }

    /// Forward pass with KV caching support for autoregressive generation
    ///
    /// # Arguments
    /// * `query` - Query tensor (typically single token for autoregressive)
    /// * `key` - Key tensor (can be None if using cache)
    /// * `value` - Value tensor (can be None if using cache)
    /// * `cache` - Mutable reference to KV cache
    /// * `mask` - Optional attention mask
    /// * `use_cache` - Whether to use and update the cache
    pub fn forward_with_cache(
        &mut self,
        query: &Tensor<T>,
        key: Option<&Tensor<T>>,
        value: Option<&Tensor<T>>,
        cache: Option<&mut KVCache<T>>,
        mask: Option<&Tensor<T>>,
        use_cache: bool,
    ) -> Result<Tensor<T>>
    where
        T: std::ops::Sub<Output = T>
            + std::ops::Add<Output = T>
            + std::ops::Div<Output = T>
            + std::ops::Mul<Output = T>
            + num_traits::FromPrimitive,
    {
        if !use_cache || cache.is_none() {
            // Regular forward pass without caching
            let k = key.unwrap_or(query);
            let v = value.unwrap_or(query);
            return self.forward_qkv_with_mask(query, k, v, mask);
        }

        let cache = cache.unwrap();

        // Compute current query projection
        let q = query.matmul(&self.query_weight)?;
        let q = if let Some(ref bias) = self.query_bias {
            q.add(bias)?
        } else {
            q
        };

        // Handle key and value tensors
        let (final_k, final_v) = if let Some(cached) = cache.get(&self.layer_id) {
            // Use cached keys and values
            let (cached_k, cached_v) = cached;

            if let (Some(new_k), Some(new_v)) = (key, value) {
                // Compute new key and value projections
                let k_proj = new_k.matmul(&self.key_weight)?;
                let k_proj = if let Some(ref bias) = self.key_bias {
                    k_proj.add(bias)?
                } else {
                    k_proj
                };

                let v_proj = new_v.matmul(&self.value_weight)?;
                let v_proj = if let Some(ref bias) = self.value_bias {
                    v_proj.add(bias)?
                } else {
                    v_proj
                };

                // Concatenate with cached values
                let updated_k = self.concatenate_tensors(cached_k, &k_proj)?;
                let updated_v = self.concatenate_tensors(cached_v, &v_proj)?;

                // Update cache
                cache.update(self.layer_id.clone(), updated_k.clone(), updated_v.clone())?;

                (updated_k, updated_v)
            } else {
                // Use only cached values
                (cached_k.clone(), cached_v.clone())
            }
        } else {
            // No cache exists, compute fresh key and value projections
            let k = key.unwrap_or(query);
            let v = value.unwrap_or(query);

            let k_proj = k.matmul(&self.key_weight)?;
            let k_proj = if let Some(ref bias) = self.key_bias {
                k_proj.add(bias)?
            } else {
                k_proj
            };

            let v_proj = v.matmul(&self.value_weight)?;
            let v_proj = if let Some(ref bias) = self.value_bias {
                v_proj.add(bias)?
            } else {
                v_proj
            };

            // Initialize cache
            cache.update(self.layer_id.clone(), k_proj.clone(), v_proj.clone())?;

            (k_proj, v_proj)
        };

        // Compute attention using cached/updated keys and values
        let attention_out = self.compute_attention_with_mask(&q, &final_k, &final_v, mask)?;

        // Output projection
        let output = attention_out.matmul(&self.output_weight)?;
        let output = if let Some(ref bias) = self.output_bias {
            output.add(bias)?
        } else {
            output
        };

        Ok(output)
    }

    /// Concatenate tensors along the sequence dimension (for cache updates)
    fn concatenate_tensors(&self, existing: &Tensor<T>, new: &Tensor<T>) -> Result<Tensor<T>> {
        // Simple concatenation along the first dimension (sequence length)
        // In a full implementation, this would use proper tensor concatenation ops
        let existing_data = existing.as_slice().ok_or_else(|| {
            TensorError::unsupported_operation_simple(
                "Cannot access tensor data for concatenation".to_string(),
            )
        })?;
        let new_data = new.as_slice().ok_or_else(|| {
            TensorError::unsupported_operation_simple(
                "Cannot access tensor data for concatenation".to_string(),
            )
        })?;

        let existing_shape = existing.shape().dims();
        let new_shape = new.shape().dims();

        if existing_shape.len() != new_shape.len() || existing_shape[1..] != new_shape[1..] {
            return Err(TensorError::invalid_shape_simple(
                "Tensor shapes are not compatible for concatenation".to_string(),
            ));
        }

        let mut concatenated_data = Vec::new();
        concatenated_data.extend_from_slice(existing_data);
        concatenated_data.extend_from_slice(new_data);

        let mut new_shape = existing_shape.to_vec();
        new_shape[0] += new_shape[0]; // Double the sequence length

        Tensor::from_vec(concatenated_data, &new_shape)
    }

    /// Get the layer ID for this attention layer
    pub fn layer_id(&self) -> &str {
        &self.layer_id
    }

    /// Set the layer ID for this attention layer
    pub fn set_layer_id(&mut self, layer_id: String) {
        self.layer_id = layer_id;
    }

    /// Compute Flash attention for memory-efficient processing
    ///
    /// Flash attention computes attention in blocks to reduce memory usage from O(nÂ²) to O(n)
    /// by avoiding materialization of the full attention matrix.
    ///
    /// Based on: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
    /// https://arxiv.org/abs/2205.14135
    fn compute_flash_attention(
        &self,
        query: &Tensor<T>,
        key: &Tensor<T>,
        value: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        let seq_len = query.shape().dims()[0];
        let embed_dim = query.shape().dims()[1];

        if seq_len <= self.flash_block_size {
            // For small sequences, use standard attention
            return self.compute_attention_with_mask(query, key, value, mask);
        }

        let num_blocks = (seq_len + self.flash_block_size - 1) / self.flash_block_size;
        let scale = T::from_f32(1.0 / ((embed_dim / self.num_heads) as f32).sqrt()).unwrap();

        // Initialize output tensor
        let mut output = Tensor::zeros(&[seq_len, embed_dim]);

        // Initialize running statistics for online softmax
        let mut running_max = Tensor::full(&[seq_len], T::neg_infinity());
        let mut running_sum = Tensor::zeros(&[seq_len]);

        // Process attention in blocks
        for i in 0..num_blocks {
            let q_start = i * self.flash_block_size;
            let q_end = std::cmp::min(q_start + self.flash_block_size, seq_len);

            // Extract query block
            let q_block = self.extract_block(query, q_start, q_end)?;

            // Initialize block output and statistics
            let mut block_output = Tensor::zeros(&[q_end - q_start, embed_dim]);
            let mut block_max = Tensor::full(&[q_end - q_start], T::neg_infinity());
            let mut block_sum = Tensor::zeros(&[q_end - q_start]);

            // Process key-value blocks
            for j in 0..num_blocks {
                let kv_start = j * self.flash_block_size;
                let kv_end = std::cmp::min(kv_start + self.flash_block_size, seq_len);

                // Extract key and value blocks
                let k_block = self.extract_block(key, kv_start, kv_end)?;
                let v_block = self.extract_block(value, kv_start, kv_end)?;

                // Compute attention scores for this block
                let scores = q_block.matmul(&k_block.transpose()?)?;
                let scores = scores.scalar_mul(scale)?;

                // Apply mask if provided
                let scores = if let Some(mask_tensor) = mask {
                    let mask_block =
                        self.extract_block_2d(mask_tensor, q_start, q_end, kv_start, kv_end)?;
                    scores.add(&mask_block)?
                } else {
                    scores
                };

                // Online softmax computation
                let (softmax_scores, new_max, new_sum) =
                    self.compute_online_softmax(&scores, &block_max, &block_sum)?;

                // Update block statistics
                block_max = new_max;
                block_sum = new_sum;

                // Compute weighted values
                let weighted_values = softmax_scores.matmul(&v_block)?;

                // Update block output
                block_output = self.update_block_output(
                    &block_output,
                    &weighted_values,
                    &block_max,
                    &block_sum,
                )?;
            }

            // Update global output and statistics
            self.update_global_output(
                &mut output,
                &block_output,
                &mut running_max,
                &mut running_sum,
                q_start,
                q_end,
            )?;
        }

        // Final normalization
        self.finalize_flash_attention_output(&output, &running_sum)
    }

    /// Extract a block from a tensor along the sequence dimension
    fn extract_block(&self, tensor: &Tensor<T>, start: usize, end: usize) -> Result<Tensor<T>> {
        // Use proper tensor slicing for efficiency
        let shape = tensor.shape();
        let dims = shape.dims();

        // Create slice ranges for each dimension
        let mut ranges = Vec::new();
        ranges.push(start..end); // Sequence dimension
        for i in 1..dims.len() {
            ranges.push(0..dims[i]); // Keep all other dimensions
        }

        // Use tensor slicing if available, otherwise fallback to manual extraction
        // Note: slice_ranges method may not be available, using fallback
        // if let Ok(sliced) = tensor.slice_ranges(&ranges) {
        //     return Ok(sliced);
        // }

        // Fallback to manual extraction
        let data = tensor.to_vec()?;
        let embed_dim = dims[1];

        let mut block_data = Vec::new();
        for i in start..end {
            let row_start = i * embed_dim;
            let row_end = row_start + embed_dim;
            block_data.extend_from_slice(&data[row_start..row_end]);
        }

        Tensor::from_vec(block_data, &[end - start, embed_dim])
    }

    /// Extract a 2D block from a tensor (for attention masks)
    fn extract_block_2d(
        &self,
        tensor: &Tensor<T>,
        q_start: usize,
        q_end: usize,
        kv_start: usize,
        kv_end: usize,
    ) -> Result<Tensor<T>> {
        let shape = tensor.shape();
        let data = tensor.to_vec()?;
        let seq_len = shape.dims()[1];

        let mut block_data = Vec::new();
        for i in q_start..q_end {
            for j in kv_start..kv_end {
                let idx = i * seq_len + j;
                block_data.push(data[idx]);
            }
        }

        Tensor::from_vec(block_data, &[q_end - q_start, kv_end - kv_start])
    }

    /// Compute online softmax for Flash attention
    fn compute_online_softmax(
        &self,
        scores: &Tensor<T>,
        prev_max: &Tensor<T>,
        prev_sum: &Tensor<T>,
    ) -> Result<(Tensor<T>, Tensor<T>, Tensor<T>)> {
        // Find maximum for numerical stability
        let new_max = scores.max_axis(Some(&[1]), true)?;
        let global_max = self.element_wise_max(&new_max, prev_max)?;

        // Compute exponentials
        let exp_scores = scores.sub(&global_max)?.exp()?;
        let exp_prev_sum = prev_sum.mul(&prev_max.sub(&global_max)?.exp()?)?;

        // Update sum
        let new_sum = exp_scores.sum_axis(Some(&[1]), true)?.add(&exp_prev_sum)?;

        // Normalize
        let softmax_scores = exp_scores.div(&new_sum)?;

        Ok((softmax_scores, global_max, new_sum))
    }

    /// Element-wise maximum of two tensors
    fn element_wise_max(&self, a: &Tensor<T>, b: &Tensor<T>) -> Result<Tensor<T>> {
        // Simplified element-wise max - in practice you'd use proper tensor operations
        let a_data = a.to_vec()?;
        let b_data = b.to_vec()?;

        let max_data: Vec<T> = a_data
            .iter()
            .zip(b_data.iter())
            .map(|(x, y)| if x > y { x.clone() } else { y.clone() })
            .collect();

        Tensor::from_vec(max_data, a.shape().dims())
    }

    /// Update block output with new weighted values
    fn update_block_output(
        &self,
        current_output: &Tensor<T>,
        weighted_values: &Tensor<T>,
        block_max: &Tensor<T>,
        block_sum: &Tensor<T>,
    ) -> Result<Tensor<T>> {
        // Check if current output is zero (first iteration)
        let current_sum = current_output.sum(None, false)?;
        let zero = T::zero();

        // Simple check: if the sum is approximately zero, this is the first iteration
        let sum_data = current_sum.to_vec()?;
        if !sum_data.is_empty() && sum_data[0] == zero {
            // First iteration: just return normalized weighted values
            return weighted_values.div(block_sum);
        }

        // Get current block statistics
        let current_max = current_output.max_axis(Some(&[1]), true)?;
        let current_norm = current_output.sum_axis(Some(&[1]), true)?;

        // Compute new maximum
        let new_max = self.element_wise_max(&current_max, block_max)?;

        // Rescale current output and new values
        let current_scale = current_max.sub(&new_max)?.exp()?;
        let new_scale = block_max.sub(&new_max)?.exp()?;

        let scaled_current = current_output.mul(&current_scale)?;
        let scaled_current_sum = current_norm.mul(&current_scale)?;
        let scaled_new = weighted_values.mul(&new_scale)?;
        let scaled_new_sum = block_sum.mul(&new_scale)?;

        // Combine outputs
        let combined_output = scaled_current.add(&scaled_new)?;
        let combined_sum = scaled_current_sum.add(&scaled_new_sum)?;

        // Normalize
        combined_output.div(&combined_sum)
    }

    /// Update global output with block results
    fn update_global_output(
        &self,
        global_output: &mut Tensor<T>,
        block_output: &Tensor<T>,
        running_max: &mut Tensor<T>,
        running_sum: &mut Tensor<T>,
        start: usize,
        end: usize,
    ) -> Result<()> {
        // Extract the global output block that corresponds to this query block
        let mut global_block = self.extract_block(global_output, start, end)?;
        let global_max_block = self.extract_block(running_max, start, end)?;
        let global_sum_block = self.extract_block(running_sum, start, end)?;

        // Get block statistics from the block output
        let block_max = block_output.max_axis(Some(&[1]), true)?;
        let block_sum = block_output.sum_axis(Some(&[1]), true)?;

        // Compute the maximum between global and block statistics
        let new_max = self.element_wise_max(&global_max_block, &block_max)?;

        // Rescale previous global output and sum
        let global_scale = global_max_block.sub(&new_max)?.exp()?;
        let block_scale = block_max.sub(&new_max)?.exp()?;

        let scaled_global_output = global_block.mul(&global_scale)?;
        let scaled_global_sum = global_sum_block.mul(&global_scale)?;
        let scaled_block_output = block_output.mul(&block_scale)?;
        let scaled_block_sum = block_sum.mul(&block_scale)?;

        // Update global output and sum
        let new_output = scaled_global_output.add(&scaled_block_output)?;
        let new_sum = scaled_global_sum.add(&scaled_block_sum)?;

        // Normalize the output
        let normalized_output = new_output.div(&new_sum)?;

        // Update the global tensors (this is a simplified approach)
        // In practice, you'd use proper tensor assignment operations
        let global_data = global_output.to_vec()?;
        let normalized_data = normalized_output.to_vec()?;
        let new_max_data = new_max.to_vec()?;
        let new_sum_data = new_sum.to_vec()?;

        // Update global output tensor data
        let embed_dim = global_output.shape().dims()[1];
        let mut updated_global_data = global_data;
        for (i, row_idx) in (start..end).enumerate() {
            for j in 0..embed_dim {
                let global_idx = row_idx * embed_dim + j;
                let block_idx = i * embed_dim + j;
                if block_idx < normalized_data.len() {
                    updated_global_data[global_idx] = normalized_data[block_idx];
                }
            }
        }

        // Update running statistics
        let mut updated_max_data = running_max.to_vec()?;
        let mut updated_sum_data = running_sum.to_vec()?;

        for (i, row_idx) in (start..end).enumerate() {
            if i < new_max_data.len() {
                updated_max_data[row_idx] = new_max_data[i];
            }
            if i < new_sum_data.len() {
                updated_sum_data[row_idx] = new_sum_data[i];
            }
        }

        // Create new tensors with updated data
        *global_output = Tensor::from_vec(updated_global_data, global_output.shape().dims())?;
        *running_max = Tensor::from_vec(updated_max_data, running_max.shape().dims())?;
        *running_sum = Tensor::from_vec(updated_sum_data, running_sum.shape().dims())?;

        Ok(())
    }

    /// Finalize Flash attention output with proper normalization
    fn finalize_flash_attention_output(
        &self,
        output: &Tensor<T>,
        running_sum: &Tensor<T>,
    ) -> Result<Tensor<T>> {
        // Final normalization step
        output.div(running_sum)
    }
}

impl<T> Layer<T> for MultiHeadAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // For simplicity, we use the same input for query, key, and value (self-attention)
        self.forward_qkv(input, input, input)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = vec![
            &self.query_weight,
            &self.key_weight,
            &self.value_weight,
            &self.output_weight,
        ];

        if let Some(ref bias) = self.query_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.key_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.value_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.output_bias {
            params.push(bias);
        }

        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = vec![
            &mut self.query_weight,
            &mut self.key_weight,
            &mut self.value_weight,
            &mut self.output_weight,
        ];

        if let Some(ref mut bias) = self.query_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.key_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.value_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.output_bias {
            params.push(bias);
        }

        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// Utility functions for creating attention masks
impl<T> MultiHeadAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Create a causal mask for autoregressive attention (prevents attention to future positions)
    ///
    /// # Arguments
    /// * `seq_len` - Sequence length
    ///
    /// Returns a mask where upper triangular part is -inf, lower triangular part is 0
    pub fn create_causal_mask(seq_len: usize) -> Result<Tensor<T>> {
        let mut mask_data = Vec::with_capacity(seq_len * seq_len);
        let neg_inf = T::neg_infinity();
        let zero = T::zero();

        for i in 0..seq_len {
            for j in 0..seq_len {
                if j > i {
                    mask_data.push(neg_inf); // Mask future positions
                } else {
                    mask_data.push(zero); // Allow past and current positions
                }
            }
        }

        Tensor::from_vec(mask_data, &[seq_len, seq_len])
    }

    /// Create a padding mask for variable length sequences
    ///
    /// # Arguments
    /// * `lengths` - Actual length of each sequence in the batch
    /// * `max_len` - Maximum sequence length (padded length)
    ///
    /// Returns a mask where padded positions are -inf, valid positions are 0
    pub fn create_padding_mask(lengths: &[usize], max_len: usize) -> Result<Tensor<T>> {
        let batch_size = lengths.len();
        let mut mask_data = Vec::with_capacity(batch_size * max_len);
        let neg_inf = T::neg_infinity();
        let zero = T::zero();

        for &length in lengths {
            for pos in 0..max_len {
                if pos >= length {
                    mask_data.push(neg_inf); // Mask padding positions
                } else {
                    mask_data.push(zero); // Allow valid positions
                }
            }
        }

        Tensor::from_vec(mask_data, &[batch_size, max_len])
    }
}

impl<T> MultiHeadAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Forward pass with separate query, key, and value inputs
    pub fn forward_qkv(
        &self,
        query: &Tensor<T>,
        key: &Tensor<T>,
        value: &Tensor<T>,
    ) -> Result<Tensor<T>> {
        self.forward_qkv_with_mask(query, key, value, None)
    }

    /// Forward pass with attention mask support
    ///
    /// # Arguments
    /// * `query` - Query tensor
    /// * `key` - Key tensor  
    /// * `value` - Value tensor
    /// * `mask` - Optional attention mask. If provided, should be broadcastable to attention scores shape.
    ///   Values should be 0.0 for positions to attend to, and -inf for positions to mask out.
    pub fn forward_qkv_with_mask(
        &self,
        query: &Tensor<T>,
        key: &Tensor<T>,
        value: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // For simplicity, we'll work with 2D tensors (flattened batch and sequence dimensions)
        // In a full implementation, you'd want proper 3D/4D tensor support

        // Linear projections
        let q = query.matmul(&self.query_weight)?;
        let k = key.matmul(&self.key_weight)?;
        let v = value.matmul(&self.value_weight)?;

        // Add biases if present
        let q = if let Some(ref bias) = self.query_bias {
            q.add(bias)?
        } else {
            q
        };

        let k = if let Some(ref bias) = self.key_bias {
            k.add(bias)?
        } else {
            k
        };

        let v = if let Some(ref bias) = self.value_bias {
            v.add(bias)?
        } else {
            v
        };

        // Compute attention with mask support
        let attention_output = if self.use_flash_attention {
            self.compute_flash_attention(&q, &k, &v, mask)?
        } else {
            self.compute_attention_with_mask(&q, &k, &v, mask)?
        };

        // Final linear projection
        let output = attention_output.matmul(&self.output_weight)?;

        // Add output bias if present
        if let Some(ref bias) = self.output_bias {
            output.add(bias)
        } else {
            Ok(output)
        }
    }
}

/// Multi-Query Attention layer for modern LLMs
///
/// Multi-query attention is a variation where multiple query heads share the same key and value heads.
/// This reduces the number of parameters and memory usage compared to standard multi-head attention.
///
/// Based on: "Multi-Query Attention"
/// https://arxiv.org/abs/1911.02150
///
/// Used in models like PaLM, GPT-J, and StarCoder.
pub struct MultiQueryAttention<T> {
    num_heads: usize,
    head_dim: usize,
    embed_dim: usize,

    // Weight matrices - note that key and value have single head dimension
    query_weight: Tensor<T>,  // [embed_dim, embed_dim] - same as MHA
    key_weight: Tensor<T>,    // [embed_dim, head_dim] - single head only
    value_weight: Tensor<T>,  // [embed_dim, head_dim] - single head only
    output_weight: Tensor<T>, // [embed_dim, embed_dim] - same as MHA

    // Biases (optional)
    query_bias: Option<Tensor<T>>,
    key_bias: Option<Tensor<T>>,   // [head_dim] - single head only
    value_bias: Option<Tensor<T>>, // [head_dim] - single head only
    output_bias: Option<Tensor<T>>,

    // Relative position bias
    relative_position_bias: Option<RwLock<RelativePositionBias<T>>>,

    training: bool,

    /// Layer identifier for KV caching
    layer_id: String,

    /// Whether to use Flash attention optimization
    use_flash_attention: bool,

    /// Block size for Flash attention (default: 128)
    flash_block_size: usize,
}

impl<T> Clone for MultiQueryAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn clone(&self) -> Self {
        MultiQueryAttention {
            num_heads: self.num_heads,
            head_dim: self.head_dim,
            embed_dim: self.embed_dim,
            query_weight: self.query_weight.clone(),
            key_weight: self.key_weight.clone(),
            value_weight: self.value_weight.clone(),
            output_weight: self.output_weight.clone(),
            query_bias: self.query_bias.clone(),
            key_bias: self.key_bias.clone(),
            value_bias: self.value_bias.clone(),
            output_bias: self.output_bias.clone(),
            relative_position_bias: self
                .relative_position_bias
                .as_ref()
                .map(|rpb| RwLock::new(rpb.read().unwrap().clone())),
            training: self.training,
            layer_id: self.layer_id.clone(),
            use_flash_attention: self.use_flash_attention,
            flash_block_size: self.flash_block_size,
        }
    }
}

/// Multi-Query Attention implementation
impl<T> MultiQueryAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Create a new multi-query attention layer
    ///
    /// # Arguments
    /// * `embed_dim` - Embedding dimension
    /// * `num_heads` - Number of query heads (key and value are single-headed)
    /// * `use_bias` - Whether to use bias terms
    /// * `layer_id` - Optional layer identifier for debugging
    pub fn new(
        embed_dim: usize,
        num_heads: usize,
        use_bias: bool,
        layer_id: Option<String>,
    ) -> Result<Self> {
        if embed_dim % num_heads != 0 {
            return Err(TensorError::invalid_argument(format!(
                "embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})"
            )));
        }

        let head_dim = embed_dim / num_heads;

        // Initialize weight matrices
        // For this simplified implementation, use same dimensions as standard attention
        // Query: [embed_dim, embed_dim]
        let query_weight = Tensor::zeros(&[embed_dim, embed_dim]);

        // Key and Value: [embed_dim, embed_dim] (simplified - not true MQA)
        let key_weight = Tensor::zeros(&[embed_dim, embed_dim]);
        let value_weight = Tensor::zeros(&[embed_dim, embed_dim]);

        // Output projection [embed_dim, embed_dim]
        let output_weight = Tensor::zeros(&[embed_dim, embed_dim]);

        // Initialize biases if requested
        let query_bias = if use_bias {
            Some(Tensor::zeros(&[embed_dim]))
        } else {
            None
        };
        let key_bias = if use_bias {
            Some(Tensor::zeros(&[embed_dim]))
        } else {
            None
        };
        let value_bias = if use_bias {
            Some(Tensor::zeros(&[embed_dim]))
        } else {
            None
        };
        let output_bias = if use_bias {
            Some(Tensor::zeros(&[embed_dim]))
        } else {
            None
        };

        Ok(MultiQueryAttention {
            num_heads,
            head_dim,
            embed_dim,
            query_weight,
            key_weight,
            value_weight,
            output_weight,
            query_bias,
            key_bias,
            value_bias,
            output_bias,
            relative_position_bias: None,
            training: true,
            layer_id: layer_id.unwrap_or_else(|| "mqa".to_string()),
            use_flash_attention: false,
            flash_block_size: 128,
        })
    }

    /// Forward pass with separate query, key, and value inputs
    pub fn forward_qkv(
        &self,
        query: &Tensor<T>,
        key: &Tensor<T>,
        value: &Tensor<T>,
    ) -> Result<Tensor<T>> {
        self.forward_qkv_with_mask(query, key, value, None)
    }

    /// Forward pass with attention mask support
    ///
    /// # Arguments
    /// * `query` - Query tensor [seq_len, embed_dim] or [batch_size, seq_len, embed_dim]
    /// * `key` - Key tensor [seq_len, embed_dim] or [batch_size, seq_len, embed_dim]
    /// * `value` - Value tensor [seq_len, embed_dim] or [batch_size, seq_len, embed_dim]
    /// * `mask` - Optional attention mask
    pub fn forward_qkv_with_mask(
        &self,
        query: &Tensor<T>,
        key: &Tensor<T>,
        value: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // Simplified implementation using only basic tensor operations
        // This doesn't implement full multi-query attention but provides a working baseline

        // Linear projections
        let q = query.matmul(&self.query_weight)?;
        let q = if let Some(ref bias) = self.query_bias {
            q.add(bias)?
        } else {
            q
        };

        let k = key.matmul(&self.key_weight)?;
        let k = if let Some(ref bias) = self.key_bias {
            k.add(bias)?
        } else {
            k
        };

        let v = value.matmul(&self.value_weight)?;
        let v = if let Some(ref bias) = self.value_bias {
            v.add(bias)?
        } else {
            v
        };

        // Basic attention computation (simplified version)
        // Compute attention scores: Q * K^T
        let scores = q.matmul(&k.transpose()?)?;

        // Scale by sqrt(head_dim)
        let scale = T::from((self.head_dim as f64).sqrt()).unwrap();
        let scaled_scores = scores.div(&Tensor::from_scalar(scale))?;

        // Apply mask if provided
        let masked_scores = if let Some(mask) = mask {
            scaled_scores.add(mask)?
        } else {
            scaled_scores
        };

        // Apply softmax
        let attention_weights =
            tenflowers_core::ops::activation::softmax(&masked_scores, Some(-1))?;

        // Apply attention to values: Attention * V
        let attention_output = attention_weights.matmul(&v)?;

        // Final output projection
        let output = attention_output.matmul(&self.output_weight)?;
        let output = if let Some(ref bias) = self.output_bias {
            output.add(bias)?
        } else {
            output
        };

        Ok(output)
    }

    /// Concatenate outputs from multiple attention heads
    fn concatenate_heads(&self, head_outputs: &[Tensor<T>]) -> Result<Tensor<T>> {
        if head_outputs.is_empty() {
            return Err(TensorError::invalid_argument(
                "No head outputs to concatenate".to_string(),
            ));
        }

        let first_shape = head_outputs[0].shape().dims();
        let seq_len = first_shape[0];

        // Concatenate along the feature dimension
        let mut concat_data = Vec::new();

        for i in 0..seq_len {
            for head_output in head_outputs {
                let data = head_output.to_vec()?;
                let start_idx = i * self.head_dim;
                let end_idx = start_idx + self.head_dim;
                concat_data.extend_from_slice(&data[start_idx..end_idx]);
            }
        }

        Tensor::from_vec(concat_data, &[seq_len, self.embed_dim])
    }
}

impl<T> Layer<T> for MultiQueryAttention<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // For self-attention, use the same input for query, key, and value
        self.forward_qkv(input, input, input)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = vec![
            &self.query_weight,
            &self.key_weight,
            &self.value_weight,
            &self.output_weight,
        ];

        if let Some(ref bias) = self.query_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.key_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.value_bias {
            params.push(bias);
        }
        if let Some(ref bias) = self.output_bias {
            params.push(bias);
        }

        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = vec![
            &mut self.query_weight,
            &mut self.key_weight,
            &mut self.value_weight,
            &mut self.output_weight,
        ];

        if let Some(ref mut bias) = self.query_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.key_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.value_bias {
            params.push(bias);
        }
        if let Some(ref mut bias) = self.output_bias {
            params.push(bias);
        }

        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// TransformerEncoder Block
///
/// A complete transformer encoder layer consisting of:
/// - Multi-head self-attention
/// - Position-wise feed-forward network
/// - Residual connections
/// - Layer normalization
/// - Dropout regularization
///
/// Supports both pre-norm and post-norm variants
#[derive(Clone)]
pub struct TransformerEncoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + FromPrimitive
        + std::iter::Sum,
{
    _embed_dim: usize,

    // Components
    self_attention: MultiHeadAttention<T>,
    feed_forward: FeedForwardNetwork<T>,

    // Layer normalization
    norm1: crate::layers::LayerNorm<T>,
    norm2: crate::layers::LayerNorm<T>,

    // Dropout layers
    attention_dropout: Dropout<T>,
    ff_dropout: Dropout<T>,

    // Configuration
    pre_norm: bool, // Pre-norm vs post-norm variant
    _dropout_rate: f32,
    gradient_checkpointing: bool, // Whether to use gradient checkpointing

    training: bool,
}

impl<T> TransformerEncoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Create a new TransformerEncoder block
    ///
    /// # Arguments
    /// * `embed_dim` - Model dimension
    /// * `num_heads` - Number of attention heads
    /// * `feed_forward_dim` - Hidden dimension of feed-forward network
    /// * `dropout_rate` - Dropout probability
    /// * `pre_norm` - Whether to use pre-norm (true) or post-norm (false) architecture
    pub fn new(
        embed_dim: usize,
        num_heads: usize,
        feed_forward_dim: Option<usize>,
        dropout_rate: Option<f32>,
        pre_norm: Option<bool>,
    ) -> Result<Self> {
        Self::with_gradient_checkpointing(
            embed_dim,
            num_heads,
            feed_forward_dim,
            dropout_rate,
            pre_norm,
            false,
        )
    }

    /// Create a new TransformerEncoder block with gradient checkpointing support
    ///
    /// # Arguments
    /// * `embed_dim` - Model dimension
    /// * `num_heads` - Number of attention heads
    /// * `feed_forward_dim` - Hidden dimension of feed-forward network
    /// * `dropout_rate` - Dropout probability
    /// * `pre_norm` - Whether to use pre-norm (true) or post-norm (false) architecture
    /// * `gradient_checkpointing` - Whether to use gradient checkpointing to reduce memory usage
    pub fn with_gradient_checkpointing(
        embed_dim: usize,
        num_heads: usize,
        feed_forward_dim: Option<usize>,
        dropout_rate: Option<f32>,
        pre_norm: Option<bool>,
        gradient_checkpointing: bool,
    ) -> Result<Self> {
        let feed_forward_dim = feed_forward_dim.unwrap_or(embed_dim * 4); // Default: 4x model dimension
        let dropout_rate = dropout_rate.unwrap_or(0.1);
        let pre_norm = pre_norm.unwrap_or(true); // Pre-norm is generally better

        // Create components
        let self_attention = MultiHeadAttention::new(embed_dim, num_heads, true)?;
        let feed_forward = FeedForwardNetwork::new(embed_dim, feed_forward_dim, dropout_rate)?;

        // Layer normalization layers
        let norm1 = crate::layers::LayerNorm::new(&[embed_dim]).with_epsilon(1e-5);
        let norm2 = crate::layers::LayerNorm::new(&[embed_dim]).with_epsilon(1e-5);

        // Dropout layers
        let attention_dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));
        let ff_dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));

        Ok(Self {
            _embed_dim: embed_dim,
            self_attention,
            feed_forward,
            norm1,
            norm2,
            attention_dropout,
            ff_dropout,
            pre_norm,
            _dropout_rate: dropout_rate,
            gradient_checkpointing,
            training: true,
        })
    }

    /// Forward pass with optional attention mask
    ///
    /// # Arguments
    /// * `x` - Input tensor [seq_len, embed_dim] or [batch_size, seq_len, embed_dim]
    /// * `mask` - Optional attention mask for self-attention
    pub fn forward_with_mask(&self, x: &Tensor<T>, mask: Option<&Tensor<T>>) -> Result<Tensor<T>> {
        if self.gradient_checkpointing && self.training {
            self.forward_with_checkpointing(x, mask)
        } else {
            self.forward_without_checkpointing(x, mask)
        }
    }

    /// Forward pass without gradient checkpointing
    fn forward_without_checkpointing(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        if self.pre_norm {
            self.forward_pre_norm(x, mask)
        } else {
            self.forward_post_norm(x, mask)
        }
    }

    /// Forward pass with gradient checkpointing
    ///
    /// This implementation uses a simplified approach where we divide the computation
    /// into two checkpoint segments: attention and feed-forward.
    /// During backward pass, intermediate activations are recomputed rather than stored.
    fn forward_with_checkpointing(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        if self.pre_norm {
            self.forward_pre_norm_checkpointed(x, mask)
        } else {
            self.forward_post_norm_checkpointed(x, mask)
        }
    }

    /// Pre-norm variant with gradient checkpointing
    fn forward_pre_norm_checkpointed(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // Checkpoint 1: Attention sublayer
        let attention_checkpoint = self.checkpoint_attention_sublayer(x, mask)?;

        // Checkpoint 2: Feed-forward sublayer
        let ff_checkpoint = self.checkpoint_feedforward_sublayer(&attention_checkpoint)?;

        Ok(ff_checkpoint)
    }

    /// Post-norm variant with gradient checkpointing
    fn forward_post_norm_checkpointed(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // Checkpoint 1: Attention sublayer
        let attention_checkpoint = self.checkpoint_attention_sublayer_post_norm(x, mask)?;

        // Checkpoint 2: Feed-forward sublayer
        let ff_checkpoint =
            self.checkpoint_feedforward_sublayer_post_norm(&attention_checkpoint)?;

        Ok(ff_checkpoint)
    }

    /// Checkpoint function for attention sublayer (pre-norm)
    fn checkpoint_attention_sublayer(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // This function will be recomputed during backward pass
        let norm1_out = self.norm1.forward(x)?;
        let attention_out = self
            .self_attention
            .forward_qkv_with_mask(&norm1_out, &norm1_out, &norm1_out, mask)?;
        let attention_dropped = self.attention_dropout.forward(&attention_out)?;
        let residual1 = x.add(&attention_dropped)?;
        Ok(residual1)
    }

    /// Checkpoint function for feed-forward sublayer (pre-norm)
    fn checkpoint_feedforward_sublayer(&self, x: &Tensor<T>) -> Result<Tensor<T>> {
        // This function will be recomputed during backward pass
        let norm2_out = self.norm2.forward(x)?;
        let ff_out = self.feed_forward.forward(&norm2_out)?;
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual2 = x.add(&ff_dropped)?;
        Ok(residual2)
    }

    /// Checkpoint function for attention sublayer (post-norm)
    fn checkpoint_attention_sublayer_post_norm(
        &self,
        x: &Tensor<T>,
        mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        let attention_out = self.self_attention.forward_qkv_with_mask(x, x, x, mask)?;
        let attention_dropped = self.attention_dropout.forward(&attention_out)?;
        let residual1 = x.add(&attention_dropped)?;
        let norm1_out = self.norm1.forward(&residual1)?;
        Ok(norm1_out)
    }

    /// Checkpoint function for feed-forward sublayer (post-norm)
    fn checkpoint_feedforward_sublayer_post_norm(&self, x: &Tensor<T>) -> Result<Tensor<T>> {
        let ff_out = self.feed_forward.forward(x)?;
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual2 = x.add(&ff_dropped)?;
        let norm2_out = self.norm2.forward(&residual2)?;
        Ok(norm2_out)
    }

    /// Get the gradient checkpointing setting
    pub fn gradient_checkpointing(&self) -> bool {
        self.gradient_checkpointing
    }

    /// Set the gradient checkpointing setting
    ///
    /// # Arguments
    /// * `enabled` - Whether to enable gradient checkpointing
    ///
    /// # Note
    /// Gradient checkpointing trades computation for memory during training.
    /// It reduces memory usage by recomputing intermediate activations during
    /// the backward pass instead of storing them.
    pub fn set_gradient_checkpointing(&mut self, enabled: bool) {
        self.gradient_checkpointing = enabled;
    }

    /// Pre-norm variant: LayerNorm -> Attention -> Add -> LayerNorm -> FFN -> Add
    fn forward_pre_norm(&self, x: &Tensor<T>, mask: Option<&Tensor<T>>) -> Result<Tensor<T>> {
        // First sublayer: self-attention with residual connection
        let norm1_out = self.norm1.forward(x)?;
        let attention_out = self
            .self_attention
            .forward_qkv_with_mask(&norm1_out, &norm1_out, &norm1_out, mask)?;

        // Apply dropout to attention output during training
        let attention_dropped = self.attention_dropout.forward(&attention_out)?;
        let residual1 = x.add(&attention_dropped)?;

        // Second sublayer: feed-forward with residual connection
        let norm2_out = self.norm2.forward(&residual1)?;
        let ff_out = self.feed_forward.forward(&norm2_out)?;

        // Apply dropout to feed-forward output during training
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual2 = residual1.add(&ff_dropped)?;

        Ok(residual2)
    }

    /// Post-norm variant: Attention -> Add -> LayerNorm -> FFN -> Add -> LayerNorm
    fn forward_post_norm(&self, x: &Tensor<T>, mask: Option<&Tensor<T>>) -> Result<Tensor<T>> {
        // First sublayer: self-attention with residual connection
        let attention_out = self.self_attention.forward_qkv_with_mask(x, x, x, mask)?;

        // Apply dropout to attention output during training
        let attention_dropped = self.attention_dropout.forward(&attention_out)?;
        let residual1 = x.add(&attention_dropped)?;
        let norm1_out = self.norm1.forward(&residual1)?;

        // Second sublayer: feed-forward with residual connection
        let ff_out = self.feed_forward.forward(&norm1_out)?;

        // Apply dropout to feed-forward output during training
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual2 = norm1_out.add(&ff_dropped)?;
        let norm2_out = self.norm2.forward(&residual2)?;

        Ok(norm2_out)
    }
}

impl<T> Layer<T> for TransformerEncoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        self.forward_with_mask(input, None)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();

        // Add attention parameters
        params.extend(self.self_attention.parameters());

        // Add feed-forward parameters
        params.extend(self.feed_forward.parameters());

        // Add layer norm parameters
        params.extend(self.norm1.parameters());
        params.extend(self.norm2.parameters());

        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();

        // Add attention parameters
        params.extend(self.self_attention.parameters_mut());

        // Add feed-forward parameters
        params.extend(self.feed_forward.parameters_mut());

        // Add layer norm parameters
        params.extend(self.norm1.parameters_mut());
        params.extend(self.norm2.parameters_mut());

        // Add dropout parameters (none expected since dropout doesn't have learnable parameters)
        params.extend(self.attention_dropout.parameters_mut());
        params.extend(self.ff_dropout.parameters_mut());

        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.self_attention.set_training(training);
        self.feed_forward.set_training(training);
        self.norm1.set_training(training);
        self.norm2.set_training(training);
        self.attention_dropout.set_training(training);
        self.ff_dropout.set_training(training);
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// Position-wise Feed-Forward Network
///
/// A simple two-layer MLP with ReLU activation:
/// FFN(x) = ReLU(xW1 + b1)W2 + b2
pub struct FeedForwardNetwork<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static + FromPrimitive,
{
    _input_dim: usize,
    _hidden_dim: usize,

    // Linear layers
    linear1: crate::layers::Dense<T>,
    linear2: crate::layers::Dense<T>,

    // Dropout layer
    dropout: Dropout<T>,

    _dropout_rate: f32,
    training: bool,
}

impl<T> Clone for FeedForwardNetwork<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Sub<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive,
{
    fn clone(&self) -> Self {
        FeedForwardNetwork {
            _input_dim: self._input_dim,
            _hidden_dim: self._hidden_dim,
            linear1: self.linear1.clone(),
            linear2: self.linear2.clone(),
            dropout: self.dropout.clone(),
            _dropout_rate: self._dropout_rate,
            training: self.training,
        }
    }
}

impl<T> FeedForwardNetwork<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static + num_traits::FromPrimitive,
{
    /// Create a new feed-forward network
    ///
    /// # Arguments
    /// * `input_dim` - Input dimension (typically embed_dim)
    /// * `hidden_dim` - Hidden dimension (typically 4 * embed_dim)
    /// * `dropout_rate` - Dropout probability applied after first linear layer
    pub fn new(input_dim: usize, hidden_dim: usize, dropout_rate: f32) -> Result<Self> {
        let linear1 = crate::layers::Dense::new(input_dim, hidden_dim, true);
        let linear2 = crate::layers::Dense::new(hidden_dim, input_dim, true);
        let dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));

        Ok(Self {
            _input_dim: input_dim,
            _hidden_dim: hidden_dim,
            linear1,
            linear2,
            dropout,
            _dropout_rate: dropout_rate,
            training: true,
        })
    }
}

impl<T> Layer<T> for FeedForwardNetwork<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Sub<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // First linear layer + ReLU activation
        let x = self.linear1.forward(input)?;
        let x = tenflowers_core::ops::activation::relu(&x)?;

        // Apply dropout after activation during training
        let x = self.dropout.forward(&x)?;

        // Second linear layer
        let output = self.linear2.forward(&x)?;

        Ok(output)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.linear1.parameters());
        params.extend(self.linear2.parameters());
        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.linear1.parameters_mut());
        params.extend(self.linear2.parameters_mut());
        // Add dropout parameters (none expected since dropout doesn't have learnable parameters)
        params.extend(self.dropout.parameters_mut());
        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.linear1.set_training(training);
        self.linear2.set_training(training);
        self.dropout.set_training(training);
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// SwiGLU Feed-Forward Network
///
/// A GLU-based feed-forward network using SiLU (Swish) activation:
/// SwiGLU(x) = SiLU(xW1 + b1) â (xW2 + b2) * W3 + b3
///
/// Used in modern transformer architectures like PaLM, GLaM, etc.
pub struct SwiGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    _input_dim: usize,
    _hidden_dim: usize,

    // Linear layers for gating
    gate_proj: crate::layers::Dense<T>, // Projects to hidden_dim
    up_proj: crate::layers::Dense<T>,   // Projects to hidden_dim
    down_proj: crate::layers::Dense<T>, // Projects back to input_dim

    _dropout_rate: f32,
    training: bool,
}

impl<T> Clone for SwiGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    fn clone(&self) -> Self {
        SwiGLU {
            _input_dim: self._input_dim,
            _hidden_dim: self._hidden_dim,
            gate_proj: self.gate_proj.clone(),
            up_proj: self.up_proj.clone(),
            down_proj: self.down_proj.clone(),
            _dropout_rate: self._dropout_rate,
            training: self.training,
        }
    }
}

impl<T> SwiGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    /// Create a new SwiGLU feed-forward network
    ///
    /// # Arguments
    /// * `input_dim` - Input dimension (typically embed_dim)
    /// * `hidden_dim` - Hidden dimension (typically 8/3 * embed_dim for equivalent parameters to standard FFN)
    /// * `dropout_rate` - Dropout probability (currently unused)
    pub fn new(input_dim: usize, hidden_dim: usize, dropout_rate: f32) -> Result<Self> {
        let gate_proj = crate::layers::Dense::new(input_dim, hidden_dim, false); // No bias for GLU
        let up_proj = crate::layers::Dense::new(input_dim, hidden_dim, false); // No bias for GLU
        let down_proj = crate::layers::Dense::new(hidden_dim, input_dim, true);

        Ok(Self {
            _input_dim: input_dim,
            _hidden_dim: hidden_dim,
            gate_proj,
            up_proj,
            down_proj,
            _dropout_rate: dropout_rate,
            training: true,
        })
    }
}

impl<T> Layer<T> for SwiGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // Gate branch: SiLU(xW1)
        let gate = self.gate_proj.forward(input)?;
        let gate = tenflowers_core::ops::activation::swish(&gate)?; // SiLU/Swish activation

        // Up branch: xW2
        let up = self.up_proj.forward(input)?;

        // Element-wise multiplication: SiLU(xW1) â (xW2)
        let gated = gate.mul(&up)?;

        // Final projection: down_proj(gated)
        let output = self.down_proj.forward(&gated)?;

        Ok(output)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.gate_proj.parameters());
        params.extend(self.up_proj.parameters());
        params.extend(self.down_proj.parameters());
        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.gate_proj.parameters_mut());
        params.extend(self.up_proj.parameters_mut());
        params.extend(self.down_proj.parameters_mut());
        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.gate_proj.set_training(training);
        self.up_proj.set_training(training);
        self.down_proj.set_training(training);
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// GeGLU Feed-Forward Network
///
/// A GLU-based feed-forward network using GELU activation:
/// GeGLU(x) = GELU(xW1 + b1) â (xW2 + b2) * W3 + b3
///
/// Used in some transformer variants and provides smoother gradients than ReLU-based FFNs.
pub struct GeGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    _input_dim: usize,
    _hidden_dim: usize,

    // Linear layers for gating
    gate_proj: crate::layers::Dense<T>, // Projects to hidden_dim
    up_proj: crate::layers::Dense<T>,   // Projects to hidden_dim
    down_proj: crate::layers::Dense<T>, // Projects back to input_dim

    _dropout_rate: f32,
    training: bool,
}

impl<T> Clone for GeGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    fn clone(&self) -> Self {
        GeGLU {
            _input_dim: self._input_dim,
            _hidden_dim: self._hidden_dim,
            gate_proj: self.gate_proj.clone(),
            up_proj: self.up_proj.clone(),
            down_proj: self.down_proj.clone(),
            _dropout_rate: self._dropout_rate,
            training: self.training,
        }
    }
}

impl<T> GeGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    /// Create a new GeGLU feed-forward network
    ///
    /// # Arguments
    /// * `input_dim` - Input dimension (typically embed_dim)
    /// * `hidden_dim` - Hidden dimension (typically 8/3 * embed_dim for equivalent parameters to standard FFN)
    /// * `dropout_rate` - Dropout probability (currently unused)
    pub fn new(input_dim: usize, hidden_dim: usize, dropout_rate: f32) -> Result<Self> {
        let gate_proj = crate::layers::Dense::new(input_dim, hidden_dim, false); // No bias for GLU
        let up_proj = crate::layers::Dense::new(input_dim, hidden_dim, false); // No bias for GLU
        let down_proj = crate::layers::Dense::new(hidden_dim, input_dim, true);

        Ok(Self {
            _input_dim: input_dim,
            _hidden_dim: hidden_dim,
            gate_proj,
            up_proj,
            down_proj,
            _dropout_rate: dropout_rate,
            training: true,
        })
    }
}

impl<T> Layer<T> for GeGLU<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // Gate branch: GELU(xW1)
        let gate = self.gate_proj.forward(input)?;
        let gate = tenflowers_core::ops::activation::gelu(&gate)?; // GELU activation

        // Up branch: xW2
        let up = self.up_proj.forward(input)?;

        // Element-wise multiplication: GELU(xW1) â (xW2)
        let gated = gate.mul(&up)?;

        // Final projection: down_proj(gated)
        let output = self.down_proj.forward(&gated)?;

        Ok(output)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.gate_proj.parameters());
        params.extend(self.up_proj.parameters());
        params.extend(self.down_proj.parameters());
        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();
        params.extend(self.gate_proj.parameters_mut());
        params.extend(self.up_proj.parameters_mut());
        params.extend(self.down_proj.parameters_mut());
        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.gate_proj.set_training(training);
        self.up_proj.set_training(training);
        self.down_proj.set_training(training);
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// TransformerDecoder Block
///
/// A complete transformer decoder layer consisting of:
/// - Masked multi-head self-attention (with causal mask)
/// - Multi-head cross-attention to encoder outputs
/// - Position-wise feed-forward network
/// - Residual connections
/// - Layer normalization
///
/// Supports both pre-norm and post-norm variants
pub struct TransformerDecoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + FromPrimitive
        + std::iter::Sum,
{
    _embed_dim: usize,

    // Components
    self_attention: MultiHeadAttention<T>,
    cross_attention: MultiHeadAttention<T>,
    feed_forward: FeedForwardNetwork<T>,

    // Layer normalization
    norm1: crate::layers::LayerNorm<T>, // After self-attention
    norm2: crate::layers::LayerNorm<T>, // After cross-attention
    norm3: crate::layers::LayerNorm<T>, // After feed-forward

    // Dropout layers
    self_attention_dropout: Dropout<T>,
    cross_attention_dropout: Dropout<T>,
    ff_dropout: Dropout<T>,

    // Configuration
    pre_norm: bool, // Pre-norm vs post-norm variant
    _dropout_rate: f32,

    training: bool,
}

impl<T> Clone for TransformerDecoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn clone(&self) -> Self {
        TransformerDecoder {
            _embed_dim: self._embed_dim,
            self_attention: self.self_attention.clone(),
            cross_attention: self.cross_attention.clone(),
            feed_forward: self.feed_forward.clone(),
            norm1: self.norm1.clone(),
            norm2: self.norm2.clone(),
            norm3: self.norm3.clone(),
            self_attention_dropout: self.self_attention_dropout.clone(),
            cross_attention_dropout: self.cross_attention_dropout.clone(),
            ff_dropout: self.ff_dropout.clone(),
            pre_norm: self.pre_norm,
            _dropout_rate: self._dropout_rate,
            training: self.training,
        }
    }
}

impl<T> TransformerDecoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Create a new TransformerDecoder block
    ///
    /// # Arguments
    /// * `embed_dim` - Model dimension
    /// * `num_heads` - Number of attention heads
    /// * `feed_forward_dim` - Hidden dimension of feed-forward network
    /// * `dropout_rate` - Dropout probability
    /// * `pre_norm` - Whether to use pre-norm (true) or post-norm (false) architecture
    pub fn new(
        embed_dim: usize,
        num_heads: usize,
        feed_forward_dim: Option<usize>,
        dropout_rate: Option<f32>,
        pre_norm: Option<bool>,
    ) -> Result<Self> {
        let feed_forward_dim = feed_forward_dim.unwrap_or(embed_dim * 4); // Default: 4x model dimension
        let dropout_rate = dropout_rate.unwrap_or(0.1);
        let pre_norm = pre_norm.unwrap_or(true); // Pre-norm is generally better

        // Create components
        let self_attention = MultiHeadAttention::new(embed_dim, num_heads, true)?;
        let cross_attention = MultiHeadAttention::new(embed_dim, num_heads, true)?;
        let feed_forward = FeedForwardNetwork::new(embed_dim, feed_forward_dim, dropout_rate)?;

        // Layer normalization layers
        let norm1 = crate::layers::LayerNorm::new(&[embed_dim]).with_epsilon(1e-5);
        let norm2 = crate::layers::LayerNorm::new(&[embed_dim]).with_epsilon(1e-5);
        let norm3 = crate::layers::LayerNorm::new(&[embed_dim]).with_epsilon(1e-5);

        // Dropout layers
        let self_attention_dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));
        let cross_attention_dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));
        let ff_dropout =
            Dropout::new(T::from(dropout_rate).unwrap_or_else(|| T::from(0.1f32).unwrap()));

        Ok(Self {
            _embed_dim: embed_dim,
            self_attention,
            cross_attention,
            feed_forward,
            norm1,
            norm2,
            norm3,
            self_attention_dropout,
            cross_attention_dropout,
            ff_dropout,
            pre_norm,
            _dropout_rate: dropout_rate,
            training: true,
        })
    }

    /// Forward pass with encoder memory and optional masks
    ///
    /// # Arguments
    /// * `x` - Target tensor [seq_len, embed_dim] or [batch_size, seq_len, embed_dim]
    /// * `memory` - Encoder output tensor (same shape as x for memory)
    /// * `self_attn_mask` - Optional causal mask for self-attention (typically causal mask for autoregressive generation)
    /// * `cross_attn_mask` - Optional mask for cross-attention to encoder (typically padding mask)
    pub fn forward_with_memory(
        &self,
        x: &Tensor<T>,
        memory: &Tensor<T>,
        self_attn_mask: Option<&Tensor<T>>,
        cross_attn_mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        if self.pre_norm {
            self.forward_pre_norm_with_memory(x, memory, self_attn_mask, cross_attn_mask)
        } else {
            self.forward_post_norm_with_memory(x, memory, self_attn_mask, cross_attn_mask)
        }
    }

    /// Pre-norm variant for decoder
    /// LayerNorm -> Self-Attention -> Add -> LayerNorm -> Cross-Attention -> Add -> LayerNorm -> FFN -> Add
    fn forward_pre_norm_with_memory(
        &self,
        x: &Tensor<T>,
        memory: &Tensor<T>,
        self_attn_mask: Option<&Tensor<T>>,
        cross_attn_mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // First sublayer: masked self-attention with residual connection
        let norm1_out = self.norm1.forward(x)?;
        let self_attention_out = self.self_attention.forward_qkv_with_mask(
            &norm1_out,
            &norm1_out,
            &norm1_out,
            self_attn_mask,
        )?;

        // Apply dropout to self-attention output during training
        let self_attention_dropped = self.self_attention_dropout.forward(&self_attention_out)?;
        let residual1 = x.add(&self_attention_dropped)?;

        // Second sublayer: cross-attention with residual connection
        let norm2_out = self.norm2.forward(&residual1)?;
        let cross_attention_out = self.cross_attention.forward_qkv_with_mask(
            &norm2_out,
            memory,
            memory,
            cross_attn_mask,
        )?;

        // Apply dropout to cross-attention output during training
        let cross_attention_dropped = self.cross_attention_dropout.forward(&cross_attention_out)?;
        let residual2 = residual1.add(&cross_attention_dropped)?;

        // Third sublayer: feed-forward with residual connection
        let norm3_out = self.norm3.forward(&residual2)?;
        let ff_out = self.feed_forward.forward(&norm3_out)?;

        // Apply dropout to feed-forward output during training
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual3 = residual2.add(&ff_dropped)?;

        Ok(residual3)
    }

    /// Post-norm variant for decoder
    /// Self-Attention -> Add -> LayerNorm -> Cross-Attention -> Add -> LayerNorm -> FFN -> Add -> LayerNorm
    fn forward_post_norm_with_memory(
        &self,
        x: &Tensor<T>,
        memory: &Tensor<T>,
        self_attn_mask: Option<&Tensor<T>>,
        cross_attn_mask: Option<&Tensor<T>>,
    ) -> Result<Tensor<T>> {
        // First sublayer: masked self-attention with residual connection
        let self_attention_out =
            self.self_attention
                .forward_qkv_with_mask(x, x, x, self_attn_mask)?;

        // Apply dropout to self-attention output during training
        let self_attention_dropped = self.self_attention_dropout.forward(&self_attention_out)?;
        let residual1 = x.add(&self_attention_dropped)?;
        let norm1_out = self.norm1.forward(&residual1)?;

        // Second sublayer: cross-attention with residual connection
        let cross_attention_out = self.cross_attention.forward_qkv_with_mask(
            &norm1_out,
            memory,
            memory,
            cross_attn_mask,
        )?;

        // Apply dropout to cross-attention output during training
        let cross_attention_dropped = self.cross_attention_dropout.forward(&cross_attention_out)?;
        let residual2 = norm1_out.add(&cross_attention_dropped)?;
        let norm2_out = self.norm2.forward(&residual2)?;

        // Third sublayer: feed-forward with residual connection
        let ff_out = self.feed_forward.forward(&norm2_out)?;

        // Apply dropout to feed-forward output during training
        let ff_dropped = self.ff_dropout.forward(&ff_out)?;
        let residual3 = norm2_out.add(&ff_dropped)?;
        let norm3_out = self.norm3.forward(&residual3)?;

        Ok(norm3_out)
    }

    /// Create a causal mask for autoregressive decoding
    /// This ensures that position i can only attend to positions <= i
    pub fn create_causal_mask(&self, seq_len: usize) -> Result<Tensor<T>> {
        MultiHeadAttention::create_causal_mask(seq_len)
    }

    /// Create a padding mask for cross-attention to encoder
    /// This masks out padded positions in the encoder sequence
    pub fn create_cross_attention_padding_mask(
        &self,
        encoder_lengths: &[usize],
        max_encoder_len: usize,
    ) -> Result<Tensor<T>> {
        MultiHeadAttention::create_padding_mask(encoder_lengths, max_encoder_len)
    }
}

impl<T> Layer<T> for TransformerDecoder<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Div<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Basic forward pass without encoder memory (self-attention only)
    /// For full decoder functionality, use forward_with_memory()
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // Create causal mask for self-attention
        let seq_len = input.shape().dims()[0]; // Assume first dimension is sequence length
        let causal_mask = self.create_causal_mask(seq_len)?;

        // Use input as both decoder input and "memory" for a self-attention-only mode
        self.forward_with_memory(input, input, Some(&causal_mask), None)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();

        // Add self-attention parameters
        params.extend(self.self_attention.parameters());

        // Add cross-attention parameters
        params.extend(self.cross_attention.parameters());

        // Add feed-forward parameters
        params.extend(self.feed_forward.parameters());

        // Add layer norm parameters
        params.extend(self.norm1.parameters());
        params.extend(self.norm2.parameters());
        params.extend(self.norm3.parameters());

        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();

        // Add self-attention parameters
        params.extend(self.self_attention.parameters_mut());

        // Add cross-attention parameters
        params.extend(self.cross_attention.parameters_mut());

        // Add feed-forward parameters
        params.extend(self.feed_forward.parameters_mut());

        // Add layer norm parameters
        params.extend(self.norm1.parameters_mut());
        params.extend(self.norm2.parameters_mut());
        params.extend(self.norm3.parameters_mut());

        // Add dropout parameters (none expected since dropout doesn't have learnable parameters)
        params.extend(self.self_attention_dropout.parameters_mut());
        params.extend(self.cross_attention_dropout.parameters_mut());
        params.extend(self.ff_dropout.parameters_mut());

        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.self_attention.set_training(training);
        self.cross_attention.set_training(training);
        self.feed_forward.set_training(training);
        self.norm1.set_training(training);
        self.norm2.set_training(training);
        self.norm3.set_training(training);
        self.self_attention_dropout.set_training(training);
        self.cross_attention_dropout.set_training(training);
        self.ff_dropout.set_training(training);
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

/// Expert Mixture (MoE) Feed-Forward Network
///
/// Implementation of Mixture of Experts for sparse and scalable neural networks.
/// Uses a gating mechanism to select a subset of expert networks for each input.
///
/// Based on: "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
/// https://arxiv.org/abs/1701.06538
///
/// # Architecture
/// - Multiple expert networks (typically FFN or specialized architectures)
/// - Gating network that learns to route inputs to appropriate experts
/// - Top-k routing for computational efficiency
/// - Load balancing mechanisms to ensure expert utilization
pub struct MixtureOfExperts<T>
where
    T: Float + Clone + Default + Zero + One + Send + Sync + 'static + FromPrimitive,
{
    input_dim: usize,
    hidden_dim: usize,
    num_experts: usize,
    top_k: usize,

    // Expert networks
    experts: Vec<FeedForwardNetwork<T>>,

    // Gating network
    gate: crate::layers::Dense<T>,

    // Load balancing
    load_balancing_loss_coeff: f32,

    training: bool,
}

impl<T> Clone for MixtureOfExperts<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Sub<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn clone(&self) -> Self {
        MixtureOfExperts {
            input_dim: self.input_dim,
            hidden_dim: self.hidden_dim,
            num_experts: self.num_experts,
            top_k: self.top_k,
            experts: self.experts.clone(),
            gate: self.gate.clone(),
            load_balancing_loss_coeff: self.load_balancing_loss_coeff,
            training: self.training,
        }
    }
}

impl<T> MixtureOfExperts<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    /// Create a new Mixture of Experts layer
    ///
    /// # Arguments
    /// * `input_dim` - Input dimension
    /// * `hidden_dim` - Hidden dimension for each expert
    /// * `num_experts` - Number of expert networks
    /// * `top_k` - Number of experts to activate per input (typically 1-2)
    /// * `dropout_rate` - Dropout rate for expert networks
    /// * `load_balancing_loss_coeff` - Coefficient for load balancing loss (typically 0.01)
    pub fn new(
        input_dim: usize,
        hidden_dim: usize,
        num_experts: usize,
        top_k: usize,
        dropout_rate: f32,
        load_balancing_loss_coeff: f32,
    ) -> Result<Self> {
        if top_k == 0 || top_k > num_experts {
            return Err(TensorError::invalid_argument(format!(
                "top_k ({top_k}) must be between 1 and num_experts ({num_experts})"
            )));
        }

        // Create expert networks
        let mut experts = Vec::with_capacity(num_experts);
        for _ in 0..num_experts {
            let expert = FeedForwardNetwork::new(input_dim, hidden_dim, dropout_rate)?;
            experts.push(expert);
        }

        // Create gating network
        let gate = crate::layers::Dense::new(input_dim, num_experts, true);

        Ok(Self {
            input_dim,
            hidden_dim,
            num_experts,
            top_k,
            experts,
            gate,
            load_balancing_loss_coeff,
            training: true,
        })
    }

    /// Forward pass with expert selection
    ///
    /// Returns (output, load_balancing_loss)
    pub fn forward_with_load_balancing(&self, input: &Tensor<T>) -> Result<(Tensor<T>, T)> {
        let batch_size = input.shape().dims()[0];
        let seq_len = if input.shape().dims().len() == 3 {
            input.shape().dims()[1]
        } else {
            1
        };

        // Reshape input to [batch_size * seq_len, input_dim] for easier processing
        let input_2d = if input.shape().dims().len() == 3 {
            input.reshape(&[batch_size * seq_len, self.input_dim])?
        } else {
            input.clone()
        };

        // Compute gate logits
        let gate_logits = self.gate.forward(&input_2d)?;

        // Apply softmax to get gating probabilities
        let gate_probs = tenflowers_core::ops::activation::softmax(&gate_logits, Some(-1))?;

        // Select top-k experts
        let (top_k_probs, top_k_indices) = self.select_top_k_experts(&gate_probs)?;

        // Compute expert outputs
        let expert_outputs = self.compute_expert_outputs(&input_2d, &top_k_indices)?;

        // Combine expert outputs using gating weights
        let combined_output =
            self.combine_expert_outputs(&expert_outputs, &top_k_probs, &top_k_indices)?;

        // Reshape output back to original shape
        let output = if input.shape().dims().len() == 3 {
            combined_output.reshape(&[batch_size, seq_len, self.input_dim])?
        } else {
            combined_output
        };

        // Compute load balancing loss
        let load_balancing_loss = if self.training {
            self.compute_load_balancing_loss(&gate_probs)?
        } else {
            T::zero()
        };

        Ok((output, load_balancing_loss))
    }

    /// Select top-k experts based on gating probabilities
    fn select_top_k_experts(&self, gate_probs: &Tensor<T>) -> Result<(Tensor<T>, Tensor<T>)> {
        let shape = gate_probs.shape();
        let data = gate_probs.to_vec()?;
        let batch_size = shape.dims()[0];

        let mut top_k_probs_data = Vec::with_capacity(batch_size * self.top_k);
        let mut top_k_indices_data = Vec::with_capacity(batch_size * self.top_k);

        for i in 0..batch_size {
            let start_idx = i * self.num_experts;
            let end_idx = start_idx + self.num_experts;
            let mut expert_probs: Vec<(T, usize)> = data[start_idx..end_idx]
                .iter()
                .enumerate()
                .map(|(idx, &prob)| (prob, idx))
                .collect();

            // Sort by probability (descending)
            expert_probs.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));

            // Take top-k
            for j in 0..self.top_k {
                let (prob, idx) = expert_probs[j];
                top_k_probs_data.push(prob);
                top_k_indices_data.push(T::from(idx).unwrap());
            }
        }

        let top_k_probs = Tensor::from_vec(top_k_probs_data, &[batch_size, self.top_k])?;
        let top_k_indices = Tensor::from_vec(top_k_indices_data, &[batch_size, self.top_k])?;

        Ok((top_k_probs, top_k_indices))
    }

    /// Compute outputs from selected experts
    fn compute_expert_outputs(
        &self,
        input: &Tensor<T>,
        top_k_indices: &Tensor<T>,
    ) -> Result<Vec<Tensor<T>>> {
        let batch_size = input.shape().dims()[0];
        let indices_data = top_k_indices.to_vec()?;

        let mut expert_outputs = Vec::with_capacity(batch_size * self.top_k);

        for i in 0..batch_size {
            let input_row = input.slice(&[i..i + 1, 0..self.input_dim])?;

            for j in 0..self.top_k {
                let expert_idx = indices_data[i * self.top_k + j].to_f32().unwrap() as usize;
                let expert_output = self.experts[expert_idx].forward(&input_row)?;
                expert_outputs.push(expert_output);
            }
        }

        Ok(expert_outputs)
    }

    /// Combine expert outputs using gating weights
    fn combine_expert_outputs(
        &self,
        expert_outputs: &[Tensor<T>],
        top_k_probs: &Tensor<T>,
        _top_k_indices: &Tensor<T>,
    ) -> Result<Tensor<T>> {
        let batch_size = top_k_probs.shape().dims()[0];
        let probs_data = top_k_probs.to_vec()?;

        // Initialize output
        let mut combined_output = Tensor::zeros(&[batch_size, self.input_dim]);

        for i in 0..batch_size {
            let mut weighted_sum = Tensor::zeros(&[1, self.input_dim]);

            for j in 0..self.top_k {
                let weight = probs_data[i * self.top_k + j];
                let expert_idx = i * self.top_k + j;
                let weighted_output = expert_outputs[expert_idx].scalar_mul(weight)?;
                weighted_sum = weighted_sum.add(&weighted_output)?;
            }

            // Copy to combined output
            let output_data = weighted_sum.to_vec()?;
            let mut combined_data = combined_output.to_vec()?;
            let start_idx = i * self.input_dim;
            let end_idx = start_idx + self.input_dim;
            combined_data[start_idx..end_idx].copy_from_slice(&output_data);
            combined_output = Tensor::from_vec(combined_data, &[batch_size, self.input_dim])?;
        }

        Ok(combined_output)
    }

    /// Compute load balancing loss to encourage uniform expert utilization
    fn compute_load_balancing_loss(&self, gate_probs: &Tensor<T>) -> Result<T> {
        // Compute average gate probability per expert
        let batch_size = gate_probs.shape().dims()[0];
        let probs_data = gate_probs.to_vec()?;

        let mut expert_sums = vec![T::zero(); self.num_experts];

        for i in 0..batch_size {
            for j in 0..self.num_experts {
                let idx = i * self.num_experts + j;
                expert_sums[j] = expert_sums[j] + probs_data[idx];
            }
        }

        // Compute coefficient of variation
        let mean = expert_sums.iter().cloned().sum::<T>() / T::from(expert_sums.len()).unwrap();
        let variance = expert_sums
            .iter()
            .map(|&x| (x - mean) * (x - mean))
            .sum::<T>()
            / T::from(expert_sums.len()).unwrap();

        let load_balancing_loss = variance / (mean * mean + T::from(1e-8).unwrap());

        Ok(load_balancing_loss * T::from(self.load_balancing_loss_coeff).unwrap())
    }

    /// Get the current load balancing loss coefficient
    pub fn load_balancing_loss_coeff(&self) -> f32 {
        self.load_balancing_loss_coeff
    }

    /// Set the load balancing loss coefficient
    pub fn set_load_balancing_loss_coeff(&mut self, coeff: f32) {
        self.load_balancing_loss_coeff = coeff;
    }

    /// Get expert utilization statistics
    pub fn expert_utilization(&self, gate_probs: &Tensor<T>) -> Result<Vec<f32>> {
        let batch_size = gate_probs.shape().dims()[0];
        let probs_data = gate_probs.to_vec()?;

        let mut utilization = vec![0.0f32; self.num_experts];

        for i in 0..batch_size {
            for j in 0..self.num_experts {
                let idx = i * self.num_experts + j;
                utilization[j] += probs_data[idx].to_f32().unwrap();
            }
        }

        // Normalize by batch size
        for util in &mut utilization {
            *util /= batch_size as f32;
        }

        Ok(utilization)
    }
}

impl<T> Layer<T> for MixtureOfExperts<T>
where
    T: Float
        + Clone
        + Default
        + Zero
        + One
        + Send
        + Sync
        + 'static
        + std::ops::Add<Output = T>
        + std::ops::Mul<Output = T>
        + std::ops::Sub<Output = T>
        + std::cmp::PartialOrd
        + num_traits::FromPrimitive
        + std::iter::Sum,
{
    fn forward(&self, input: &Tensor<T>) -> Result<Tensor<T>> {
        // Standard forward pass without returning load balancing loss
        let (output, _load_balancing_loss) = self.forward_with_load_balancing(input)?;
        Ok(output)
    }

    fn parameters(&self) -> Vec<&Tensor<T>> {
        let mut params = Vec::new();

        // Add gate parameters
        params.extend(self.gate.parameters());

        // Add expert parameters
        for expert in &self.experts {
            params.extend(expert.parameters());
        }

        params
    }

    fn parameters_mut(&mut self) -> Vec<&mut Tensor<T>> {
        let mut params = Vec::new();

        // Add gate parameters
        params.extend(self.gate.parameters_mut());

        // Add expert parameters
        for expert in &mut self.experts {
            params.extend(expert.parameters_mut());
        }

        params
    }

    fn set_training(&mut self, training: bool) {
        self.training = training;
        self.gate.set_training(training);
        for expert in &mut self.experts {
            expert.set_training(training);
        }
    }

    fn clone_box(&self) -> Box<dyn Layer<T>> {
        Box::new(self.clone())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_multi_query_attention_creation() {
        let mqa = MultiQueryAttention::<f32>::new(512, 8, true, Some("test_mqa".to_string()));
        assert!(mqa.is_ok());

        let mqa = mqa.unwrap();
        assert_eq!(mqa.embed_dim, 512);
        assert_eq!(mqa.num_heads, 8);
        assert_eq!(mqa.head_dim, 64); // 512 / 8
        assert_eq!(mqa.layer_id, "test_mqa");

        // Check parameter shapes (simplified implementation)
        assert_eq!(mqa.query_weight.shape().dims(), &[512, 512]); // Multi-head queries
        assert_eq!(mqa.key_weight.shape().dims(), &[512, 512]); // Simplified - same as query
        assert_eq!(mqa.value_weight.shape().dims(), &[512, 512]); // Simplified - same as query
        assert_eq!(mqa.output_weight.shape().dims(), &[512, 512]);
    }

    #[test]
    fn test_multi_query_attention_invalid_embed_dim() {
        let mqa = MultiQueryAttention::<f32>::new(513, 8, true, None); // 513 not divisible by 8
        assert!(mqa.is_err());
    }

    #[test]
    fn test_multi_query_attention_forward() {
        let mqa = MultiQueryAttention::<f32>::new(128, 4, false, None).unwrap();

        // Test with simpler 2D input first
        let simple_input = Tensor::<f32>::zeros(&[10, 128]); // [seq_len, embed_dim]
        let result = mqa.forward(&simple_input);
        if let Err(e) = &result {
            eprintln!("Forward error: {:?}", e);
        }
        assert!(result.is_ok());

        let output = result.unwrap();
        assert_eq!(output.shape().dims(), &[10, 128]); // Same shape as input
    }

    #[test]
    fn test_multi_query_attention_qkv() {
        let mqa = MultiQueryAttention::<f32>::new(64, 2, true, None).unwrap();
        // Use 2D tensors for simplified implementation
        let query = Tensor::<f32>::zeros(&[5, 64]);
        let key = Tensor::<f32>::zeros(&[5, 64]);
        let value = Tensor::<f32>::zeros(&[5, 64]);

        let result = mqa.forward_qkv(&query, &key, &value);
        if let Err(e) = &result {
            eprintln!("QKV error: {:?}", e);
        }
        assert!(result.is_ok());

        let output = result.unwrap();
        assert_eq!(output.shape().dims(), &[5, 64]);
    }

    #[test]
    fn test_multi_query_attention_parameters() {
        let mqa = MultiQueryAttention::<f32>::new(256, 4, true, None).unwrap();
        let params = mqa.parameters();

        // 4 weight matrices + 4 bias vectors = 8 parameters
        assert_eq!(params.len(), 8);
    }

    #[test]
    fn test_multi_query_attention_no_bias() {
        let mqa = MultiQueryAttention::<f32>::new(256, 4, false, None).unwrap();
        let params = mqa.parameters();

        // Only 4 weight matrices, no biases
        assert_eq!(params.len(), 4);
        assert!(mqa.query_bias.is_none());
        assert!(mqa.key_bias.is_none());
        assert!(mqa.value_bias.is_none());
        assert!(mqa.output_bias.is_none());
    }

    #[test]
    fn test_multi_query_attention_efficiency() {
        // Test that MQA uses fewer parameters than MHA
        let embed_dim = 512;
        let num_heads = 8;

        let mqa = MultiQueryAttention::<f32>::new(embed_dim, num_heads, false, None).unwrap();
        let mha = MultiHeadAttention::<f32>::new(embed_dim, num_heads, false).unwrap();

        let mqa_params = mqa.parameters().len();
        let mha_params = mha.parameters().len();

        // MQA should have the same number of parameter tensors,
        // but key and value weights are smaller
        assert_eq!(mqa_params, mha_params); // Same number of tensors

        // Check actual parameter counts by examining tensor sizes
        let mqa_key_size: usize = mqa.key_weight.shape().dims().iter().product();
        let mha_key_size: usize = mha.parameters()[1].shape().dims().iter().product();

        let mqa_value_size: usize = mqa.value_weight.shape().dims().iter().product();
        let mha_value_size: usize = mha.parameters()[2].shape().dims().iter().product();

        // In this simplified implementation, MQA and MHA have the same parameter sizes
        // In a full implementation, MQA key and value would be smaller
        assert_eq!(
            mqa_key_size, mha_key_size,
            "Simplified MQA has same key size as MHA"
        );
        assert_eq!(
            mqa_value_size, mha_value_size,
            "Simplified MQA has same value size as MHA"
        );
    }

    // Original MultiHeadAttention tests

    #[test]
    fn test_multihead_attention_creation() {
        let attention = MultiHeadAttention::<f32>::new(64, 8, true).unwrap();
        assert_eq!(attention.embed_dim, 64);
        assert_eq!(attention.num_heads, 8);
        assert_eq!(attention.head_dim, 8);

        // Should have 8 parameters (4 weights + 4 biases)
        assert_eq!(attention.parameters().len(), 8);
    }

    #[test]
    fn test_multihead_attention_invalid_dims() {
        // embed_dim not divisible by num_heads should fail
        let result = MultiHeadAttention::<f32>::new(65, 8, false);
        assert!(result.is_err());
    }

    #[test]
    fn test_multihead_attention_forward() {
        let attention = MultiHeadAttention::<f32>::new(64, 8, false).unwrap();

        // Create input tensor (simplified to 2D for this basic test)
        let input = Tensor::<f32>::ones(&[10, 64]);
        let output = attention.forward(&input).unwrap();

        // Output should have same shape as input
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_multihead_attention_no_bias_parameters() {
        let attention = MultiHeadAttention::<f32>::new(64, 8, false).unwrap();

        // Should have 4 parameters (4 weights, no biases)
        assert_eq!(attention.parameters().len(), 4);
    }

    #[test]
    fn test_feed_forward_network_creation() {
        let ffn = FeedForwardNetwork::<f32>::new(512, 2048, 0.1).unwrap();

        // Should have 4 parameters (2 weights + 2 biases from Dense layers)
        assert_eq!(ffn.parameters().len(), 4);
    }

    #[test]
    fn test_feed_forward_network_forward() {
        let ffn = FeedForwardNetwork::<f32>::new(64, 256, 0.1).unwrap();

        // Test with 2D input
        let input = Tensor::<f32>::ones(&[10, 64]);
        let output = ffn.forward(&input).unwrap();

        // Output should have same shape as input (after second linear layer)
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_transformer_encoder_creation() {
        let encoder =
            TransformerEncoder::<f32>::new(256, 8, Some(1024), Some(0.1), Some(true)).unwrap();
        assert_eq!(encoder._embed_dim, 256);
        assert!(encoder.pre_norm);
        assert_eq!(encoder._dropout_rate, 0.1);

        // Should have parameters from attention, feed-forward, and layer norms
        let params = encoder.parameters();
        assert!(params.len() > 0); // Should have multiple parameter groups
    }

    #[test]
    fn test_transformer_encoder_forward() {
        let encoder = TransformerEncoder::<f32>::new(128, 4, None, None, None).unwrap();

        // Test with 2D input (simplified)
        let input = Tensor::<f32>::ones(&[20, 128]); // [seq_len, embed_dim]
        let output = encoder.forward(&input).unwrap();

        // Output should have same shape as input
        assert_eq!(output.shape().dims(), &[20, 128]);
    }

    #[test]
    fn test_transformer_encoder_with_mask() {
        let encoder =
            TransformerEncoder::<f32>::new(64, 2, Some(128), Some(0.0), Some(false)).unwrap(); // post-norm variant

        let input = Tensor::<f32>::ones(&[10, 64]);

        // Create a simple causal mask
        let mask = MultiHeadAttention::<f32>::create_causal_mask(10).unwrap();
        let output = encoder.forward_with_mask(&input, Some(&mask)).unwrap();

        // Output should have same shape as input
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_transformer_encoder_parameters() {
        let mut encoder = TransformerEncoder::<f32>::new(32, 2, Some(64), None, None).unwrap();

        // Should have parameters from all components
        let params_len = encoder.parameters().len();
        let params_mut_len = encoder.parameters_mut().len();

        assert!(params_len > 10); // Should have many parameters
        assert_eq!(params_len, params_mut_len);
    }

    #[test]
    fn test_transformer_encoder_training_mode() {
        let mut encoder = TransformerEncoder::<f32>::new(16, 1, Some(32), None, None).unwrap();

        // Test setting training mode
        encoder.set_training(false);
        // Note: We can't directly test the training state of sub-components without additional getters
        // But this ensures the method doesn't panic

        encoder.set_training(true);
    }

    #[test]
    fn test_transformer_decoder_creation() {
        let decoder =
            TransformerDecoder::<f32>::new(256, 8, Some(1024), Some(0.1), Some(true)).unwrap();
        assert_eq!(decoder._embed_dim, 256);
        assert!(decoder.pre_norm);
        assert_eq!(decoder._dropout_rate, 0.1);

        // Should have parameters from self-attention, cross-attention, feed-forward, and layer norms
        let params = decoder.parameters();
        assert!(params.len() > 10); // Should have many parameters (more than encoder due to cross-attention)
    }

    #[test]
    fn test_transformer_decoder_forward() {
        let decoder = TransformerDecoder::<f32>::new(128, 4, None, None, None).unwrap();

        // Test with 2D input (simplified)
        let input = Tensor::<f32>::ones(&[20, 128]); // [seq_len, embed_dim]
        let output = decoder.forward(&input).unwrap();

        // Output should have same shape as input
        assert_eq!(output.shape().dims(), &[20, 128]);
    }

    #[test]
    fn test_transformer_decoder_with_memory() {
        let decoder =
            TransformerDecoder::<f32>::new(64, 2, Some(128), Some(0.0), Some(false)).unwrap(); // post-norm variant

        let decoder_input = Tensor::<f32>::ones(&[10, 64]); // Target sequence
        let encoder_memory = Tensor::<f32>::ones(&[15, 64]); // Encoder output sequence (different length)

        // Create causal mask for decoder self-attention
        let causal_mask = decoder.create_causal_mask(10).unwrap();

        // Create padding mask for cross-attention (assume all encoder positions are valid)
        let encoder_lengths = vec![15]; // All positions valid
        let cross_mask = decoder
            .create_cross_attention_padding_mask(&encoder_lengths, 15)
            .unwrap();

        let output = decoder
            .forward_with_memory(
                &decoder_input,
                &encoder_memory,
                Some(&causal_mask),
                Some(&cross_mask),
            )
            .unwrap();

        // Output should have same shape as decoder input
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_transformer_decoder_parameters() {
        let mut decoder = TransformerDecoder::<f32>::new(32, 2, Some(64), None, None).unwrap();

        // Should have parameters from all components
        let params_len = decoder.parameters().len();
        let params_mut_len = decoder.parameters_mut().len();

        assert!(params_len > 15); // Should have many parameters (more than encoder)
        assert_eq!(params_len, params_mut_len);
    }

    #[test]
    fn test_transformer_decoder_training_mode() {
        let mut decoder = TransformerDecoder::<f32>::new(16, 1, Some(32), None, None).unwrap();

        // Test setting training mode
        decoder.set_training(false);
        // Note: We can't directly test the training state of sub-components without additional getters
        // But this ensures the method doesn't panic

        decoder.set_training(true);
    }

    #[test]
    fn test_transformer_decoder_mask_creation() {
        let decoder = TransformerDecoder::<f32>::new(32, 2, None, None, None).unwrap();

        // Test causal mask creation
        let causal_mask = decoder.create_causal_mask(5).unwrap();
        assert_eq!(causal_mask.shape().dims(), &[5, 5]);

        // Test cross-attention padding mask creation
        let encoder_lengths = vec![3, 4, 2]; // Variable length sequences
        let cross_mask = decoder
            .create_cross_attention_padding_mask(&encoder_lengths, 4)
            .unwrap();
        assert_eq!(cross_mask.shape().dims(), &[3, 4]); // batch_size x max_len
    }

    #[test]
    fn test_transformer_decoder_pre_vs_post_norm() {
        // Test both pre-norm and post-norm variants
        let pre_norm_decoder =
            TransformerDecoder::<f32>::new(32, 2, Some(64), None, Some(true)).unwrap();
        let post_norm_decoder =
            TransformerDecoder::<f32>::new(32, 2, Some(64), None, Some(false)).unwrap();

        assert!(pre_norm_decoder.pre_norm);
        assert!(!post_norm_decoder.pre_norm);

        let input = Tensor::<f32>::ones(&[8, 32]);
        let memory = Tensor::<f32>::ones(&[10, 32]);

        // Both variants should work without errors
        let pre_norm_output = pre_norm_decoder
            .forward_with_memory(&input, &memory, None, None)
            .unwrap();
        let post_norm_output = post_norm_decoder
            .forward_with_memory(&input, &memory, None, None)
            .unwrap();

        // Both should have the same output shape
        assert_eq!(pre_norm_output.shape().dims(), &[8, 32]);
        assert_eq!(post_norm_output.shape().dims(), &[8, 32]);
    }

    #[test]
    fn test_swiglu_creation() {
        let swiglu = SwiGLU::<f32>::new(512, 1365, 0.1).unwrap(); // 1365 â 8/3 * 512

        // Should have 4 parameters (gate_proj: 1 weight, up_proj: 1 weight, down_proj: 1 weight + 1 bias)
        assert_eq!(swiglu.parameters().len(), 4);
    }

    #[test]
    fn test_swiglu_forward() {
        let swiglu = SwiGLU::<f32>::new(64, 171, 0.0).unwrap(); // 171 â 8/3 * 64

        // Test with 2D input
        let input = Tensor::<f32>::ones(&[10, 64]);
        let output = swiglu.forward(&input).unwrap();

        // Output should have same shape as input (after down projection)
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_geglu_creation() {
        let geglu = GeGLU::<f32>::new(512, 1365, 0.1).unwrap(); // 1365 â 8/3 * 512

        // Should have 4 parameters (gate_proj: 1 weight, up_proj: 1 weight, down_proj: 1 weight + 1 bias)
        assert_eq!(geglu.parameters().len(), 4);
    }

    #[test]
    fn test_geglu_forward() {
        let geglu = GeGLU::<f32>::new(64, 171, 0.0).unwrap(); // 171 â 8/3 * 64

        // Test with 2D input
        let input = Tensor::<f32>::ones(&[10, 64]);
        let output = geglu.forward(&input).unwrap();

        // Output should have same shape as input (after down projection)
        assert_eq!(output.shape().dims(), &[10, 64]);
    }

    #[test]
    fn test_glu_variants_parameter_count() {
        let input_dim = 256;
        let hidden_dim = 341; // 8/3 * 256 â 341

        // Create all variants
        let swiglu = SwiGLU::<f32>::new(input_dim, hidden_dim, 0.1).unwrap();
        let geglu = GeGLU::<f32>::new(input_dim, hidden_dim, 0.1).unwrap();
        let standard_ffn = FeedForwardNetwork::<f32>::new(input_dim, input_dim * 4, 0.1).unwrap();

        // GLU variants should have more parameters due to the extra projection
        let swiglu_params = swiglu.parameters().len();
        let geglu_params = geglu.parameters().len();
        let ffn_params = standard_ffn.parameters().len();

        assert_eq!(swiglu_params, 4); // gate_proj(1) + up_proj(1) + down_proj(2) = 4
        assert_eq!(geglu_params, 4); // Same structure
        assert_eq!(ffn_params, 4); // linear1(2) + linear2(2) = 4

        // GLU variants have one more weight matrix but potentially fewer total parameters
        // when hidden_dim is chosen properly (8/3 * input_dim instead of 4 * input_dim)
    }

    #[test]
    fn test_glu_variants_training_mode() {
        let mut swiglu = SwiGLU::<f32>::new(32, 43, 0.1).unwrap();
        let mut geglu = GeGLU::<f32>::new(32, 43, 0.1).unwrap();

        // Test setting training mode
        swiglu.set_training(false);
        geglu.set_training(false);

        // Test setting back to training
        swiglu.set_training(true);
        geglu.set_training(true);

        // Should not panic
    }

    #[test]
    fn test_kv_cache_creation() {
        let mut cache = KVCache::<f32>::new(100);

        assert_eq!(cache.max_seq_len, 100);
        assert_eq!(cache.current_pos, 0);
        assert!(!cache.is_full());
        assert!(cache.get("test_layer").is_none());
    }

    #[test]
    fn test_kv_cache_operations() {
        let mut cache = KVCache::<f32>::new(10);

        // Test cache update
        let key = Tensor::zeros(&[5, 64]);
        let value = Tensor::zeros(&[5, 64]);

        cache
            .update("layer1".to_string(), key.clone(), value.clone())
            .unwrap();

        // Test cache retrieval
        let (cached_k, cached_v) = cache.get("layer1").unwrap();
        assert_eq!(cached_k.shape().dims(), key.shape().dims());
        assert_eq!(cached_v.shape().dims(), value.shape().dims());

        // Test position advancement
        cache.advance();
        assert_eq!(cache.position(), 1);

        // Test cache clearing
        cache.clear();
        assert_eq!(cache.position(), 0);
        assert!(cache.get("layer1").is_none());
    }

    #[test]
    fn test_attention_with_cache() {
        let mut mha =
            MultiHeadAttention::<f32>::with_layer_id(64, 4, false, "test_layer".to_string())
                .unwrap();
        let mut cache = KVCache::<f32>::new(100);

        // First forward pass - should initialize cache
        let query1 = Tensor::zeros(&[1, 64]); // Single token
        let key1 = Tensor::zeros(&[1, 64]);
        let value1 = Tensor::zeros(&[1, 64]);

        let output1 = mha
            .forward_with_cache(
                &query1,
                Some(&key1),
                Some(&value1),
                Some(&mut cache),
                None,
                true,
            )
            .unwrap();

        assert_eq!(output1.shape().dims(), &[1, 64]);
        assert!(cache.get("test_layer").is_some());

        // Second forward pass - should use and extend cache
        let query2 = Tensor::zeros(&[1, 64]);
        let key2 = Tensor::zeros(&[1, 64]);
        let value2 = Tensor::zeros(&[1, 64]);

        let output2 = mha
            .forward_with_cache(
                &query2,
                Some(&key2),
                Some(&value2),
                Some(&mut cache),
                None,
                true,
            )
            .unwrap();

        assert_eq!(output2.shape().dims(), &[1, 64]);

        // Cache should now contain concatenated keys/values
        let (cached_k, cached_v) = cache.get("test_layer").unwrap();
        assert_eq!(cached_k.shape().dims()[0], 2); // Should have 2 tokens worth of keys
        assert_eq!(cached_v.shape().dims()[0], 2); // Should have 2 tokens worth of values
    }

    #[test]
    fn test_attention_without_cache() {
        let mut mha = MultiHeadAttention::<f32>::new(64, 4, false).unwrap();
        let query = Tensor::zeros(&[5, 64]);
        let key = Tensor::zeros(&[5, 64]);
        let value = Tensor::zeros(&[5, 64]);

        // Test with cache disabled
        let output1 = mha
            .forward_with_cache(&query, Some(&key), Some(&value), None, None, false)
            .unwrap();

        // Should be equivalent to regular forward pass
        let output2 = mha.forward_qkv(&query, &key, &value).unwrap();

        assert_eq!(output1.shape().dims(), output2.shape().dims());
    }

    #[test]
    fn test_layer_id_operations() {
        let mut mha =
            MultiHeadAttention::<f32>::with_layer_id(64, 4, false, "custom_layer".to_string())
                .unwrap();

        assert_eq!(mha.layer_id(), "custom_layer");

        mha.set_layer_id("new_layer".to_string());
        assert_eq!(mha.layer_id(), "new_layer");
    }
}
